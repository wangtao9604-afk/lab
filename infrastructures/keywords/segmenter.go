package keywords

import (
	"fmt"
	"regexp"
	"sort"
	"strings"
	"sync"

	"github.com/go-ego/gse"
)

// AdvancedSegmenter é«˜çº§åˆ†è¯å™¨
type AdvancedSegmenter struct {
	gse         gse.Segmenter
	userDict    map[string]bool
	mu          sync.RWMutex
	initialized bool
}

// ChineseNumberRegex ä¸­æ–‡æ•°å­—æ­£åˆ™
var ChineseNumberRegex = regexp.MustCompile(`[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤ä¿©ä»¨å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾é›¶ã€‡ç™¾åƒä¸‡äº¿]{1,10}`)

// WordWithTag è¯æ±‡æ ‡è®°ç»“æ„
type WordWithTag struct {
	Word string
	Tag  string
}

// NewAdvancedSegmenter åˆ›å»ºé«˜çº§åˆ†è¯å™¨
func NewAdvancedSegmenter() *AdvancedSegmenter {
	segmenter := &AdvancedSegmenter{
		userDict: make(map[string]bool),
	}

	if err := segmenter.init(); err != nil {
		return nil
	}

	return segmenter
}

// init åˆå§‹åŒ–åˆ†è¯å™¨
func (seg *AdvancedSegmenter) init() error {
	seg.mu.Lock()
	defer seg.mu.Unlock()

	// åˆå§‹åŒ–gseåˆ†è¯å™¨
	seg.gse.LoadDict()

	// åŠ è½½è‡ªå®šä¹‰è¯å…¸
	seg.loadCustomDict()

	seg.initialized = true
	return nil
}

// loadCustomDict åŠ è½½è‡ªå®šä¹‰è¯å…¸
func (seg *AdvancedSegmenter) loadCustomDict() {
	// æˆ¿äº§ç›¸å…³ä¸“ä¸šè¯æ±‡
	customWords := []string{
		// æˆ¿å‹ç›¸å…³ - ç¡®ä¿è¿™äº›è¯æ±‡ä¸è¢«æ‹†åˆ†
		"ä¸€å±…å®¤", "äºŒå±…å®¤", "ä¸‰å±…å®¤", "å››å±…å®¤", "äº”å±…å®¤",
		"1å±…å®¤", "2å±…å®¤", "3å±…å®¤", "4å±…å®¤", "5å±…å®¤",
		"ä¸€å®¤ä¸€å…", "äºŒå®¤ä¸€å…", "ä¸‰å®¤äºŒå…", "å››å®¤äºŒå…",
		"1å®¤1å…", "2å®¤1å…", "3å®¤2å…", "4å®¤2å…", "5å®¤3å…",
		"å•èº«å…¬å¯“", "ç²¾è£…å…¬å¯“", "loftå…¬å¯“", "å¤å¼å…¬å¯“",
		"å¼€é—´", "å•é—´", "å¤§å¼€é—´", "å°å¼€é—´",

		// æˆ¿äº§ç±»å‹
		"åœ°é“æˆ¿", "åˆšéœ€æˆ¿", "æ”¹å–„æˆ¿", "æŠ•èµ„æˆ¿",
		"å—åŒ—é€šé€", "ä¸œå—å‘", "è¥¿å—å‘", "æœå—", "æœåŒ—",
		"æ¯›å¯æˆ¿", "ç²¾è£…ä¿®", "ç®€è£…ä¿®", "è±ªåè£…ä¿®", "æ‹åŒ…å…¥ä½",
		"äºŒæ‰‹æˆ¿", "æ–°æˆ¿", "æœŸæˆ¿", "ç°æˆ¿", "å‡†ç°æˆ¿",
		"åˆ«å¢…", "æ´‹æˆ¿", "é«˜å±‚", "å°é«˜å±‚", "å¤šå±‚",
		"åœ°é“å£", "åœ°é“ç«™", "å…¬äº¤ç«™", "å•†åœˆ", "é…å¥—",
		"æˆ·å‹å›¾", "æ ·æ¿é—´", "å”®æ¥¼å¤„", "ä¸­ä»‹è´¹", "è¿‡æˆ·è´¹",
	}

	for _, word := range customWords {
		seg.userDict[word] = true
		// gse æ·»åŠ è‡ªå®šä¹‰è¯å…¸
		seg.gse.AddToken(word, 1000, "n") // è®¾ç½®ä¸ºåè¯ï¼Œæƒé‡1000
	}
}

// AddUserWord æ·»åŠ ç”¨æˆ·è‡ªå®šä¹‰è¯æ±‡
func (seg *AdvancedSegmenter) AddUserWord(word string) {
	seg.mu.Lock()
	defer seg.mu.Unlock()

	seg.userDict[word] = true
	seg.gse.AddToken(word, 500, "n")
}

// CutWithTag åˆ†è¯å¹¶æ ‡æ³¨è¯æ€§
func (seg *AdvancedSegmenter) CutWithTag(text string) []WordWithTag {
	if !seg.initialized {
		return []WordWithTag{}
	}

	// ä½¿ç”¨gseè¿›è¡Œåˆ†è¯
	segments := seg.gse.Cut(text, true)

	result := make([]WordWithTag, 0, len(segments))
	for _, word := range segments {
		if strings.TrimSpace(word) == "" {
			continue
		}

		// ç®€å•çš„è¯æ€§æ ‡æ³¨é€»è¾‘
		tag := seg.inferPOSTag(word)
		result = append(result, WordWithTag{
			Word: word,
			Tag:  tag,
		})
	}

	return result
}

// Cut ç®€å•åˆ†è¯
func (seg *AdvancedSegmenter) Cut(text string) []string {
	if !seg.initialized {
		return []string{}
	}

	return seg.gse.Cut(text, true)
}

// CutForSearch æœç´¢åˆ†è¯
func (seg *AdvancedSegmenter) CutForSearch(text string) []string {
	if !seg.initialized {
		return []string{}
	}

	return seg.gse.CutSearch(text, true)
}

// inferPOSTag æ¨æ–­è¯æ€§æ ‡ç­¾
func (seg *AdvancedSegmenter) inferPOSTag(word string) string {
	// åŸºäºè§„åˆ™çš„ç®€å•è¯æ€§æ¨æ–­
	if len(word) == 0 {
		return "x"
	}

	// æ•°å­—
	if regexp.MustCompile(`^\d+$`).MatchString(word) {
		return "m"
	}

	// ä¸­æ–‡æ•°å­—
	if ChineseNumberRegex.MatchString(word) {
		return "m"
	}

	// å¸¸è§é‡è¯
	quantifiers := []string{"ä¸ª", "å¥—", "é—´", "æ ‹", "å±‚", "ç±³", "å¹³", "ä¸‡", "åƒ", "å…ƒ"}
	for _, q := range quantifiers {
		if strings.Contains(word, q) {
			return "mq"
		}
	}

	// åœ°å
	locations := []string{"åŒº", "å¿", "å¸‚", "çœ", "è¡—", "è·¯", "æ‘", "é•‡", "é‡Œ", "å›­"}
	for _, loc := range locations {
		if strings.HasSuffix(word, loc) {
			return "ns"
		}
	}

	// æ—¶é—´è¯
	timeWords := []string{"å¹´", "æœˆ", "æ—¥", "ç‚¹", "æ—¶", "åˆ†", "ä»Šå¤©", "æ˜å¤©", "æ˜¨å¤©", "ä¸‹å‘¨", "ä¸Šæœˆ"}
	for _, t := range timeWords {
		if strings.Contains(word, t) {
			return "tg"
		}
	}

	// é»˜è®¤ä¸ºåè¯
	return "n"
}

// ExtractNumbers æå–æ•°å­—
func (seg *AdvancedSegmenter) ExtractNumbers(text string) []string {
	// é˜¿æ‹‰ä¼¯æ•°å­—
	arabicNumbers := regexp.MustCompile(`\d+(?:\.\d+)?`).FindAllString(text, -1)

	// ä¸­æ–‡æ•°å­—
	chineseNumbers := ChineseNumberRegex.FindAllString(text, -1)

	// åˆå¹¶ç»“æœ
	result := make([]string, 0, len(arabicNumbers)+len(chineseNumbers))
	result = append(result, arabicNumbers...)
	result = append(result, chineseNumbers...)

	return result
}

// IsInitialized æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–
func (seg *AdvancedSegmenter) IsInitialized() bool {
	seg.mu.RLock()
	defer seg.mu.RUnlock()
	return seg.initialized
}

// GetStats è·å–ç»Ÿè®¡ä¿¡æ¯
func (seg *AdvancedSegmenter) GetStats() map[string]interface{} {
	seg.mu.RLock()
	defer seg.mu.RUnlock()

	return map[string]interface{}{
		"initialized":    seg.initialized,
		"user_dict_size": len(seg.userDict),
		"segmenter_type": "gse",
	}
}

// Close å…³é—­åˆ†è¯å™¨
func (seg *AdvancedSegmenter) Close() {
	seg.mu.Lock()
	defer seg.mu.Unlock()

	// gse ä¸éœ€è¦æ˜¾å¼å…³é—­
	seg.initialized = false
}

// PreprocessText æ–‡æœ¬é¢„å¤„ç†
func (seg *AdvancedSegmenter) PreprocessText(text string) string {
	// ç§»é™¤å¤šä½™ç©ºç™½
	text = regexp.MustCompile(`\s+`).ReplaceAllString(text, " ")
	text = strings.TrimSpace(text)

	// æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
	replacements := map[string]string{
		"ï¼Œ": ",", "ã€‚": ".", "ï¼": "!", "ï¼Ÿ": "?",
		"ï¼›": ";", "ï¼š": ":", "ï¼ˆ": "(", "ï¼‰": ")",
		"ã€": "[", "ã€‘": "]", "ã€Š": "<", "ã€‹": ">",
	}

	for old, new := range replacements {
		text = strings.ReplaceAll(text, old, new)
	}

	return text
}

// ValidateText éªŒè¯æ–‡æœ¬
func (seg *AdvancedSegmenter) ValidateText(text string) error {
	if text == "" {
		return fmt.Errorf("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
	}

	if len(text) > 10000 {
		return fmt.Errorf("æ–‡æœ¬é•¿åº¦ä¸èƒ½è¶…è¿‡10000å­—ç¬¦")
	}

	// æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆçš„ä¸­æ–‡å­—ç¬¦
	chineseRegex := regexp.MustCompile(`[\p{Han}]`)
	if !chineseRegex.MatchString(text) {
		return fmt.Errorf("æ–‡æœ¬åº”åŒ…å«ä¸­æ–‡å­—ç¬¦")
	}

	return nil
}

// AnalyzeText åˆ†ææ–‡æœ¬ç‰¹å¾
func (seg *AdvancedSegmenter) AnalyzeText(text string) map[string]interface{} {
	words := seg.Cut(text)

	// ç»Ÿè®¡ç‰¹å¾
	chineseCount := len(regexp.MustCompile(`[\p{Han}]`).FindAllString(text, -1))
	englishCount := len(regexp.MustCompile(`[a-zA-Z]`).FindAllString(text, -1))
	digitCount := len(regexp.MustCompile(`\d`).FindAllString(text, -1))
	punctCount := len(regexp.MustCompile(`[\p{P}]`).FindAllString(text, -1))

	// è®¡ç®—å¹³å‡è¯é•¿ï¼Œé¿å…é™¤é›¶é”™è¯¯
	var avgWordLength float64
	if len(words) > 0 {
		avgWordLength = float64(len(text)) / float64(len(words))
	} else {
		avgWordLength = 0.0
	}

	return map[string]interface{}{
		"text_length":     len(text),
		"word_count":      len(words),
		"chinese_chars":   chineseCount,
		"english_chars":   englishCount,
		"digit_chars":     digitCount,
		"punct_chars":     punctCount,
		"avg_word_length": avgWordLength,
	}
}

// ConvertToSimplified ç¹ä½“è½¬ç®€ä½“ï¼ˆç®€å•å®ç°ï¼‰
func (seg *AdvancedSegmenter) ConvertToSimplified(text string) string {
	// ç®€å•çš„ç¹ä½“å­—æ˜ å°„
	traditionalToSimplified := map[string]string{
		"è³¼": "è´­", "è²·": "ä¹°", "è³£": "å–", "åƒ¹": "ä»·",
		"æ¨“": "æ¥¼", "å±¤": "å±‚", "å€": "åŒº", "ç¸£": "å¿",
		"ç’°": "ç¯", "é»": "ç‚¹", "æ™‚": "æ—¶", "é–“": "é—´",
		"å‹•": "åŠ¨", "ç”£": "äº§", "æ¥­": "ä¸š", "å‹™": "åŠ¡",
		"é–€": "é—¨", "è»Š": "è½¦", "å­¸": "å­¦", "åœ’": "å›­",
	}

	result := text

	// ğŸ”§ ä¿®å¤ç¨³å®šæ€§é—®é¢˜ï¼šç¡®ä¿ç¡®å®šæ€§çš„éå†é¡ºåº
	var traditionalKeys []string
	for traditional := range traditionalToSimplified {
		traditionalKeys = append(traditionalKeys, traditional)
	}
	sort.Strings(traditionalKeys)

	for _, traditional := range traditionalKeys {
		simplified := traditionalToSimplified[traditional]
		result = strings.ReplaceAll(result, traditional, simplified)
	}

	return result
}

// SplitSentences åˆ†å¥
func (seg *AdvancedSegmenter) SplitSentences(text string) []string {
	// å¥å­åˆ†å‰²ç¬¦
	sentenceEnders := regexp.MustCompile(`[ã€‚ï¼ï¼Ÿ.!?]+`)

	sentences := sentenceEnders.Split(text, -1)

	// æ¸…ç†ç©ºå¥å­
	result := make([]string, 0, len(sentences))
	for _, sentence := range sentences {
		sentence = strings.TrimSpace(sentence)
		if sentence != "" {
			result = append(result, sentence)
		}
	}

	return result
}

// TokenizeBasic åŸºç¡€åˆ†è¯ï¼ˆçº¯ç©ºæ ¼åˆ†å‰²ï¼‰
func (seg *AdvancedSegmenter) TokenizeBasic(text string) []string {
	return strings.Fields(text)
}

// FilterStopWords è¿‡æ»¤åœç”¨è¯
func (seg *AdvancedSegmenter) FilterStopWords(words []string) []string {
	// åŸºç¡€åœç”¨è¯åˆ—è¡¨
	stopWords := map[string]bool{
		"çš„": true, "äº†": true, "åœ¨": true, "æ˜¯": true, "æˆ‘": true,
		"æœ‰": true, "å’Œ": true, "å°±": true, "ä¸": true, "äºº": true,
		"éƒ½": true, "ä¸€": true, "ä¸€ä¸ª": true, "ä¸Š": true, "ä¹Ÿ": true,
		"å¾ˆ": true, "åˆ°": true, "è¯´": true, "è¦": true, "å»": true,
		"ä½ ": true, "ä¼š": true, "ç€": true, "æ²¡æœ‰": true, "çœ‹": true,
		"å¥½": true, "è‡ªå·±": true, "è¿™": true, "é‚£": true, "é‡Œ": true,
		"å°±æ˜¯": true, "è¿˜": true, "æŠŠ": true, "æ¯”": true, "ä»–": true,
	}

	result := make([]string, 0, len(words))
	for _, word := range words {
		if !stopWords[word] && len(word) > 1 {
			result = append(result, word)
		}
	}

	return result
}
