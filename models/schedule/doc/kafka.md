# ä¼ä¸šå¾®ä¿¡å®¢æœæ¶ˆæ¯ç³»ç»Ÿ Kafka é›†æˆæ–¹æ¡ˆ

## 1. æ¶æ„æ¦‚è¿°

### 1.1 æ ¸å¿ƒè®¾è®¡åŸåˆ™
- **æ¶ˆæ¯é¡ºåºæ€§ä¿è¯**ï¼šåŒä¸€ç”¨æˆ·çš„æ¶ˆæ¯å¿…é¡»æŒ‰æ—¶é—´é¡ºåºå¤„ç†
- **é«˜å¹¶å‘å¤„ç†**ï¼šä¸åŒç”¨æˆ·çš„æ¶ˆæ¯å¯ä»¥å¹¶è¡Œå¤„ç†
- **å®¹é”™ä¸é«˜å¯ç”¨**ï¼šæ”¯æŒèŠ‚ç‚¹æ•…éšœè‡ªåŠ¨æ¢å¤ï¼Œä¸ä¸¢æ¶ˆæ¯
- **æ°´å¹³æ‰©å±•**ï¼šæ”¯æŒåŠ¨æ€å¢å‡èŠ‚ç‚¹
- **å¹³æ»‘è¿‡æ¸¡**ï¼šæ”¯æŒä»å•ç‚¹Kafkaå¹³æ»‘å‡çº§åˆ°é›†ç¾¤

### 1.2 æ•´ä½“æ¶æ„
```
å¾®ä¿¡å›è°ƒ â†’ KFæ¨¡å—(ç”Ÿäº§è€…) â†’ Kafka Topic(12 Partitions) â†’ Scheduleæ¨¡å—(æ¶ˆè´¹è€…)
                â†“                     â†“                          â†“
          è·å–å¹¶åŒæ­¥æ¶ˆæ¯        æŒ‰UserID Hashåˆ†åŒº          ç”¨æˆ·çº§å¹¶è¡Œå¤„ç†
```

## 2. Kafka Topic è®¾è®¡

### 2.1 Topic é…ç½®

#### ç”Ÿäº§ç¯å¢ƒé…ç½®ï¼ˆé›†ç¾¤ï¼‰
```yaml
topic:
  name: qywx-chat-messages
  partitions: 12          # 3èŠ‚ç‚¹ Ã— 4ï¼Œé¢„ç•™æ‰©å±•ç©ºé—´
  replication-factor: 3   # 3å‰¯æœ¬ï¼Œä¿è¯é«˜å¯ç”¨
  min.insync.replicas: 2  # æœ€å°‘åŒæ­¥å‰¯æœ¬æ•°
  retention.ms: 604800000 # 7å¤©ä¿ç•™æœŸ
  compression.type: lz4    # å‹ç¼©ç®—æ³•
  max.message.bytes: 1048576  # æœ€å¤§æ¶ˆæ¯1MB
```

#### å¼€å‘ç¯å¢ƒé…ç½®ï¼ˆå•ç‚¹ï¼‰
```yaml
# âš ï¸ é‡è¦ï¼šå•ç‚¹Kafkaå¿…é¡»è°ƒæ•´çš„å‚æ•°
topic:
  name: qywx-chat-messages
  partitions: 4           # å•èŠ‚ç‚¹å»ºè®®1-4ä¸ªåˆ†åŒº
  replication-factor: 1   # âš ï¸ å¿…é¡»ä¸º1ï¼ˆå•ç‚¹åªæœ‰1ä¸ªbrokerï¼‰
  min.insync.replicas: 1  # âš ï¸ å¿…é¡»ä¸º1ï¼ˆå¦åˆ™æ— æ³•å‘é€æ¶ˆæ¯ï¼‰
  retention.ms: 604800000 # 7å¤©ä¿ç•™æœŸ
  compression.type: lz4    # å‹ç¼©ç®—æ³•
  max.message.bytes: 1048576  # æœ€å¤§æ¶ˆæ¯1MB
  
# DLQ TopicåŒæ ·éœ€è¦è°ƒæ•´
dlq_topic:
  name: qywx-dead-letter-queue
  partitions: 2           # å•ç‚¹å»ºè®®2ä¸ªåˆ†åŒº
  replication-factor: 1   # âš ï¸ å¿…é¡»ä¸º1
  min.insync.replicas: 1  # âš ï¸ å¿…é¡»ä¸º1
```

### 2.2 åˆ†åŒºç­–ç•¥
- ä½¿ç”¨ `external_user_id` ä½œä¸º Partition Key
- Kafka å†…ç½® Murmur2 Hash ä¿è¯åŒä¸€ç”¨æˆ·æ¶ˆæ¯è·¯ç”±åˆ°åŒä¸€åˆ†åŒº
- 12ä¸ªåˆ†åŒºå‡åŒ€åˆ†é…ç»™3ä¸ªæ¶ˆè´¹èŠ‚ç‚¹ï¼ˆæ¯èŠ‚ç‚¹4ä¸ªåˆ†åŒºï¼‰

## 3. ç”Ÿäº§è€…è®¾è®¡

### 3.1 ç”Ÿäº§è€…å®ç°
```go
package producer

import (
    "encoding/json"
    "fmt"
    "sync"
    "time"
    
    "github.com/confluentinc/confluent-kafka-go/v2/kafka"
    "qywx/infrastructures/log"
    "qywx/infrastructures/wxmsg/kefu"
)

type KafkaProducer struct {
    producer *kafka.Producer
    topic    string
    mu       sync.Mutex
}

// NewKafkaProducer åˆ›å»ºKafkaç”Ÿäº§è€…
func NewKafkaProducer(brokers string, topic string) (*KafkaProducer, error) {
    // ä»é…ç½®æ–‡ä»¶è¯»å–ç¯å¢ƒç±»å‹
    cfg := config.GetInstance()
    
    config := &kafka.ConfigMap{
        "bootstrap.servers": brokers,  // æ”¯æŒå•ç‚¹æˆ–é›†ç¾¤åˆ—è¡¨
        
        // å¯é æ€§é…ç½®ï¼ˆå•ç‚¹å’Œé›†ç¾¤é€šç”¨ï¼‰
        "acks":                   "all",  // å•ç‚¹æ—¶ç­‰åŒäºacks=1
        "retries":                10,     // é‡è¯•æ¬¡æ•°
        "max.in.flight.requests.per.connection": 5,
        "enable.idempotence":     true,   // å¹‚ç­‰æ€§ï¼Œé˜²æ­¢é‡å¤
        
        // æ€§èƒ½ä¼˜åŒ–
        "compression.type":       "lz4",
        "linger.ms":             10,      // æ‰¹é‡å‘é€å»¶è¿Ÿ
        "batch.size":            16384,   // æ‰¹é‡å¤§å°
        
        // é”™è¯¯å¤„ç†
        "retry.backoff.ms":      100,
        "request.timeout.ms":    30000,
    }
    
    p, err := kafka.NewProducer(config)
    if err != nil {
        return nil, fmt.Errorf("create producer failed: %w", err)
    }
    
    // åˆ›å»ºç”Ÿäº§è€…å®ä¾‹
    kp := &KafkaProducer{
        producer:     p,
        topic:        topic,
        deliveryChan: make(chan kafka.Event, 10000), // å¤§ç¼“å†²åŒº
        callbacks:    make(map[string]func(error)),
        metrics:      &ProducerMetrics{},
    }
    
    // å¯åŠ¨äº‹ä»¶å¤„ç†åç¨‹ï¼ˆå¤„ç†å¼‚æ­¥å‘é€ç»“æœï¼‰
    go kp.handleDeliveryReports()
    
    return kp, nil
}

// ProduceMessage çœŸæ­£çš„å¼‚æ­¥å‘é€æ¶ˆæ¯åˆ°Kafka
func (kp *KafkaProducer) ProduceMessage(msg *kefu.KFRecvMessage) error {
    // æ„å»ºæ¶ˆæ¯
    message := &ChatMessage{
        MessageID:      generateMessageID(),
        ExternalUserID: msg.ExternalUserID,
        OpenKFID:       msg.OpenKFID,
        MsgType:        msg.MsgType,
        Content:        msg,
        Timestamp:      time.Now().Unix(),
    }
    
    value, err := json.Marshal(message)
    if err != nil {
        return fmt.Errorf("marshal message failed: %w", err)
    }
    
    // å‘é€åˆ°Kafka
    kafkaMsg := &kafka.Message{
        TopicPartition: kafka.TopicPartition{
            Topic:     &kp.topic,
            Partition: kafka.PartitionAny,
        },
        Key:   []byte(msg.ExternalUserID),
        Value: value,
        Headers: []kafka.Header{
            {Key: "msg_type", Value: []byte(msg.MsgType)},
            {Key: "msg_id", Value: []byte(message.MessageID)},
            {Key: "timestamp", Value: []byte(fmt.Sprintf("%d", time.Now().Unix()))},
        },
    }
    
    // çœŸæ­£çš„å¼‚æ­¥å‘é€ - ä¸é˜»å¡ç­‰å¾…ï¼
    err = kp.producer.Produce(kafkaMsg, nil)  // nil è¡¨ç¤ºfire-and-forget
    if err != nil {
        // é˜Ÿåˆ—æ»¡ï¼Œéœ€è¦å¤„ç†èƒŒå‹
        if err.(kafka.Error).Code() == kafka.ErrQueueFull {
            // æ–¹æ¡ˆ1ï¼šç­‰å¾…é˜Ÿåˆ—æœ‰ç©ºé—´
            kp.producer.Flush(100) // ç­‰å¾…100ms
            
            // é‡è¯•ä¸€æ¬¡
            err = kp.producer.Produce(kafkaMsg, nil)
            if err != nil {
                kp.metrics.ProduceErrors.Inc()
                return fmt.Errorf("produce failed after retry: %w", err)
            }
        } else {
            return fmt.Errorf("produce failed: %w", err)
        }
    }
    
    kp.metrics.ProducedMessages.Inc()
    return nil
}

// handleDeliveryReports åå°å¤„ç†å‘é€ç»“æœ
func (kp *KafkaProducer) handleDeliveryReports() {
    for {
        select {
        case e := <-kp.producer.Events():
            switch ev := e.(type) {
            case *kafka.Message:
                // å¤„ç†å‘é€ç»“æœ
                if ev.TopicPartition.Error != nil {
                    kp.metrics.ProduceErrors.Inc()
                    log.GetInstance().Sugar.Error("Delivery failed: ", 
                        ev.TopicPartition.Error)
                    
                    // å¦‚æœæœ‰å›è°ƒï¼Œæ‰§è¡Œå›è°ƒ
                    msgID := kp.extractMessageID(ev)
                    if callback, ok := kp.callbacks[msgID]; ok {
                        callback(ev.TopicPartition.Error)
                        delete(kp.callbacks, msgID)
                    }
                } else {
                    kp.metrics.ProducedMessages.Inc()
                    kp.metrics.ProduceLatency.Observe(time.Since(ev.Timestamp).Seconds())
                    
                    log.GetInstance().Sugar.Debug("Message delivered to ", 
                        ev.TopicPartition)
                    
                    // æˆåŠŸå›è°ƒ
                    msgID := kp.extractMessageID(ev)
                    if callback, ok := kp.callbacks[msgID]; ok {
                        callback(nil)
                        delete(kp.callbacks, msgID)
                    }
                }
                
            case kafka.Error:
                kp.metrics.ProduceErrors.Inc()
                log.GetInstance().Sugar.Error("Kafka error: ", ev)
            }
            
        case <-kp.stopChan:
            return
        }
    }
}

// ProduceMessageWithCallback éœ€è¦ç¡®è®¤çš„åœºæ™¯ä½¿ç”¨å›è°ƒ
func (kp *KafkaProducer) ProduceMessageWithCallback(msg *kefu.KFRecvMessage, 
    callback func(err error)) error {
    // æ„å»ºæ¶ˆæ¯ï¼ˆé€»è¾‘åŒ ProduceMessageï¼‰
    message := &ChatMessage{
        MessageID:      generateMessageID(),
        ExternalUserID: msg.ExternalUserID,
        // ... å…¶ä»–å­—æ®µ
    }
    
    // æ³¨å†Œå›è°ƒ
    kp.mu.Lock()
    kp.callbacks[message.MessageID] = callback
    kp.mu.Unlock()
    
    // å‘é€æ¶ˆæ¯ï¼ˆä½¿ç”¨Events channelï¼‰
    err = kp.producer.Produce(kafkaMsg, nil) // ä¾ç„¶æ˜¯å¼‚æ­¥ï¼
    if err != nil {
        // æ¸…ç†å›è°ƒ
        kp.mu.Lock()
        delete(kp.callbacks, message.MessageID)
        kp.mu.Unlock()
        return fmt.Errorf("produce failed: %w", err)
    }
    
    return nil
}

// Close å…³é—­ç”Ÿäº§è€…
func (kp *KafkaProducer) Close() {
    kp.producer.Flush(10000) // ç­‰å¾…10ç§’ï¼Œå‘é€å‰©ä½™æ¶ˆæ¯
    kp.producer.Close()
}
```

### 3.2 å¼‚æ­¥å‘é€æ€§èƒ½åˆ†æ

#### åŒæ­¥ vs å¼‚æ­¥æ€§èƒ½å¯¹æ¯”
```
åŒæ­¥å‘é€ï¼ˆé”™è¯¯è®¾è®¡ï¼‰ï¼š
- æ¯æ¡æ¶ˆæ¯ç­‰å¾…ç¡®è®¤ï¼š~20ms
- TPSä¸Šé™ï¼š50æ¡/ç§’/çº¿ç¨‹
- CPUåˆ©ç”¨ç‡ï¼š< 5%ï¼ˆå¤§éƒ¨åˆ†æ—¶é—´åœ¨ç­‰å¾…ï¼‰
- æ‰¹é‡ä¼˜åŒ–ï¼šå®Œå…¨å¤±æ•ˆ

å¼‚æ­¥å‘é€ï¼ˆæ­£ç¡®è®¾è®¡ï¼‰ï¼š
- æ¶ˆæ¯ç«‹å³è¿”å›ï¼š< 1ms
- TPSï¼š10,000+æ¡/ç§’/çº¿ç¨‹
- CPUåˆ©ç”¨ç‡ï¼š~60%ï¼ˆå……åˆ†åˆ©ç”¨ï¼‰
- æ‰¹é‡ä¼˜åŒ–ï¼šå……åˆ†å‘æŒ¥ä½œç”¨
```

#### ä¸‰ç§å‘é€æ¨¡å¼é€‰æ‹©
```go
// æ¨¡å¼1ï¼šFire-and-forgetï¼ˆæœ€é«˜æ€§èƒ½ï¼‰
// é€‚ç”¨ï¼šæ—¥å¿—ã€ç›‘æ§æ•°æ®ç­‰å…è®¸å°‘é‡ä¸¢å¤±çš„åœºæ™¯
producer.Produce(msg, nil)

// æ¨¡å¼2ï¼šå¼‚æ­¥å›è°ƒï¼ˆå¹³è¡¡æ€§èƒ½å’Œå¯é æ€§ï¼‰
// é€‚ç”¨ï¼šéœ€è¦ç¡®è®¤ä½†ä¸é˜»å¡ä¸»æµç¨‹çš„åœºæ™¯
producer.Produce(msg, nil) + handleDeliveryReports()

// æ¨¡å¼3ï¼šåŒæ­¥ç¡®è®¤ï¼ˆæœ€ä½æ€§èƒ½ï¼Œæœ€é«˜å¯é æ€§ï¼‰
// é€‚ç”¨ï¼šé‡‘èäº¤æ˜“ç­‰ç»å¯¹ä¸èƒ½ä¸¢å¤±çš„åœºæ™¯
deliveryChan := make(chan kafka.Event, 1)
producer.Produce(msg, deliveryChan)
<-deliveryChan  // ä»…åœ¨ç‰¹æ®Šåœºæ™¯ä½¿ç”¨ï¼
```

### 3.3 ç”Ÿäº§è€…é›†æˆåˆ°KFæ¨¡å—

#### æ­£å¸¸å‘é€æµç¨‹
```go
// åœ¨ kf.go çš„ doHandleEvent æ–¹æ³•ä¸­
func (s *KFService) doHandleEvent(event *kefu.KFCallbackMessage, cm *cursorManager) error {
    // ... å‰é¢çš„åŒæ­¥é€»è¾‘ä¿æŒä¸å˜ ...
    
    // è·å–æ¶ˆæ¯åï¼Œå‘é€åˆ°Kafka
    for _, msg := range resp.MsgList {
        msgCopy := msg
        
        // å‘é€åˆ°Kafka
        if err := s.kafkaProducer.ProduceMessage(&msgCopy); err != nil {
            log.GetInstance().Sugar.Error("Send to Kafka failed: ", err)
            
            // é™çº§å¤„ç†ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼Œæœ‰ç¼ºé™·ï¼‰
            if err := s.handleKafkaFailure(&msgCopy); err != nil {
                log.GetInstance().Sugar.Error("Fallback also failed: ", err)
            }
        }
    }
    
    // ... cursoræ›´æ–°é€»è¾‘ ...
}
```

#### é™çº§ç­–ç•¥ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰

**âš ï¸ å½“å‰é™çº§æ–¹æ¡ˆçš„é—®é¢˜**ï¼š
1. **æ¶ˆæ¯ä¹±åºé£é™©**ï¼šé™çº§æ¶ˆæ¯èµ°æœ¬åœ°ï¼Œåç»­æ¶ˆæ¯èµ°Kafkaï¼Œå¯èƒ½å¯¼è‡´åŒä¸€ç”¨æˆ·æ¶ˆæ¯ä¹±åº
2. **åˆ†å¸ƒå¼èƒ½åŠ›ä¸§å¤±**ï¼šé™çº§æ¶ˆæ¯åªèƒ½å•èŠ‚ç‚¹å¤„ç†
3. **æ— é‡è¯•æœºåˆ¶**ï¼šé™çº§æ˜¯æ°¸ä¹…çš„ï¼Œå³ä½¿Kafkaæ¢å¤ä¹Ÿä¸ä¼šé‡è¯•

```go
// ä¸´æ—¶é™çº§æ–¹æ¡ˆï¼ˆæœ‰ç¼ºé™·ï¼Œå¾…ä¼˜åŒ–ï¼‰
func (s *KFService) handleKafkaFailure(msg *kefu.KFRecvMessage) error {
    // TODO: å®ç°æŒä¹…åŒ–å¤±è´¥é˜Ÿåˆ—
    // 1. å°†æ¶ˆæ¯å†™å…¥æ•°æ®åº“çš„å¤±è´¥é˜Ÿåˆ—è¡¨
    // 2. è®°å½•å¤±è´¥æ—¶é—´ã€é‡è¯•æ¬¡æ•°ç­‰å…ƒä¿¡æ¯
    // 3. å¯åŠ¨ç‹¬ç«‹çš„è¡¥å¿ä»»åŠ¡å®šæœŸé‡è¯•
    
    // ä¸´æ—¶æ–¹æ¡ˆï¼šç›´æ¥é™çº§åˆ°æœ¬åœ°channelï¼ˆä¼šå¯¼è‡´æ¶ˆæ¯ä¹±åºï¼‰
    select {
    case s.processorChan <- msg:
        log.GetInstance().Sugar.Warn("Message degraded to local processing, may cause out-of-order: ", 
            msg.ExternalUserID)
        // TODO: è®°å½•é™çº§æŒ‡æ ‡ï¼Œè§¦å‘å‘Šè­¦
        return nil
    case <-time.After(1 * time.Second):
        return fmt.Errorf("local channel also full")
    }
}
```

#### ç†æƒ³çš„é™çº§æ–¹æ¡ˆï¼ˆTODOï¼‰

```go
// TODO: å¾…æŒä¹…å­˜å‚¨å±‚å®ç°åçš„ç†æƒ³æ–¹æ¡ˆ
type FailedMessage struct {
    ID            int64
    MessageID     string
    UserID        string
    Content       []byte  // åºåˆ—åŒ–çš„æ¶ˆæ¯
    FailedAt      time.Time
    RetryCount    int
    LastError     string
    Status        string  // pending/retrying/success/failed
}

// æŒä¹…åŒ–å¤±è´¥æ¶ˆæ¯
func (s *KFService) persistFailedMessage(msg *kefu.KFRecvMessage) error {
    // TODO: å®ç°
    // 1. åºåˆ—åŒ–æ¶ˆæ¯
    // 2. å†™å…¥æ•°æ®åº“
    // 3. è¿”å›æˆåŠŸ/å¤±è´¥
    return nil
}

// è¡¥å¿ä»»åŠ¡ï¼ˆç‹¬ç«‹åç¨‹ï¼‰
func (s *KFService) retryFailedMessages() {
    ticker := time.NewTicker(30 * time.Second)
    for range ticker.C {
        // TODO: å®ç°
        // 1. æŸ¥è¯¢å¾…é‡è¯•çš„æ¶ˆæ¯ï¼ˆæŒ‰ç”¨æˆ·åˆ†ç»„ï¼Œä¿è¯é¡ºåºï¼‰
        // 2. æŒ‰ç”¨æˆ·æ‰¹é‡é‡è¯•
        // 3. æ›´æ–°é‡è¯•çŠ¶æ€
        // 4. è¶…è¿‡é‡è¯•æ¬¡æ•°çš„è¿›å…¥æ­»ä¿¡é˜Ÿåˆ—
    }
}

// æ¶ˆæ¯é¡ºåºä¿è¯ç­–ç•¥
// 1. åŒä¸€ç”¨æˆ·çš„å¤±è´¥æ¶ˆæ¯å¿…é¡»æŒ‰é¡ºåºé‡è¯•
// 2. å¦‚æœç”¨æˆ·æœ‰å¤±è´¥æ¶ˆæ¯æœªé‡è¯•æˆåŠŸï¼Œæ–°æ¶ˆæ¯ä¹Ÿåº”è¯¥è¿›å…¥å¤±è´¥é˜Ÿåˆ—
// 3. åªæœ‰å½“ç”¨æˆ·çš„æ‰€æœ‰å¤±è´¥æ¶ˆæ¯éƒ½é‡è¯•æˆåŠŸåï¼Œæ‰èƒ½æ¢å¤æ­£å¸¸å‘é€
```

#### é™çº§æ–¹æ¡ˆçš„ç›‘æ§ä¸å‘Šè­¦

```go
// é™çº§ç›‘æ§æŒ‡æ ‡
type DegradationMetrics struct {
    DegradedMessages   counter  // é™çº§æ¶ˆæ¯æ€»æ•°
    DegradedUsers      gauge    // å—å½±å“çš„ç”¨æˆ·æ•°
    KafkaFailures      counter  // Kafkaå‘é€å¤±è´¥æ¬¡æ•°
    LastDegradationTime time.Time // æœ€è¿‘ä¸€æ¬¡é™çº§æ—¶é—´
}

// å‘Šè­¦è§„åˆ™
alerts:
  - name: KafkaProducerDegraded
    condition: degraded_messages > 10
    duration: 1m
    action: page
    message: "Kafka producer is degrading, messages may be out of order"
    
  - name: KafkaProducerDown
    condition: kafka_failures > 100
    duration: 5m
    action: critical
    message: "Kafka producer completely down, all messages degraded"
```

#### å®æ–½è·¯çº¿å›¾

**Phase 1ï¼ˆå½“å‰ï¼‰ï¼šåŸºç¡€é™çº§**
- âœ… å®ç°é™çº§åˆ°æœ¬åœ°channel
- âš ï¸ æ¥å—æ¶ˆæ¯å¯èƒ½ä¹±åºçš„é£é™©
- âš ï¸ æ·»åŠ æ˜æ˜¾çš„æ—¥å¿—è­¦å‘Š
- TODO: æ·»åŠ ç›‘æ§æŒ‡æ ‡

**Phase 2ï¼ˆçŸ­æœŸï¼‰ï¼šæŒä¹…åŒ–é˜Ÿåˆ—**
- TODO: å®ç°å¤±è´¥æ¶ˆæ¯æŒä¹…åŒ–è¡¨
- TODO: å®ç°æŒ‰ç”¨æˆ·çš„é¡ºåºé‡è¯•æœºåˆ¶
- TODO: æ·»åŠ æ­»ä¿¡é˜Ÿåˆ—å¤„ç†
- TODO: å®ç°è¡¥å¿ä»»åŠ¡è°ƒåº¦

**Phase 3ï¼ˆé•¿æœŸï¼‰ï¼šæ™ºèƒ½é™çº§**
- TODO: å®ç°ç†”æ–­å™¨æ¨¡å¼
- TODO: åŠ¨æ€è°ƒæ•´é‡è¯•ç­–ç•¥
- TODO: å®ç°å¤šçº§é™çº§ï¼ˆKafka â†’ æ•°æ®åº“ â†’ æœ¬åœ°ï¼‰
- TODO: æ”¯æŒæ‰‹åŠ¨é‡æ”¾å†å²æ¶ˆæ¯

### 3.4 æ­»ä¿¡é˜Ÿåˆ—ï¼ˆDead Letter Queueï¼‰è®¾è®¡

#### ä¸ºä»€ä¹ˆéœ€è¦DLQ

**é—®é¢˜è¯Šæ–­ï¼šæŠ•é€’ä¿è¯çš„é™çº§**
```
ç³»ç»Ÿæ‰¿è¯ºï¼šAt-least-onceï¼ˆè‡³å°‘ä¸€æ¬¡ï¼‰
å®é™…æƒ…å†µï¼š
âœ“ æ­£å¸¸è´Ÿè½½ï¼šä¿è¯è‡³å°‘ä¸€æ¬¡
âœ— é«˜è´Ÿè½½æ—¶ï¼šé™çº§ä¸ºBest-effort
âœ— é™æµ/ç†”æ–­ï¼šæ¶ˆæ¯ç›´æ¥ä¸¢å¼ƒ
âœ— å¤„ç†è¶…æ—¶ï¼šæ¶ˆæ¯æ°¸ä¹…ä¸¢å¤±

å®¢æœåœºæ™¯çš„ç‰¹æ®Šæ€§ï¼š
- æ¯æ¡æ¶ˆæ¯ä»·å€¼æé«˜ï¼ˆå®¢æˆ·æŠ•è¯‰ã€è®¢å•é—®é¢˜ï¼‰
- ä¸¢å¤±æ¶ˆæ¯ = ä¸¢å¤±å®¢æˆ·ä¿¡ä»»
- å¯èƒ½æ¶‰åŠæ³•å¾‹åˆè§„ï¼ˆå®¡è®¡è¦æ±‚ï¼‰
- éœ€è¦äº‹ååˆ†æï¼ˆä¸ºä»€ä¹ˆå¤±è´¥ï¼Ÿï¼‰
```

#### DLQæ¶ˆæ¯æ ¼å¼è®¾è®¡

```go
// DeadLetterMessage DLQæ¶ˆæ¯å®Œæ•´æ ¼å¼
type DeadLetterMessage struct {
    // åŸå§‹æ¶ˆæ¯ï¼ˆå®Œæ•´ä¿ç•™ï¼‰
    OriginalMessage  *kefu.KFRecvMessage `json:"original_message"`
    MessageID        string              `json:"message_id"`        // å”¯ä¸€æ ‡è¯†
    
    // å¤±è´¥ä¸Šä¸‹æ–‡
    FailureReason    FailureReason       `json:"failure_reason"`    // å¤±è´¥åŸå› æšä¸¾
    FailureDetails   string              `json:"failure_details"`   // è¯¦ç»†é”™è¯¯ä¿¡æ¯
    FailureTime      time.Time           `json:"failure_time"`      // å¤±è´¥æ—¶é—´
    ProcessorID      string              `json:"processor_id"`      // å¤„ç†å™¨æ ‡è¯†
    NodeID           string              `json:"node_id"`           // èŠ‚ç‚¹æ ‡è¯†
    
    // é‡è¯•ä¿¡æ¯
    RetryCount       int                 `json:"retry_count"`       // å·²é‡è¯•æ¬¡æ•°
    LastRetryTime    *time.Time          `json:"last_retry_time"`   // æœ€åé‡è¯•æ—¶é—´
    NextRetryTime    *time.Time          `json:"next_retry_time"`   // è®¡åˆ’é‡è¯•æ—¶é—´
    
    // Kafkaå…ƒæ•°æ®ï¼ˆç”¨äºè¿½è¸ªï¼‰
    SourceTopic      string              `json:"source_topic"`
    SourcePartition  int32               `json:"source_partition"`
    SourceOffset     int64               `json:"source_offset"`
    SourceTimestamp  time.Time           `json:"source_timestamp"`
    
    // ä¸šåŠ¡å…ƒæ•°æ®
    UserID           string              `json:"user_id"`
    UserGroup        string              `json:"user_group"`        // normal/vip/suspicious
    OpenKFID         string              `json:"open_kf_id"`        // å®¢æœè´¦å·
    SessionID        string              `json:"session_id"`        // ä¼šè¯ID
    
    // å¤„ç†ç­–ç•¥
    Recoverable      bool                `json:"recoverable"`       // æ˜¯å¦å¯æ¢å¤
    Priority         int                 `json:"priority"`          // æ¢å¤ä¼˜å…ˆçº§
    MaxRetries       int                 `json:"max_retries"`       // æœ€å¤§é‡è¯•æ¬¡æ•°
    TTL              time.Duration       `json:"ttl"`               // æ¶ˆæ¯æœ‰æ•ˆæœŸ
    
    // å¤„ç†å»ºè®®
    SuggestedAction  ActionType          `json:"suggested_action"`  // å»ºè®®æ“ä½œ
    ManualReview     bool                `json:"manual_review"`     // éœ€è¦äººå·¥å®¡æ ¸
}

// FailureReason å¤±è´¥åŸå› æšä¸¾
type FailureReason string

const (
    ReasonRateLimited      FailureReason = "rate_limited"       // é™æµ
    ReasonUserBlocked      FailureReason = "user_blocked"       // ç”¨æˆ·è¢«å°ç¦
    ReasonCircuitOpen      FailureReason = "circuit_open"       // ç†”æ–­å™¨æ‰“å¼€
    ReasonQueueFull        FailureReason = "queue_full"         // é˜Ÿåˆ—æ»¡
    ReasonProcessTimeout   FailureReason = "process_timeout"    // å¤„ç†è¶…æ—¶
    ReasonKafkaFailure     FailureReason = "kafka_failure"      // Kafkaæ•…éšœ
    ReasonInvalidMessage   FailureReason = "invalid_message"    // æ¶ˆæ¯æ ¼å¼é”™è¯¯
    ReasonDownstreamError  FailureReason = "downstream_error"   // ä¸‹æ¸¸æœåŠ¡é”™è¯¯
    ReasonSystemOverload   FailureReason = "system_overload"    // ç³»ç»Ÿè¿‡è½½
)

// ActionType å»ºè®®æ“ä½œç±»å‹
type ActionType string

const (
    ActionAutoRetry        ActionType = "auto_retry"         // è‡ªåŠ¨é‡è¯•
    ActionDelayedRetry     ActionType = "delayed_retry"      // å»¶è¿Ÿé‡è¯•
    ActionManualReview     ActionType = "manual_review"      // äººå·¥å®¡æ ¸
    ActionDiscard          ActionType = "discard"            // ä¸¢å¼ƒ
    ActionEscalate         ActionType = "escalate"           // å‡çº§å¤„ç†
)
```

#### DLQç”Ÿäº§è€…å®ç°

```go
// DLQProducer æ­»ä¿¡é˜Ÿåˆ—ç”Ÿäº§è€…
type DLQProducer struct {
    producer        *kafka.Producer
    topic           string              // DLQ Topicåç§°
    metrics         *DLQMetrics
    circuitBreaker  *CircuitBreaker     // DLQè‡ªå·±çš„ç†”æ–­å™¨
    mu              sync.Mutex
}

// NewDLQProducer åˆ›å»ºDLQç”Ÿäº§è€…
func NewDLQProducer(brokers string, topic string) (*DLQProducer, error) {
    config := &kafka.ConfigMap{
        "bootstrap.servers": brokers,
        
        // å¯é æ€§æœ€é«˜é…ç½®ï¼ˆDLQä¸èƒ½å†å¤±è´¥ï¼‰
        "acks":                   "all",
        "retries":                100,    // æ›´å¤šé‡è¯•
        "max.in.flight.requests.per.connection": 1,  // ä¿è¯é¡ºåº
        "enable.idempotence":     true,
        
        // æ€§èƒ½é…ç½®
        "compression.type":       "snappy",  // å¹³è¡¡å‹ç¼©ç‡å’ŒCPU
        "linger.ms":             50,         // ç¨é•¿çš„æ‰¹é‡æ—¶é—´
        "batch.size":            65536,      // æ›´å¤§çš„æ‰¹é‡
    }
    
    p, err := kafka.NewProducer(config)
    if err != nil {
        return nil, fmt.Errorf("create DLQ producer failed: %w", err)
    }
    
    dlq := &DLQProducer{
        producer: p,
        topic:    topic,
        metrics:  NewDLQMetrics(),
        circuitBreaker: &CircuitBreaker{
            failureThreshold: 10,
            halfOpenDelay:    1 * time.Minute,
        },
    }
    
    // å¯åŠ¨åå°äº‹ä»¶å¤„ç†
    go dlq.handleEvents()
    
    return dlq, nil
}

// SendToDeadLetter å‘é€æ¶ˆæ¯åˆ°æ­»ä¿¡é˜Ÿåˆ—
func (dlq *DLQProducer) SendToDeadLetter(
    msg *kefu.KFRecvMessage,
    reason FailureReason,
    details string,
    kafkaMetadata *KafkaMetadata,
) error {
    // æ£€æŸ¥DLQè‡ªå·±çš„ç†”æ–­å™¨
    if err := dlq.circuitBreaker.Allow(); err != nil {
        // DLQä¹Ÿå¤±è´¥äº†ï¼Œè¿™æ˜¯æœ€åçš„æƒ…å†µ
        // å†™å…¥æœ¬åœ°ç´§æ€¥æ–‡ä»¶
        dlq.writeToEmergencyFile(msg, reason, details)
        return fmt.Errorf("DLQ circuit breaker open: %w", err)
    }
    
    // æ„å»ºæ­»ä¿¡æ¶ˆæ¯
    deadLetter := &DeadLetterMessage{
        OriginalMessage: msg,
        MessageID:       generateMessageID(),
        FailureReason:   reason,
        FailureDetails:  details,
        FailureTime:     time.Now(),
        UserID:          msg.ExternalUserID,
        
        // è®¾ç½®æ¢å¤ç­–ç•¥
        Recoverable:     dlq.isRecoverable(reason),
        Priority:        dlq.calculatePriority(msg, reason),
        MaxRetries:      dlq.getMaxRetries(reason),
        TTL:             dlq.getTTL(reason),
        SuggestedAction: dlq.suggestAction(reason),
    }
    
    // æ·»åŠ Kafkaå…ƒæ•°æ®
    if kafkaMetadata != nil {
        deadLetter.SourceTopic = kafkaMetadata.Topic
        deadLetter.SourcePartition = kafkaMetadata.Partition
        deadLetter.SourceOffset = kafkaMetadata.Offset
    }
    
    // åºåˆ—åŒ–
    value, err := json.Marshal(deadLetter)
    if err != nil {
        dlq.metrics.SerializationErrors.Inc()
        return fmt.Errorf("marshal dead letter failed: %w", err)
    }
    
    // å‘é€åˆ°Kafkaï¼ˆå¼‚æ­¥ä½†è¦ç›‘æ§ç»“æœï¼‰
    err = dlq.producer.Produce(&kafka.Message{
        TopicPartition: kafka.TopicPartition{
            Topic:     &dlq.topic,
            Partition: kafka.PartitionAny,
        },
        Key:   []byte(msg.ExternalUserID),
        Value: value,
        Headers: []kafka.Header{
            {Key: "failure_reason", Value: []byte(reason)},
            {Key: "retry_count", Value: []byte("0")},
            {Key: "message_id", Value: []byte(deadLetter.MessageID)},
        },
    }, nil)
    
    if err != nil {
        dlq.circuitBreaker.OnFailure()
        dlq.metrics.ProduceErrors.Inc()
        
        // æœ€åçš„ä¿é™©ï¼šå†™å…¥æœ¬åœ°æ–‡ä»¶
        dlq.writeToEmergencyFile(msg, reason, details)
        return fmt.Errorf("produce to DLQ failed: %w", err)
    }
    
    dlq.circuitBreaker.OnSuccess()
    dlq.metrics.MessagesProduced.Inc()
    dlq.metrics.MessagesByReason[reason].Inc()
    
    log.GetInstance().Sugar.Warn("Message sent to DLQ",
        ", user: ", msg.ExternalUserID,
        ", reason: ", reason,
        ", message_id: ", deadLetter.MessageID)
    
    return nil
}

// writeToEmergencyFile ç´§æ€¥æƒ…å†µä¸‹å†™å…¥æœ¬åœ°æ–‡ä»¶
func (dlq *DLQProducer) writeToEmergencyFile(
    msg *kefu.KFRecvMessage,
    reason FailureReason,
    details string,
) {
    // å†™å…¥ç´§æ€¥å¤‡ä»½æ–‡ä»¶ï¼Œç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±
    emergencyFile := fmt.Sprintf("/var/log/qywx/dlq_emergency_%s.jsonl",
        time.Now().Format("20060102"))
    
    data := map[string]interface{}{
        "timestamp": time.Now().Unix(),
        "message":   msg,
        "reason":    reason,
        "details":   details,
    }
    
    jsonData, _ := json.Marshal(data)
    
    // è¿½åŠ å†™å…¥ï¼Œå¸¦é”
    dlq.mu.Lock()
    defer dlq.mu.Unlock()
    
    file, err := os.OpenFile(emergencyFile, 
        os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644)
    if err != nil {
        // çœŸçš„æ²¡åŠæ³•äº†ï¼Œåªèƒ½æ‰“å°åˆ°stderr
        fmt.Fprintf(os.Stderr, "EMERGENCY: Failed to write DLQ: %s\n", jsonData)
        return
    }
    defer file.Close()
    
    file.WriteString(string(jsonData) + "\n")
    
    dlq.metrics.EmergencyWrites.Inc()
}

// ç­–ç•¥æ–¹æ³•
func (dlq *DLQProducer) isRecoverable(reason FailureReason) bool {
    switch reason {
    case ReasonRateLimited, ReasonUserBlocked, ReasonCircuitOpen,
         ReasonQueueFull, ReasonProcessTimeout, ReasonSystemOverload:
        return true  // è¿™äº›éƒ½æ˜¯æš‚æ—¶æ€§é—®é¢˜ï¼Œå¯æ¢å¤
    case ReasonInvalidMessage:
        return false // æ¶ˆæ¯æœ¬èº«æœ‰é—®é¢˜ï¼Œä¸å¯æ¢å¤
    default:
        return true  // é»˜è®¤è®¤ä¸ºå¯æ¢å¤
    }
}

func (dlq *DLQProducer) calculatePriority(msg *kefu.KFRecvMessage, reason FailureReason) int {
    priority := 5  // é»˜è®¤ä¸­ç­‰ä¼˜å…ˆçº§
    
    // VIPç”¨æˆ·æé«˜ä¼˜å…ˆçº§
    if isVIPUser(msg.ExternalUserID) {
        priority += 3
    }
    
    // æŸäº›å¤±è´¥åŸå› é™ä½ä¼˜å…ˆçº§
    switch reason {
    case ReasonUserBlocked:
        priority -= 3  // è¢«å°ç”¨æˆ·ä½ä¼˜å…ˆçº§
    case ReasonRateLimited:
        priority -= 1  // é™æµç¨å¾®é™ä½
    }
    
    // é™åˆ¶åœ¨1-10èŒƒå›´
    if priority < 1 {
        priority = 1
    }
    if priority > 10 {
        priority = 10
    }
    
    return priority
}
```

#### DLQæ¶ˆè´¹è€…ä¸æ¢å¤ç­–ç•¥

```go
// DLQConsumer DLQæ¶ˆè´¹è€…ï¼ˆç‹¬ç«‹æœåŠ¡ï¼‰
type DLQConsumer struct {
    consumer        *kafka.Consumer
    strategies      map[FailureReason]RecoveryStrategy
    retryProducer   *kafka.Producer  // ç”¨äºé‡æ–°å‘é€åˆ°ä¸»Topic
    metrics         *DLQConsumerMetrics
    ctx             context.Context
    wg              sync.WaitGroup
}

// RecoveryStrategy æ¢å¤ç­–ç•¥æ¥å£
type RecoveryStrategy interface {
    CanRecover(msg *DeadLetterMessage) bool
    CalculateRetryDelay(msg *DeadLetterMessage) time.Duration
    PrepareRetry(msg *DeadLetterMessage) (*kefu.KFRecvMessage, error)
    OnSuccess(msg *DeadLetterMessage)
    OnFailure(msg *DeadLetterMessage, err error)
}

// å…·ä½“æ¢å¤ç­–ç•¥å®ç°
type RateLimitRecovery struct{}

func (r *RateLimitRecovery) CanRecover(msg *DeadLetterMessage) bool {
    // æ£€æŸ¥ç”¨æˆ·æ˜¯å¦è¿˜åœ¨å°ç¦æœŸ
    if isUserBlocked(msg.UserID) {
        return false
    }
    // æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
    return msg.RetryCount < msg.MaxRetries
}

func (r *RateLimitRecovery) CalculateRetryDelay(msg *DeadLetterMessage) time.Duration {
    // æŒ‡æ•°é€€é¿ï¼š1åˆ†é’Ÿã€2åˆ†é’Ÿã€4åˆ†é’Ÿ...
    delay := time.Duration(math.Pow(2, float64(msg.RetryCount))) * time.Minute
    if delay > 1*time.Hour {
        delay = 1 * time.Hour  // æœ€å¤š1å°æ—¶
    }
    return delay
}

// DLQæ¶ˆè´¹ä¸»å¾ªç¯
func (dc *DLQConsumer) Start() {
    dc.wg.Add(1)
    go dc.consumeLoop()
    
    dc.wg.Add(1)
    go dc.retryScheduler()  // å®šæ—¶é‡è¯•è°ƒåº¦å™¨
}

func (dc *DLQConsumer) consumeLoop() {
    defer dc.wg.Done()
    
    for {
        select {
        case <-dc.ctx.Done():
            return
            
        default:
            msg, err := dc.consumer.ReadMessage(100 * time.Millisecond)
            if err != nil {
                continue
            }
            
            var deadLetter DeadLetterMessage
            if err := json.Unmarshal(msg.Value, &deadLetter); err != nil {
                log.GetInstance().Sugar.Error("Unmarshal DLQ message failed: ", err)
                dc.consumer.CommitMessage(msg)
                continue
            }
            
            // å¤„ç†æ­»ä¿¡æ¶ˆæ¯
            dc.processDeadLetter(&deadLetter)
            
            // æäº¤offset
            dc.consumer.CommitMessage(msg)
        }
    }
}

func (dc *DLQConsumer) processDeadLetter(msg *DeadLetterMessage) {
    // è·å–å¯¹åº”çš„æ¢å¤ç­–ç•¥
    strategy, exists := dc.strategies[msg.FailureReason]
    if !exists {
        log.GetInstance().Sugar.Warn("No recovery strategy for reason: ", msg.FailureReason)
        dc.metrics.NoStrategyMessages.Inc()
        return
    }
    
    // æ£€æŸ¥æ˜¯å¦å¯æ¢å¤
    if !strategy.CanRecover(msg) {
        if msg.RetryCount >= msg.MaxRetries {
            // è¶…è¿‡æœ€å¤§é‡è¯•ï¼Œéœ€è¦äººå·¥ä»‹å…¥
            dc.escalateToManual(msg)
        }
        return
    }
    
    // è®¡ç®—é‡è¯•å»¶è¿Ÿ
    delay := strategy.CalculateRetryDelay(msg)
    
    // å¦‚æœéœ€è¦å»¶è¿Ÿï¼ŒåŠ å…¥è°ƒåº¦é˜Ÿåˆ—
    if delay > 0 {
        msg.NextRetryTime = ptrTime(time.Now().Add(delay))
        dc.scheduleRetry(msg, delay)
        return
    }
    
    // ç«‹å³é‡è¯•
    dc.retryMessage(msg, strategy)
}

// retryMessage é‡è¯•æ¶ˆæ¯
func (dc *DLQConsumer) retryMessage(msg *DeadLetterMessage, strategy RecoveryStrategy) {
    // å‡†å¤‡é‡è¯•æ¶ˆæ¯
    retryMsg, err := strategy.PrepareRetry(msg)
    if err != nil {
        log.GetInstance().Sugar.Error("Prepare retry failed: ", err)
        strategy.OnFailure(msg, err)
        return
    }
    
    // å‘é€å›ä¸»Topic
    err = dc.retryProducer.Produce(&kafka.Message{
        TopicPartition: kafka.TopicPartition{
            Topic:     &mainTopic,
            Partition: kafka.PartitionAny,
        },
        Key:   []byte(retryMsg.ExternalUserID),
        Value: jsonMarshal(retryMsg),
        Headers: []kafka.Header{
            {Key: "retry_from_dlq", Value: []byte("true")},
            {Key: "retry_count", Value: []byte(fmt.Sprintf("%d", msg.RetryCount+1))},
            {Key: "original_failure", Value: []byte(string(msg.FailureReason))},
        },
    }, nil)
    
    if err != nil {
        // é‡è¯•å¤±è´¥ï¼Œæ›´æ–°DLQæ¶ˆæ¯
        msg.RetryCount++
        msg.LastRetryTime = ptrTime(time.Now())
        strategy.OnFailure(msg, err)
        dc.metrics.RetryFailures.Inc()
        
        // é‡æ–°å‘é€åˆ°DLQï¼ˆæ›´æ–°åçš„ç‰ˆæœ¬ï¼‰
        dc.updateDLQMessage(msg)
    } else {
        strategy.OnSuccess(msg)
        dc.metrics.RetrySuccesses.Inc()
        
        log.GetInstance().Sugar.Info("Message recovered from DLQ",
            ", user: ", msg.UserID,
            ", reason: ", msg.FailureReason,
            ", retry: ", msg.RetryCount+1)
    }
}
```

#### ç›‘æ§ä¸å‘Šè­¦

```yaml
# DLQç›‘æ§æŒ‡æ ‡
dlq_metrics:
  # ç”Ÿäº§è€…æŒ‡æ ‡
  - name: dlq_messages_produced_total
    description: å‘é€åˆ°DLQçš„æ¶ˆæ¯æ€»æ•°
    alert_threshold: 100/hour
    
  - name: dlq_messages_by_reason
    description: æŒ‰å¤±è´¥åŸå› åˆ†ç±»çš„æ¶ˆæ¯æ•°
    labels: [reason]
    
  - name: dlq_emergency_writes_total
    description: ç´§æ€¥å†™å…¥æœ¬åœ°æ–‡ä»¶æ¬¡æ•°
    alert_threshold: 1  # ä»»ä½•ç´§æ€¥å†™å…¥éƒ½åº”å‘Šè­¦
    
  # æ¶ˆè´¹è€…æŒ‡æ ‡
  - name: dlq_messages_recovered_total
    description: æˆåŠŸæ¢å¤çš„æ¶ˆæ¯æ•°
    
  - name: dlq_messages_unrecoverable_total
    description: ä¸å¯æ¢å¤çš„æ¶ˆæ¯æ•°
    alert_threshold: 10/day
    
  - name: dlq_consumer_lag
    description: DLQæ¶ˆè´¹å»¶è¿Ÿ
    alert_threshold: 1000
    
  - name: dlq_manual_review_pending
    description: å¾…äººå·¥å®¡æ ¸çš„æ¶ˆæ¯æ•°
    alert_threshold: 100

# DLQå‘Šè­¦è§„åˆ™
dlq_alerts:
  - level: WARNING
    condition: DLQæ¶ˆæ¯å¢é•¿é€Ÿç‡ > æ­£å¸¸å€¼3å€
    action: æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æœ‰å¼‚å¸¸
    
  - level: CRITICAL
    condition: DLQç´§æ€¥æ–‡ä»¶å†™å…¥
    action: ç«‹å³æ£€æŸ¥Kafkaé›†ç¾¤çŠ¶æ€
    
  - level: CRITICAL
    condition: DLQæ¶ˆè´¹è€…åœæ­¢
    action: ç«‹å³é‡å¯DLQæ¶ˆè´¹è€…
    
  - level: P1
    condition: ä¸å¯æ¢å¤æ¶ˆæ¯ > 100
    action: äººå·¥ä»‹å…¥å¤„ç†
```

#### ç®¡ç†ç•Œé¢

```go
// DLQManager DLQç®¡ç†æ¥å£
type DLQManager interface {
    // æŸ¥è¯¢DLQçŠ¶æ€
    GetStats() (*DLQStats, error)
    
    // æŸ¥è¯¢ç‰¹å®šç”¨æˆ·çš„æ­»ä¿¡æ¶ˆæ¯
    GetUserMessages(userID string) ([]*DeadLetterMessage, error)
    
    // æ‰‹åŠ¨é‡è¯•æ¶ˆæ¯
    ManualRetry(messageID string) error
    
    // æ‰¹é‡é‡è¯•
    BatchRetry(filter DLQFilter) (int, error)
    
    // æ¸…ç†è¿‡æœŸæ¶ˆæ¯
    PurgeExpired() (int, error)
    
    // å¯¼å‡ºæ¶ˆæ¯ï¼ˆç”¨äºåˆ†æï¼‰
    Export(filter DLQFilter, format ExportFormat) ([]byte, error)
}

// ç®¡ç†API
POST /admin/dlq/retry/{messageID}      # æ‰‹åŠ¨é‡è¯•å•æ¡æ¶ˆæ¯
POST /admin/dlq/batch-retry            # æ‰¹é‡é‡è¯•
GET  /admin/dlq/stats                  # è·å–DLQç»Ÿè®¡
GET  /admin/dlq/messages?user={userID} # æŸ¥è¯¢ç”¨æˆ·çš„æ­»ä¿¡æ¶ˆæ¯
POST /admin/dlq/purge                  # æ¸…ç†è¿‡æœŸæ¶ˆæ¯
GET  /admin/dlq/export?format=csv      # å¯¼å‡ºDLQæ•°æ®
```

### 3.5 å¤šç”Ÿäº§è€…åœºæ™¯ä¸‹çš„åˆ†å¸ƒå¼åè°ƒ

#### 3.5.1 ä¸ºä»€ä¹ˆéœ€è¦åˆ†å¸ƒå¼é”

åœ¨å¤šç”Ÿäº§è€…å®ä¾‹åœºæ™¯ä¸‹ï¼Œéœ€è¦é¿å…ï¼š
- **é‡å¤æ‹‰å–**ï¼šå¤šä¸ªKFå®ä¾‹åŒæ—¶ä»å¾®ä¿¡æœåŠ¡å™¨æ‹‰å–åŒä¸€æ‰¹æ¶ˆæ¯
- **é‡å¤å¤„ç†**ï¼šåŒä¸€ä¸ªäº‹ä»¶è¢«å¤šä¸ªå®ä¾‹å¹¶å‘å¤„ç†
- **èµ„æºç«äº‰**ï¼šå¤šä¸ªå®ä¾‹åŒæ—¶æ›´æ–°åŒä¸€ä¸ªcursor

#### 3.5.2 å¼€æºç¤¾åŒºæˆç†Ÿæ–¹æ¡ˆå¯¹æ¯”

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **Redis + Redlock** | â€¢ æ€§èƒ½é«˜ï¼ˆ4x fasterï¼‰<br>â€¢ éƒ¨ç½²ç®€å•<br>â€¢ å·²æœ‰Rediså¯å¤ç”¨ | â€¢ æ—¶é’Ÿè·³å˜é£é™©<br>â€¢ ç½‘ç»œåˆ†åŒºé—®é¢˜<br>â€¢ æ— fencing token | æ•ˆç‡ä¼˜åŒ–åœºæ™¯ |
| **ZooKeeper** | â€¢ å¼ºä¸€è‡´æ€§ï¼ˆCPï¼‰<br>â€¢ æœ‰fencing token<br>â€¢ æ”¯æŒé˜»å¡ç­‰å¾… | â€¢ éƒ¨ç½²å¤æ‚<br>â€¢ æ€§èƒ½è¾ƒä½<br>â€¢ å­¦ä¹ æ›²çº¿é™¡ | æ­£ç¡®æ€§å…³é”®åœºæ™¯ |
| **etcd** | â€¢ Raftå…±è¯†<br>â€¢ äº‘åŸç”Ÿå‹å¥½<br>â€¢ K8sç”Ÿæ€é›†æˆ | â€¢ ç›¸å¯¹è¾ƒé‡<br>â€¢ éœ€è¦é›†ç¾¤ | K8sç¯å¢ƒ |
| **Consul** | â€¢ å¥åº·æ£€æŸ¥<br>â€¢ æœåŠ¡å‘ç°é›†æˆ<br>â€¢ Sessionæœºåˆ¶ | â€¢ é¢å¤–ç»„ä»¶<br>â€¢ å¤æ‚åº¦é«˜ | å¾®æœåŠ¡æ¶æ„ |

#### 3.5.3 æœ€ç»ˆæ–¹æ¡ˆï¼šRedis + go-redsyncï¼ˆå•å®ä¾‹â†’Sentinelï¼‰

**âœ… æ–¹æ¡ˆç¡®å®šï¼šç»Ÿä¸€ä½¿ç”¨Rediså•å®ä¾‹å’ŒSentinelï¼Œä¸è€ƒè™‘Cluster**

åŸºäºæ·±å…¥ç ”ç©¶å’Œcontext7æ–‡æ¡£éªŒè¯ï¼Œç¡®å®šé‡‡ç”¨ä»¥ä¸‹æ–¹æ¡ˆï¼š

- âœ… **å¼€å‘ç¯å¢ƒ**ï¼šRediså•å®ä¾‹ï¼ˆç®€å•é«˜æ•ˆï¼‰
- âœ… **ç”Ÿäº§ç¯å¢ƒ**ï¼šRedis Sentinelï¼ˆé«˜å¯ç”¨ä¸»ä»ï¼‰
- âŒ **ä¸ä½¿ç”¨**ï¼šRedis Clusterï¼ˆä¸é€‚åˆåˆ†å¸ƒå¼é”åœºæ™¯ï¼‰
- ğŸ“ **æ ¸å¿ƒä¼˜åŠ¿**ï¼šåŒä¸€å¥—ä»£ç å®Œç¾æ”¯æŒä¸¤ç§ç¯å¢ƒ

**å†³ç­–ä¾æ®**ï¼š
1. **go-redsyncå®˜æ–¹æ”¯æŒ**ï¼šæ˜ç¡®æ”¯æŒå•å®ä¾‹å’ŒSentinelæ¨¡å¼
2. **ä»£ç é›¶æ”¹åŠ¨**ï¼šé€šè¿‡go-redisçš„UniversalClientæ¥å£è‡ªåŠ¨é€‚é…
3. **å¹³æ»‘å‡çº§**ï¼šä»å¼€å‘åˆ°ç”Ÿäº§ä»…éœ€ä¿®æ”¹é…ç½®æ–‡ä»¶
4. **Redis Sentinelä¼˜åŠ¿**ï¼š
   - è‡ªåŠ¨æ•…éšœè½¬ç§»ï¼ˆç§’çº§ï¼‰
   - ä¸»ä»å¤åˆ¶ä¿è¯æ•°æ®å®‰å…¨
   - æ‰€æœ‰æ•°æ®åœ¨ä¸€ä¸ªä¸»èŠ‚ç‚¹ï¼Œé€‚åˆåˆ†å¸ƒå¼é”
5. **é¿å…Clusterå¤æ‚æ€§**ï¼š
   - Clusterè®¾è®¡ç”¨äºæ•°æ®åˆ†ç‰‡ï¼Œä¸é€‚åˆé”åœºæ™¯
   - Redlockç®—æ³•éœ€è¦ç‹¬ç«‹Rediså®ä¾‹ï¼Œéåˆ†ç‰‡é›†ç¾¤

### ç»Ÿä¸€å®ç°ä»£ç ï¼ˆæ”¯æŒå•å®ä¾‹å’ŒSentinelï¼‰

```go
import (
    "github.com/go-redsync/redsync/v4"
    "github.com/go-redsync/redsync/v4/redis/goredis/v9"
    goredislib "github.com/redis/go-redis/v9"
)

// RedisLockManager Redisåˆ†å¸ƒå¼é”ç®¡ç†å™¨
type RedisLockManager struct {
    redSync *redsync.Redsync
    client  goredislib.UniversalClient
}

// NewRedisLockManager åˆ›å»ºé”ç®¡ç†å™¨ï¼ˆæ”¯æŒå•å®ä¾‹å’Œå“¨å…µæ¨¡å¼ï¼‰
func NewRedisLockManager(cfg *config.RedisConfig) (*RedisLockManager, error) {
    var client goredislib.UniversalClient
    
    switch cfg.Mode {
    case "single":
        // å¼€å‘ç¯å¢ƒï¼šå•å®ä¾‹
        client = goredislib.NewClient(&goredislib.Options{
            Addr:     cfg.Addr,
            Password: cfg.Password,
            DB:       cfg.DB,
        })
        
    case "sentinel":
        // ç”Ÿäº§ç¯å¢ƒï¼šå“¨å…µæ¨¡å¼ï¼ˆé«˜å¯ç”¨ï¼‰
        client = goredislib.NewFailoverClient(&goredislib.FailoverOptions{
            MasterName:    cfg.MasterName,
            SentinelAddrs: cfg.SentinelAddrs,
            Password:      cfg.Password,
            DB:            cfg.DB,
        })
        
    default:
        return nil, fmt.Errorf("unsupported redis mode: %s, use 'single' or 'sentinel'", cfg.Mode)
    
    // æµ‹è¯•è¿æ¥
    ctx := context.Background()
    if err := client.Ping(ctx).Err(); err != nil {
        return nil, fmt.Errorf("redis connection failed: %w", err)
    }
    
    // åˆ›å»ºredsyncå®ä¾‹
    pool := goredis.NewPool(client)
    rs := redsync.New(pool)
    
    return &RedisLockManager{
        redSync: rs,
        client:  client,
    }, nil
}

// AcquireLock è·å–åˆ†å¸ƒå¼é”
func (m *RedisLockManager) AcquireLock(key string, ttl time.Duration) (*redsync.Mutex, error) {
    mutex := m.redSync.NewMutex(
        key,
        redsync.WithExpiry(ttl),
        redsync.WithTries(1),        // éé˜»å¡ï¼Œå¿«é€Ÿå¤±è´¥
        redsync.WithRetryDelay(100*time.Millisecond),
    )
    
    if err := mutex.Lock(); err != nil {
        return nil, err // é”å·²è¢«å ç”¨
    }
    
    return mutex, nil
}

// åœ¨KFæœåŠ¡ä¸­ä½¿ç”¨
func (s *KFService) syncWithLock(openKFID string) error {
    lockKey := fmt.Sprintf("sync:lock:%s", openKFID)
    
    // è·å–é”ï¼ˆæ”¯æŒå•å®ä¾‹ã€å“¨å…µï¼‰
    mutex, err := s.lockManager.AcquireLock(lockKey, 30*time.Second)
    if err != nil {
        log.Debug("Another node is syncing for ", openKFID)
        return nil // å…¶ä»–èŠ‚ç‚¹æ­£åœ¨å¤„ç†
    }
    defer mutex.Unlock()
    
    // æ‰§è¡ŒåŒæ­¥
    return s.doHandleEvent(...)
}
```

#### 3.5.4 é…ç½®ç¤ºä¾‹ï¼ˆæ”¯æŒå¹³æ»‘å‡çº§ï¼‰

```yaml
# config/development.yaml - å¼€å‘ç¯å¢ƒ
distributed_lock:
  enabled: false  # å•KFå®ä¾‹å¯å…³é—­
  redis:
    mode: single
    addr: localhost:6379
    db: 0
    
# config/staging.yaml - é¢„å‘ç¯å¢ƒ
distributed_lock:
  enabled: true
  redis:
    mode: sentinel  # å“¨å…µæ¨¡å¼
    master_name: mymaster
    sentinel_addrs:
      - sentinel1:26379
      - sentinel2:26379
      - sentinel3:26379
    db: 0
    password: ""
    
# config/production.yaml - ç”Ÿäº§ç¯å¢ƒ
distributed_lock:
  enabled: true
  redis:
    mode: sentinel  # å“¨å…µæ¨¡å¼ï¼ˆé«˜å¯ç”¨ï¼‰
    master_name: mymaster
    sentinel_addrs:
      - sentinel1:26379
      - sentinel2:26379
      - sentinel3:26379
    db: 0
    password: ""
```

#### 3.5.5 ç”Ÿäº§éƒ¨ç½²å»ºè®®

**ç¯å¢ƒæ¼”è¿›è·¯å¾„**ï¼š
```
å¼€å‘ç¯å¢ƒ          â†’     é¢„å‘ç¯å¢ƒ        â†’     ç”Ÿäº§ç¯å¢ƒ
Rediså•å®ä¾‹             Redis Sentinel        Redis Sentinel
(æ— éœ€åˆ†å¸ƒå¼é”)          (3èŠ‚ç‚¹é«˜å¯ç”¨)         (3èŠ‚ç‚¹é«˜å¯ç”¨)
```

**å…³é”®ä¼˜åŠ¿**ï¼š
1. **ä»£ç æ— éœ€ä¿®æ”¹**ï¼šåŒä¸€å¥—ä»£ç æ”¯æŒå•å®ä¾‹å’ŒSentinel
2. **é…ç½®é©±åŠ¨**ï¼šé€šè¿‡é…ç½®æ–‡ä»¶åˆ‡æ¢Redisæ¨¡å¼
3. **å¹³æ»‘è¿‡æ¸¡**ï¼šä»å•å®ä¾‹åˆ°Sentinelé›¶ä»£ç æ”¹åŠ¨
4. **æ•…éšœéš”ç¦»**ï¼šé”çš„é—®é¢˜ä¸å½±å“ä¸»ä¸šåŠ¡æµç¨‹
5. **æˆç†Ÿç¨³å®š**ï¼šRedis Sentinelæ˜¯å®˜æ–¹æ¨èçš„é«˜å¯ç”¨æ–¹æ¡ˆ

**å®æ–½æŒ‡å—**ï¼š

1. **å¼€å‘ç¯å¢ƒéƒ¨ç½²**ï¼š
   ```bash
   # å¯åŠ¨å•ä¸ªRediså®ä¾‹
   docker run -d -p 6379:6379 redis:latest
   ```

2. **ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²**ï¼š
   ```bash
   # éƒ¨ç½²Redis Sentinelï¼ˆ3ä¸ªå“¨å…µ + 1ä¸»2ä»ï¼‰
   # å‚è€ƒï¼šhttps://redis.io/docs/manual/sentinel/
   ```

3. **ä»£ç é›†æˆ**ï¼š
   - å¼•å…¥ä¾èµ–ï¼š`go get github.com/go-redsync/redsync/v4`
   - å¤åˆ¶ä¸Šè¿°`RedisLockManager`ä»£ç 
   - åœ¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šRedisæ¨¡å¼

4. **æµ‹è¯•éªŒè¯**ï¼š
   - å•å…ƒæµ‹è¯•ï¼šæ¨¡æ‹Ÿé”ç«äº‰
   - é›†æˆæµ‹è¯•ï¼šå¤šå®ä¾‹å¹¶å‘æµ‹è¯•
   - æ•…éšœæ¼”ç»ƒï¼šSentinelä¸»ä»åˆ‡æ¢æµ‹è¯•

**ç›‘æ§æŒ‡æ ‡**ï¼š
```go
// åˆ†å¸ƒå¼é”ç›‘æ§
type LockMetrics struct {
    LockAcquired    counter  // è·å–é”æˆåŠŸæ¬¡æ•°
    LockFailed      counter  // è·å–é”å¤±è´¥æ¬¡æ•°ï¼ˆè¢«å ç”¨ï¼‰
    LockExpired     counter  // é”è¿‡æœŸæ¬¡æ•°
    LockHoldTime    histogram // é”æŒæœ‰æ—¶é—´åˆ†å¸ƒ
}
```

## 4. æ¶ˆè´¹è€…è®¾è®¡

### 4.1 æ¶ˆè´¹è€…æ ¸å¿ƒå®ç°
```go
package consumer

import (
    "context"
    "encoding/json"
    "sync"
    "time"
    
    "github.com/confluentinc/confluent-kafka-go/v2/kafka"
    "qywx/infrastructures/log"
)

// KafkaConsumer Kafkaæ¶ˆè´¹è€…
type KafkaConsumer struct {
    consumer       *kafka.Consumer
    userProcessors sync.Map  // userID -> *UserProcessor
    ctx            context.Context
    cancel         context.CancelFunc
    wg             sync.WaitGroup
}

// UserProcessor ç”¨æˆ·æ¶ˆæ¯å¤„ç†å™¨ï¼ˆè‡ªç®¡ç†ç”Ÿå‘½å‘¨æœŸ + ç†”æ–­ä¿æŠ¤ï¼‰
type UserProcessor struct {
    userID       string
    messageChan  chan *MessageWrapper
    consumer     *kafka.Consumer
    registry     *sync.Map          // æŒ‡å‘å…¨å±€æ³¨å†Œè¡¨
    state        atomic.Int32       // 0=active, 1=dying, 2=dead
    lastActive   atomic.Int64       // Unix timestamp
    lifetime     time.Duration      // ç”Ÿå‘½å‘¨æœŸï¼ˆå¦‚3å°æ—¶ï¼‰
    deduplicator *MessageDeduplicator
    
    // ç†”æ–­å™¨ï¼šé˜²æŠ¤æ¶æ„ç”¨æˆ·
    circuitBreaker *CircuitBreaker
    
    // æµé‡æ§åˆ¶ï¼šé™åˆ¶æ¶ˆæ¯é€Ÿç‡
    rateLimiter    *RateLimiter
}

// CircuitBreaker ç†”æ–­å™¨å®ç°
type CircuitBreaker struct {
    userID              string
    state               atomic.Int32  // 0=closed, 1=open, 2=half-open
    consecutiveFailures atomic.Int32
    openedAt            atomic.Int64  // Unix timestamp
    halfOpenDelay       time.Duration
    failureThreshold    int32
    successThreshold    int32         // half-openæˆåŠŸæ¬¡æ•°é˜ˆå€¼
    successCount        atomic.Int32  // half-openæˆåŠŸè®¡æ•°
}

// RateLimiter é™æµå™¨å®ç°ï¼ˆæ»‘åŠ¨çª—å£ + æƒ©ç½šå‡çº§ï¼‰
type RateLimiter struct {
    // é…ç½®å‚æ•°
    userID           string         // ç”¨æˆ·æ ‡è¯†ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    windowSize       int           // Nç§’çš„æ—¶é—´çª—å£
    maxMessages      int           // Xæ¡æ¶ˆæ¯ä¸Šé™
    penaltyThreshold int           // Yæ¬¡è¿ç»­è¿è§„
    blockDuration    time.Duration // Zå°æ—¶æ‹’ç»æœåŠ¡
    
    // è¿è¡Œæ—¶çŠ¶æ€
    messageTimestamps []int64      // æ¶ˆæ¯æ—¶é—´æˆ³é˜Ÿåˆ—ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
    violations        int           // è¿ç»­è¿è§„æ¬¡æ•°
    blockedUntil      time.Time     // å°ç¦åˆ°ä½•æ—¶
    mu                sync.Mutex    // ä¿æŠ¤å¹¶å‘è®¿é—®
}

const (
    StateActive = iota
    StateDying  
    StateDead
)

// NewKafkaConsumer åˆ›å»ºæ¶ˆè´¹è€…
func NewKafkaConsumer(brokers string, groupID string, topics []string) (*KafkaConsumer, error) {
    config := &kafka.ConfigMap{
        "bootstrap.servers": brokers,
        "group.id":          groupID,
        
        // åˆ†é…ç­–ç•¥ï¼šä½¿ç”¨stickyä¿æŒåˆ†åŒºç¨³å®š
        "partition.assignment.strategy": "cooperative-sticky",
        
        // ä¼šè¯ç®¡ç†
        "session.timeout.ms":    30000,  // 30ç§’è¶…æ—¶
        "heartbeat.interval.ms": 3000,   // 3ç§’å¿ƒè·³
        "max.poll.interval.ms":  300000, // 5åˆ†é’Ÿæœ€å¤§å¤„ç†æ—¶é—´
        
        // Offsetç®¡ç†
        "enable.auto.commit":    false,     // æ‰‹åŠ¨æäº¤
        "auto.offset.reset":     "earliest", // ä»æœ€æ—©å¼€å§‹
        "isolation.level":       "read_committed", // åªè¯»å·²æäº¤
        
        // æ€§èƒ½ä¼˜åŒ–
        "fetch.min.bytes":       1024,
        "fetch.wait.max.ms":     500,
    }
    
    c, err := kafka.NewConsumer(config)
    if err != nil {
        return nil, fmt.Errorf("create consumer failed: %w", err)
    }
    
    // è®¢é˜…ä¸»é¢˜
    err = c.SubscribeTopics(topics, nil)
    if err != nil {
        return nil, fmt.Errorf("subscribe failed: %w", err)
    }
    
    ctx, cancel := context.WithCancel(context.Background())
    
    return &KafkaConsumer{
        consumer: c,
        ctx:      ctx,
        cancel:   cancel,
    }, nil
}

// Start å¯åŠ¨æ¶ˆè´¹
func (kc *KafkaConsumer) Start() {
    kc.wg.Add(1)
    go kc.consumeLoop()
    
    // æ³¨æ„ï¼šä¸éœ€è¦ cleanupLoopï¼
    // æ¯ä¸ª processor è‡ªå·±ç®¡ç†ç”Ÿå‘½å‘¨æœŸ
}

// MessageWrapper åŒ…è£…æ¶ˆæ¯å’ŒKafkaå…ƒä¿¡æ¯
type MessageWrapper struct {
    ChatMessage    *ChatMessage
    KafkaMessage   *kafka.Message  // ä¿ç•™åŸå§‹Kafkaæ¶ˆæ¯ç”¨äºæäº¤
}

// consumeLoop æ¶ˆè´¹ä¸»å¾ªç¯ï¼ˆä¼˜åŒ–ç‰ˆï¼šæ— é˜Ÿå¤´é˜»å¡ï¼‰
func (kc *KafkaConsumer) consumeLoop() {
    defer kc.wg.Done()
    
    for {
        select {
        case <-kc.ctx.Done():
            return
            
        default:
            // æ‰¹é‡æ‹‰å–æ¶ˆæ¯
            msg, err := kc.consumer.ReadMessage(100 * time.Millisecond)
            if err != nil {
                if err.(kafka.Error).Code() != kafka.ErrTimedOut {
                    log.GetInstance().Sugar.Error("Read message failed: ", err)
                }
                continue
            }
            
            // è§£ææ¶ˆæ¯
            var chatMsg ChatMessage
            if err := json.Unmarshal(msg.Value, &chatMsg); err != nil {
                log.GetInstance().Sugar.Error("Unmarshal failed: ", err)
                // æ ¼å¼é”™è¯¯çš„æ¶ˆæ¯ç›´æ¥æäº¤ï¼Œé¿å…é˜»å¡
                kc.consumer.CommitMessage(msg)
                continue
            }
            
            // åŒ…è£…æ¶ˆæ¯ï¼ˆå…³é”®ï¼šä¼ é€’Kafkaå…ƒä¿¡æ¯ï¼‰
            wrapper := &MessageWrapper{
                ChatMessage:  &chatMsg,
                KafkaMessage: msg,
            }
            
            // è·¯ç”±åˆ°ç”¨æˆ·å¤„ç†å™¨
            processor, err := kc.getOrCreateProcessor(chatMsg.ExternalUserID)
            if err != nil {
                log.GetInstance().Sugar.Error("Failed to get processor: ", err)
                continue
            }
            
            // ä½¿ç”¨TrySendå®‰å…¨å‘é€ï¼ˆå¸¦ç†”æ–­ä¿æŠ¤ï¼‰
            err = processor.TrySend(wrapper)
            if err != nil {
                if strings.Contains(err.Error(), "not active") {
                    // processoræ­£åœ¨dyingï¼Œç¨ç­‰åé‡è¯•åˆ›å»ºæ–°çš„
                    time.Sleep(10 * time.Millisecond)
                    if processor, err = kc.getOrCreateProcessor(chatMsg.ExternalUserID); err == nil {
                        processor.TrySend(wrapper) // é‡è¯•ä¸€æ¬¡
                    }
                } else if strings.Contains(err.Error(), "circuit broken") {
                    // ç”¨æˆ·è¢«ç†”æ–­ï¼Œå‘é€åˆ°DLQ
                    log.GetInstance().Sugar.Warn("User circuit broken, sending to DLQ: ", chatMsg.ExternalUserID)
                    
                    // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
                    kc.dlqProducer.SendToDeadLetter(
                        &chatMsg,
                        ReasonCircuitOpen,
                        fmt.Sprintf("Circuit breaker open for user %s", chatMsg.ExternalUserID),
                        &KafkaMetadata{
                            Topic:     *msg.TopicPartition.Topic,
                            Partition: msg.TopicPartition.Partition,
                            Offset:    int64(msg.TopicPartition.Offset),
                        },
                    )
                    
                    metrics.CircuitBrokenMessages.Inc()
                    // æäº¤offsetï¼Œç»§ç»­å¤„ç†å…¶ä»–ç”¨æˆ·
                    kc.consumer.CommitMessage(msg)
                    
                } else if strings.Contains(err.Error(), "rate limited") {
                    // ç”¨æˆ·è¢«é™æµï¼Œå‘é€åˆ°DLQ
                    log.GetInstance().Sugar.Warn("User rate limited, sending to DLQ: ", chatMsg.ExternalUserID)
                    
                    // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
                    kc.dlqProducer.SendToDeadLetter(
                        &chatMsg,
                        ReasonRateLimited,
                        err.Error(),
                        &KafkaMetadata{
                            Topic:     *msg.TopicPartition.Topic,
                            Partition: msg.TopicPartition.Partition,
                            Offset:    int64(msg.TopicPartition.Offset),
                        },
                    )
                    
                    metrics.RateLimitedMessages.Inc()
                    // æäº¤offsetï¼Œç»§ç»­å¤„ç†å…¶ä»–ç”¨æˆ·
                    kc.consumer.CommitMessage(msg)
                    
                } else if strings.Contains(err.Error(), "user blocked") {
                    // ç”¨æˆ·è¢«å°ç¦ï¼Œå‘é€åˆ°DLQ
                    log.GetInstance().Sugar.Warn("User blocked, sending to DLQ: ", chatMsg.ExternalUserID)
                    
                    // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
                    kc.dlqProducer.SendToDeadLetter(
                        &chatMsg,
                        ReasonUserBlocked,
                        err.Error(),
                        &KafkaMetadata{
                            Topic:     *msg.TopicPartition.Topic,
                            Partition: msg.TopicPartition.Partition,
                            Offset:    int64(msg.TopicPartition.Offset),
                        },
                    )
                    
                    metrics.UserBlockedMessages.Inc()
                    // æäº¤offsetï¼Œç»§ç»­å¤„ç†å…¶ä»–ç”¨æˆ·
                    kc.consumer.CommitMessage(msg)
                    
                } else if strings.Contains(err.Error(), "queue full") {
                    // é˜Ÿåˆ—æ»¡ï¼Œå‘é€åˆ°DLQ
                    log.GetInstance().Sugar.Warn("User queue full, sending to DLQ: ", chatMsg.ExternalUserID)
                    
                    // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
                    kc.dlqProducer.SendToDeadLetter(
                        &chatMsg,
                        ReasonQueueFull,
                        "Processor message queue is full",
                        &KafkaMetadata{
                            Topic:     *msg.TopicPartition.Topic,
                            Partition: msg.TopicPartition.Partition,
                            Offset:    int64(msg.TopicPartition.Offset),
                        },
                    )
                    
                    metrics.QueueFullMessages.Inc()
                    // æäº¤offseté¿å…é˜»å¡å…¶ä»–ç”¨æˆ·
                    kc.consumer.CommitMessage(msg)
                    
                    // TODO: å¯é€‰æ‹©å°†æ¶ˆæ¯å­˜å…¥é‡è¯•é˜Ÿåˆ—
                    // kc.saveToRetryQueue(wrapper)
                }
            }
        }
    }
}

// getOrCreateProcessor è·å–æˆ–åˆ›å»ºç”¨æˆ·å¤„ç†å™¨ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
func (kc *KafkaConsumer) getOrCreateProcessor(userID string) (*UserProcessor, error) {
    // å¿«é€Ÿè·¯å¾„ï¼šè·å–å·²å­˜åœ¨çš„æ´»è·ƒprocessor
    if p, ok := kc.userProcessors.Load(userID); ok {
        processor := p.(*UserProcessor)
        // åªè¿”å›æ´»è·ƒçš„processor
        if processor.state.Load() == StateActive {
            return processor, nil
        }
        // éæ´»è·ƒï¼Œç»§ç»­åˆ›å»ºæ–°çš„
    }
    
    // åˆ›å»ºæ–°å¤„ç†å™¨ï¼ˆå¸¦ç†”æ–­å’Œé™æµä¿æŠ¤ï¼‰
    processor := &UserProcessor{
        userID:       userID,
        messageChan:  make(chan *MessageWrapper, 1000),
        consumer:     kc.consumer,
        registry:     &kc.userProcessors,
        state:        atomic.Int32{},
        lastActive:   atomic.Int64{},
        lifetime:     3 * time.Hour,  // TODO: ä»é…ç½®è¯»å–
        deduplicator: NewMessageDeduplicator(),
        
        // åˆå§‹åŒ–ç†”æ–­å™¨
        circuitBreaker: &CircuitBreaker{
            userID:           userID,
            state:            atomic.Int32{},      // åˆå§‹ä¸ºClosed
            halfOpenDelay:    30 * time.Second,    // 30ç§’åå°è¯•æ¢å¤
            failureThreshold: 100,                 // è¿ç»­100æ¬¡å¤±è´¥è§¦å‘ç†”æ–­
            successThreshold: 10,                  // Half-OpençŠ¶æ€éœ€è¦10æ¬¡æˆåŠŸæ‰èƒ½æ¢å¤
        },
        
        // åˆå§‹åŒ–é™æµå™¨ï¼ˆç²¾ç¡®çš„æ»‘åŠ¨çª—å£é™æµï¼‰
        rateLimiter: &RateLimiter{
            windowSize:       10,              // æ¯10ç§’
            maxMessages:      5,               // æœ€å¤š5æ¡æ¶ˆæ¯
            penaltyThreshold: 3,               // è¿ç»­è¿è§„3æ¬¡
            blockDuration:    1 * time.Hour,   // å°ç¦1å°æ—¶
            messageTimestamps: make([]int64, 0, 5),
            userID:           userID,          // ç”¨äºæ—¥å¿—å’Œå‘Šè­¦
        },
    }
    
    // åŸå­å­˜å‚¨ï¼Œé¿å…å¹¶å‘åˆ›å»º
    actual, loaded := kc.userProcessors.LoadOrStore(userID, processor)
    
    if !loaded {
        // æˆ‘ä»¬èµ¢å¾—äº†ç«äº‰ï¼Œå¯åŠ¨processor
        kc.wg.Add(1)
        go kc.runProcessor(actual.(*UserProcessor))
        log.GetInstance().Sugar.Info("Created new processor for user: ", userID)
    }
    
    return actual.(*UserProcessor), nil
}

// runProcessor processorçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸï¼ˆè‡ªç®¡ç†ï¼‰
func (kc *KafkaConsumer) runProcessor(p *UserProcessor) {
    defer kc.wg.Done()
    
    log.GetInstance().Sugar.Info("Processor started for user: ", p.userID)
    defer log.GetInstance().Sugar.Info("Processor stopped for user: ", p.userID)
    
    // åˆå§‹åŒ–çŠ¶æ€
    p.state.Store(StateActive)
    p.lastActive.Store(time.Now().Unix())
    
    // ç”Ÿå‘½å‘¨æœŸè®¡æ—¶å™¨
    idleTimer := time.NewTimer(p.lifetime)
    defer idleTimer.Stop()
    
    // æ¸…ç†èµ„æº
    defer p.cleanup()
    
    // æ¶ˆæ¯å¤„ç†ä¸»å¾ªç¯
    for {
        select {
        case wrapper, ok := <-p.messageChan:
            if !ok {
                // channelè¢«å¤–éƒ¨å…³é—­ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰
                log.GetInstance().Sugar.Warn("Message channel closed externally for user: ", p.userID)
                return
            }
            
            // æ£€æŸ¥è‡ªå·±çš„çŠ¶æ€
            if p.state.Load() != StateActive {
                log.GetInstance().Sugar.Debug("Processor dying, reject message for user: ", p.userID)
                continue
            }
            
            // å¤„ç†æ¶ˆæ¯
            kc.processMessage(p, wrapper)
            
            // æ›´æ–°æ´»è·ƒæ—¶é—´
            p.lastActive.Store(time.Now().Unix())
            
            // é‡ç½®ç”Ÿå‘½å‘¨æœŸè®¡æ—¶å™¨
            if !idleTimer.Stop() {
                select {
                case <-idleTimer.C:
                default:
                }
            }
            idleTimer.Reset(p.lifetime)
            
        case <-idleTimer.C:
            // ç”Ÿå‘½å‘¨æœŸåˆ°æœŸ
            log.GetInstance().Sugar.Info("Processor lifetime expired for user: ", p.userID)
            
            // æ ‡è®°ä¸ºdying
            if !p.state.CompareAndSwap(StateActive, StateDying) {
                return // å·²ç»åœ¨dying
            }
            
            // ä¼˜é›…é€€å‡ºï¼šå°è¯•å¤„ç†å‰©ä½™æ¶ˆæ¯
            p.drainMessages(kc, 5*time.Second)
            
            return
        }
    }
}

// processMessage å¤„ç†å•æ¡æ¶ˆæ¯ï¼ˆå¸¦å¹‚ç­‰æ€§æ£€æŸ¥ï¼‰
func (kc *KafkaConsumer) processMessage(p *UserProcessor, wrapper *MessageWrapper) {
    // å¹‚ç­‰æ€§æ£€æŸ¥
    isNew, err := p.deduplicator.CheckAndMark(wrapper.ChatMessage.MessageID)
    if err != nil {
        log.GetInstance().Sugar.Error("Dedup check failed: ", err)
        return
    }
    
    if !isNew {
        // é‡å¤æ¶ˆæ¯ä¹Ÿè¦æäº¤offsetï¼Œé¿å…æ— é™é‡è¯•
        p.consumer.CommitMessage(wrapper.KafkaMessage)
        return
    }
    
    // å®é™…ä¸šåŠ¡å¤„ç†
    err = kc.handleMessage(p.userID, wrapper.ChatMessage)
    
    if err != nil {
        log.GetInstance().Sugar.Error("Handle message failed: ", err)
        // å¤„ç†å¤±è´¥ï¼Œä¸æäº¤offsetï¼Œç­‰å¾…é‡å¯é‡è¯•
        return
    }
    
    // æˆåŠŸå¤„ç†ï¼Œæäº¤offset
    if err := p.consumer.CommitMessage(wrapper.KafkaMessage); err != nil {
        log.GetInstance().Sugar.Error("Commit offset failed: ", err,
            ", message may be reprocessed on restart")
    }
}

// drainMessages ä¼˜é›…é€€å‡ºå‰å°½åŠ›å¤„ç†å‰©ä½™æ¶ˆæ¯
func (p *UserProcessor) drainMessages(kc *KafkaConsumer, timeout time.Duration) {
    deadline := time.Now().Add(timeout)
    
    for time.Now().Before(deadline) {
        select {
        case wrapper, ok := <-p.messageChan:
            if !ok {
                return
            }
            kc.processMessage(p, wrapper)
            
        default:
            // é˜Ÿåˆ—ç©ºäº†
            return
        }
    }
}

// cleanup processoræ¸…ç†èµ„æº
func (p *UserProcessor) cleanup() {
    // 1. æ ‡è®°ä¸ºæ­»äº¡
    p.state.Store(StateDead)
    
    // 2. ä»æ³¨å†Œè¡¨åˆ é™¤è‡ªå·±ï¼ˆå…³é”®ï¼šé˜²æ­¢æ–°æ¶ˆæ¯è¿›å…¥ï¼‰
    p.registry.Delete(p.userID)
    
    // 3. å…³é—­channel
    close(p.messageChan)
    
    // 4. æ¸…ç†å…¶ä»–èµ„æº
    if p.deduplicator != nil {
        // p.deduplicator.Close() // å¦‚æœæœ‰æ¸…ç†æ–¹æ³•
    }
    
    log.GetInstance().Sugar.Info("Processor cleaned up for user: ", p.userID)
}

// TrySend å¤–éƒ¨å®‰å…¨å‘é€æ¶ˆæ¯æ¥å£ï¼ˆå¸¦ç†”æ–­å’Œé™æµä¿æŠ¤ï¼‰
func (p *UserProcessor) TrySend(wrapper *MessageWrapper) error {
    // 1. æ£€æŸ¥processorçŠ¶æ€
    state := p.state.Load()
    if state != StateActive {
        return fmt.Errorf("processor not active: state=%d", state)
    }
    
    // 2. ç†”æ–­å™¨æ£€æŸ¥
    if err := p.circuitBreaker.Allow(); err != nil {
        metrics.CircuitBreakerDenied.Inc()
        return fmt.Errorf("circuit broken: %w", err)
    }
    
    // 3. é™æµæ£€æŸ¥
    if err := p.rateLimiter.Allow(); err != nil {
        metrics.RateLimitExceeded.Inc()
        log.GetInstance().Sugar.Warn("Rate limit exceeded for user: ", p.userID)
        return fmt.Errorf("rate limited: %w", err)
    }
    
    // 4. éé˜»å¡å‘é€
    select {
    case p.messageChan <- wrapper:
        // æˆåŠŸå‘é€ï¼Œæ›´æ–°ç†”æ–­å™¨çŠ¶æ€
        p.circuitBreaker.OnSuccess()
        return nil
    default:
        // é˜Ÿåˆ—æ»¡ï¼Œè§¦å‘ç†”æ–­å™¨å¤±è´¥è®¡æ•°
        p.circuitBreaker.OnFailure()
        metrics.QueueFullEvents.Inc()
        return fmt.Errorf("processor queue full")
    }
}

// CircuitBreakeræ–¹æ³•å®ç°
func (cb *CircuitBreaker) Allow() error {
    state := cb.state.Load()
    
    switch state {
    case 0: // Closed - æ­£å¸¸çŠ¶æ€
        return nil
        
    case 1: // Open - ç†”æ–­çŠ¶æ€
        openedAt := time.Unix(cb.openedAt.Load(), 0)
        if time.Since(openedAt) > cb.halfOpenDelay {
            // å°è¯•è¿›å…¥åŠå¼€çŠ¶æ€
            if cb.state.CompareAndSwap(1, 2) {
                cb.successCount.Store(0)
                log.GetInstance().Sugar.Info("Circuit breaker half-open for user: ", cb.userID)
            }
            return nil // å…è®¸æµ‹è¯•æµé‡é€šè¿‡
        }
        return fmt.Errorf("circuit open for user %s", cb.userID)
        
    case 2: // Half-Open - åŠå¼€çŠ¶æ€ï¼ˆæµ‹è¯•æ¢å¤ï¼‰
        // é™åˆ¶æµ‹è¯•æµé‡
        if cb.successCount.Load() < cb.successThreshold {
            return nil
        }
        return fmt.Errorf("circuit half-open, limiting traffic for user %s", cb.userID)
        
    default:
        return fmt.Errorf("invalid circuit breaker state: %d", state)
    }
}

func (cb *CircuitBreaker) OnSuccess() {
    state := cb.state.Load()
    
    switch state {
    case 0: // Closed
        // é‡ç½®å¤±è´¥è®¡æ•°
        cb.consecutiveFailures.Store(0)
        
    case 2: // Half-Open
        count := cb.successCount.Add(1)
        if count >= cb.successThreshold {
            // æ¢å¤åˆ°å…³é—­çŠ¶æ€
            cb.state.Store(0)
            cb.consecutiveFailures.Store(0)
            log.GetInstance().Sugar.Info("Circuit breaker closed for user: ", cb.userID)
            metrics.CircuitBreakerRecovered.Inc()
        }
    }
}

func (cb *CircuitBreaker) OnFailure() {
    state := cb.state.Load()
    
    switch state {
    case 0: // Closed
        failures := cb.consecutiveFailures.Add(1)
        if failures >= cb.failureThreshold {
            // è§¦å‘ç†”æ–­
            if cb.state.CompareAndSwap(0, 1) {
                cb.openedAt.Store(time.Now().Unix())
                log.GetInstance().Sugar.Error("Circuit breaker opened for user: ", cb.userID,
                    ", failures: ", failures)
                metrics.CircuitBreakerOpened.Inc()
                
                // å‘é€å‘Šè­¦
                alerting.Send("UserCircuitBreakerOpen", map[string]string{
                    "user_id": cb.userID,
                    "failures": fmt.Sprintf("%d", failures),
                })
            }
        }
        
    case 2: // Half-Open
        // æµ‹è¯•å¤±è´¥ï¼Œç«‹å³å›åˆ°OpençŠ¶æ€
        cb.state.Store(1)
        cb.openedAt.Store(time.Now().Unix())
        log.GetInstance().Sugar.Warn("Circuit breaker re-opened for user: ", cb.userID)
    }
}

// RateLimiteræ–¹æ³•å®ç°ï¼ˆæ»‘åŠ¨çª—å£ç®—æ³•ï¼‰
func (rl *RateLimiter) Allow() error {
    rl.mu.Lock()
    defer rl.mu.Unlock()
    
    now := time.Now()
    
    // 1. æ£€æŸ¥æ˜¯å¦è¢«å°ç¦
    if now.Before(rl.blockedUntil) {
        remaining := rl.blockedUntil.Sub(now)
        metrics.BlockedUserDenied.Inc()
        return fmt.Errorf("user blocked for %v", remaining)
    }
    
    // 2. æ¸…ç†è¿‡æœŸçš„æ—¶é—´æˆ³ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
    cutoff := now.Unix() - int64(rl.windowSize)
    validTimestamps := []int64{}
    for _, ts := range rl.messageTimestamps {
        if ts > cutoff {
            validTimestamps = append(validTimestamps, ts)
        }
    }
    rl.messageTimestamps = validTimestamps
    
    // 3. æ£€æŸ¥çª—å£å†…æ¶ˆæ¯æ•°é‡
    if len(rl.messageTimestamps) >= rl.maxMessages {
        // è¿è§„ï¼
        rl.violations++
        metrics.RateLimitViolations.Inc()
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦å°ç¦
        if rl.violations >= rl.penaltyThreshold {
            rl.blockedUntil = now.Add(rl.blockDuration)
            
            // å‘é€å‘Šè­¦
            log.GetInstance().Sugar.Error("User blocked due to rate limit violations: ",
                rl.userID, ", violations: ", rl.violations, ", blocked for: ", rl.blockDuration)
            
            alerting.Send("UserBlocked", map[string]string{
                "user_id": rl.userID,
                "violations": fmt.Sprintf("%d", rl.violations),
                "block_hours": fmt.Sprintf("%.1f", rl.blockDuration.Hours()),
            })
            
            metrics.UsersBlocked.Inc()
            
            // é‡ç½®è¿è§„è®¡æ•°ï¼ˆä¸‹æ¬¡è§£å°åé‡æ–°è®¡ç®—ï¼‰
            rl.violations = 0
            
            return fmt.Errorf("user blocked: exceeded %d violations, blocked for %v",
                rl.penaltyThreshold, rl.blockDuration)
        }
        
        return fmt.Errorf("rate limit exceeded: %d messages in %d seconds (violation %d/%d)",
            len(rl.messageTimestamps), rl.windowSize, rl.violations, rl.penaltyThreshold)
    }
    
    // 4. å…è®¸å‘é€ï¼Œè®°å½•æ—¶é—´æˆ³
    rl.messageTimestamps = append(rl.messageTimestamps, now.Unix())
    
    // å¦‚æœæ²¡æœ‰è¿è§„ï¼Œé‡ç½®è¿è§„è®¡æ•°
    if len(rl.messageTimestamps) <= rl.maxMessages/2 {
        rl.violations = 0  // æµé‡é™åˆ°ä¸€åŠä»¥ä¸‹æ—¶é‡ç½®è¿è§„è®¡æ•°
    }
    
    return nil
}

// handleMessage å®é™…å¤„ç†æ¶ˆæ¯
func (kc *KafkaConsumer) handleMessage(userID string, msg *ChatMessage) error {
    // åˆ›å»ºè¶…æ—¶context
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()
    
    log.GetInstance().Sugar.Info("Processing message for user ", userID, 
        ", type: ", msg.MsgType, ", msgID: ", msg.MessageID)
    
    // æ ¹æ®æ¶ˆæ¯ç±»å‹å¤„ç†
    switch msg.MsgType {
    case "text":
        return handleTextMessage(ctx, msg)
    case "image":
        return handleImageMessage(ctx, msg)
    case "event":
        return handleEventMessage(ctx, msg)
    default:
        log.GetInstance().Sugar.Debug("Unknown message type: ", msg.MsgType)
        return nil
    }
}

// æ³¨æ„ï¼šé‡‡ç”¨è‡ªç®¡ç†ç”Ÿå‘½å‘¨æœŸè®¾è®¡ï¼Œä¸éœ€è¦ cleanupLoopï¼
// æ¯ä¸ª processor è‡ªå·±ç®¡ç†ç”Ÿå‘½å‘¨æœŸï¼Œè§ä¸‹æ–¹ processUserMessages å®ç°

// commitMessage æäº¤æ¶ˆæ¯offset
func (kc *KafkaConsumer) commitMessage(msg *kafka.Message) {
    _, err := kc.consumer.CommitMessage(msg)
    if err != nil {
        log.GetInstance().Sugar.Error("Commit failed: ", err)
    }
}

// Stop åœæ­¢æ¶ˆè´¹è€…
func (kc *KafkaConsumer) Stop() {
    kc.cancel()
    
    // ç­‰å¾…æ‰€æœ‰åç¨‹é€€å‡º
    done := make(chan struct{})
    go func() {
        kc.wg.Wait()
        close(done)
    }()
    
    select {
    case <-done:
        log.GetInstance().Sugar.Info("Consumer stopped gracefully")
    case <-time.After(30 * time.Second):
        log.GetInstance().Sugar.Warn("Consumer stop timeout")
    }
    
    kc.consumer.Close()
}
```

### 4.2 å¹‚ç­‰æ€§ä¿è¯ï¼ˆé‡è¦ï¼‰

ç”±äºé‡‡ç”¨ **At-least-once** è¯­ä¹‰ï¼Œæ¶ˆæ¯å¯èƒ½é‡å¤å¤„ç†ï¼Œå¿…é¡»å®ç°å¹‚ç­‰æ€§ï¼š

```go
// MessageDeduplicator åŸºäºRedisçš„å¹‚ç­‰æ€§ä¿è¯
type MessageDeduplicator struct {
    redis *redis.Client
}

// CheckAndMark åŸå­æ€§æ£€æŸ¥å¹¶æ ‡è®°æ¶ˆæ¯
func (md *MessageDeduplicator) CheckAndMark(messageID string) (bool, error) {
    key := fmt.Sprintf("msg:processed:%s", messageID)
    
    // SetNX: åªæœ‰ä¸å­˜åœ¨æ—¶æ‰è®¾ç½®ï¼ˆåŸå­æ“ä½œï¼‰
    success, err := md.redis.SetNX(key, "1", 24*time.Hour).Result()
    if err != nil {
        return false, err
    }
    
    return success, nil // trueè¡¨ç¤ºæ–°æ¶ˆæ¯ï¼Œfalseè¡¨ç¤ºé‡å¤
}

// handleMessage å¸¦å¹‚ç­‰æ€§æ£€æŸ¥çš„æ¶ˆæ¯å¤„ç†
func (kc *KafkaConsumer) handleMessage(userID string, msg *ChatMessage) error {
    // 1. å¹‚ç­‰æ€§æ£€æŸ¥ï¼ˆå¿…é¡»åœ¨æ‰€æœ‰å¤„ç†ä¹‹å‰ï¼‰
    isNew, err := kc.deduplicator.CheckAndMark(msg.MessageID)
    if err != nil {
        // Redisé”™è¯¯ï¼Œå¯ä»¥é€‰æ‹©ï¼š
        // A. è¿”å›é”™è¯¯ï¼Œä¸æäº¤offsetï¼ˆä¿å®ˆï¼‰
        // B. ç»§ç»­å¤„ç†ï¼Œæ¥å—å¯èƒ½é‡å¤ï¼ˆæ¿€è¿›ï¼‰
        return fmt.Errorf("dedup check failed: %w", err)
    }
    
    if !isNew {
        // é‡å¤æ¶ˆæ¯ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        // æ³¨æ„ï¼šè¿”å›nilè®©offsetè¢«æäº¤ï¼Œé¿å…æ— é™é‡è¯•
        log.GetInstance().Sugar.Debug("Duplicate message skipped: ", msg.MessageID)
        return nil
    }
    
    // 2. å®é™…ä¸šåŠ¡å¤„ç†
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()
    
    // å¤„ç†æ¶ˆæ¯...
    
    return nil
}

// å¹‚ç­‰æ€§è®¾è®¡åŸåˆ™
/*
1. ä½¿ç”¨å…¨å±€å”¯ä¸€çš„MessageID
2. åœ¨æ•°æ®åº“æ“ä½œä¸­ä½¿ç”¨ UPSERT è€Œé INSERT
3. çŠ¶æ€æ›´æ–°ä½¿ç”¨ç‰ˆæœ¬å·æˆ–æ—¶é—´æˆ³é˜²æ­¢å›é€€
4. é‡‘é¢æ“ä½œä½¿ç”¨äº‹åŠ¡IDç¡®ä¿ä¸é‡å¤æ‰£æ¬¾
*/
```

## 5. ç›‘æ§ä¸è¿ç»´

### 5.1 å…³é”®ç›‘æ§æŒ‡æ ‡
```go
type Metrics struct {
    // ç”Ÿäº§è€…æŒ‡æ ‡
    ProducedMessages   counter
    ProduceErrors      counter
    ProduceLatency     histogram
    
    // æ¶ˆè´¹è€…æŒ‡æ ‡
    ConsumedMessages   counter
    ConsumerLag        gauge
    ProcessingTime     histogram
    ActiveProcessors   gauge
    
    // åˆ†åŒºæŒ‡æ ‡
    PartitionLag       map[int32]gauge
    PartitionThroughput map[int32]counter
}

// Prometheusé›†æˆç¤ºä¾‹
func setupMetrics() {
    prometheus.MustRegister(
        consumerLag,
        processingTime,
        activeProcessors,
    )
}
```

### 5.2 å‘Šè­¦è§„åˆ™
```yaml
alerts:
  - name: ConsumerLag
    condition: lag > 10000
    duration: 5m
    action: alert
    
  - name: RebalanceFrequent
    condition: rebalance_count > 3
    duration: 10m
    action: page
    
  - name: ProcessorMemoryLeak
    condition: active_processors > 1000
    duration: 30m
    action: alert
```

## 6. æ•…éšœå¤„ç†

### 6.1 èŠ‚ç‚¹æ•…éšœ
- **æ£€æµ‹**ï¼šKafka é€šè¿‡å¿ƒè·³æ£€æµ‹ï¼ˆ30ç§’è¶…æ—¶ï¼‰
- **æ¢å¤**ï¼šè‡ªåŠ¨è§¦å‘ Rebalanceï¼Œå…¶ä»–èŠ‚ç‚¹æ¥ç®¡åˆ†åŒº
- **å½±å“**ï¼šRebalance æœŸé—´ï¼ˆçº¦3-30ç§’ï¼‰æš‚åœæ¶ˆè´¹

### 6.2 ç½‘ç»œåˆ†åŒº
- **é—®é¢˜**ï¼šæ¶ˆè´¹è€…ä¸ Kafka å¤±è”ä½†ä»åœ¨è¿è¡Œ
- **è§£å†³**ï¼šæ¶ˆè´¹è€…è‡ªåŠ¨é‡è¿ï¼Œä½¿ç”¨æŒ‡æ•°é€€é¿ç­–ç•¥
- **é™çº§**ï¼šå¯é€‰é™çº§åˆ°æœ¬åœ°é˜Ÿåˆ—æ¨¡å¼

### 6.3 æ¶ˆæ¯ç§¯å‹
- **æ£€æµ‹**ï¼šç›‘æ§ Consumer Lag
- **å¤„ç†**ï¼š
  1. å¢åŠ æ¶ˆè´¹è€…èŠ‚ç‚¹
  2. å¢åŠ æ¯èŠ‚ç‚¹çš„å¹¶å‘å¤„ç†å™¨æ•°
  3. ä¼˜åŒ–æ¶ˆæ¯å¤„ç†é€»è¾‘

## 7. æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 7.1 ç”Ÿäº§è€…ä¼˜åŒ–
- ä½¿ç”¨æ‰¹é‡å‘é€ï¼š`linger.ms=10`
- å¯ç”¨å‹ç¼©ï¼š`compression.type=lz4`
- å¼‚æ­¥å‘é€ï¼šä½¿ç”¨ callback å¤„ç†ç»“æœ

### 7.2 æ¶ˆè´¹è€…ä¼˜åŒ–
- æ‰¹é‡æ‹‰å–ï¼š`fetch.min.bytes=1024`
- å¢åŠ ç¼“å†²ï¼šæ¯ç”¨æˆ· channel 1000 å®¹é‡
- å¹¶è¡Œå¤„ç†ï¼šä¸åŒç”¨æˆ·å®Œå…¨å¹¶è¡Œ

### 7.3 Kafka é›†ç¾¤ä¼˜åŒ–
```bash
# JVM è°ƒä¼˜
KAFKA_HEAP_OPTS="-Xmx4G -Xms4G"
KAFKA_JVM_PERFORMANCE_OPTS="-XX:+UseG1GC -XX:MaxGCPauseMillis=20"

# ç³»ç»Ÿè°ƒä¼˜
vm.swappiness=1
net.core.rmem_max=134217728
net.core.wmem_max=134217728
```

## 8. éƒ¨ç½²æ£€æŸ¥æ¸…å•

### 8.1 ç”Ÿäº§ç¯å¢ƒï¼ˆé›†ç¾¤ï¼‰
- [ ] Kafka é›†ç¾¤è‡³å°‘3èŠ‚ç‚¹
- [ ] ä¸»Topicåˆ›å»ºï¼š12ä¸ªåˆ†åŒºï¼Œ3å‰¯æœ¬
- [ ] **DLQ Topicåˆ›å»º**ï¼š`qywx-dead-letter-queue`ï¼Œ6ä¸ªåˆ†åŒºï¼Œ3å‰¯æœ¬
- [ ] Redis é›†ç¾¤ç”¨äºåˆ†å¸ƒå¼é”å’Œå»é‡
- [ ] ç›‘æ§ç³»ç»Ÿå°±ç»ªï¼ˆPrometheus + Grafanaï¼‰
- [ ] æ—¥å¿—æ”¶é›†é…ç½®ï¼ˆELK Stackï¼‰
- [ ] **DLQç´§æ€¥æ–‡ä»¶ç›®å½•**ï¼š`/var/log/qywx/` å…·æœ‰å†™æƒé™

### 8.2 å¼€å‘ç¯å¢ƒï¼ˆå•ç‚¹ï¼‰
- [ ] Kafka å•èŠ‚ç‚¹è¿è¡Œæ­£å¸¸
- [ ] **ä¸»Topicåˆ›å»º**ï¼š4ä¸ªåˆ†åŒºï¼Œ**1å‰¯æœ¬**ï¼ˆâš ï¸ å…³é”®ï¼‰
- [ ] **DLQ Topicåˆ›å»º**ï¼š2ä¸ªåˆ†åŒºï¼Œ**1å‰¯æœ¬**ï¼ˆâš ï¸ å…³é”®ï¼‰
- [ ] **min.insync.replicasè®¾ç½®ä¸º1**ï¼ˆâš ï¸ å¦åˆ™æ— æ³•å‘é€æ¶ˆæ¯ï¼‰
- [ ] Redis å•å®ä¾‹ï¼ˆå¦‚æœåªæœ‰ä¸€ä¸ªç”Ÿäº§è€…å®ä¾‹å¯æš‚æ—¶ä¸éœ€è¦ï¼‰
- [ ] ç®€åŒ–ç›‘æ§ï¼ˆå¯é€‰ï¼‰
- [ ] **DLQç´§æ€¥æ–‡ä»¶ç›®å½•**ï¼š`/var/log/qywx/` å…·æœ‰å†™æƒé™

### é™çº§æ–¹æ¡ˆå‡†å¤‡
- [ ] **ç¡®è®¤æ¥å—ä¸´æ—¶é™çº§æ–¹æ¡ˆçš„é£é™©**
  - âš ï¸ å¯èƒ½å¯¼è‡´æ¶ˆæ¯ä¹±åº
  - âš ï¸ é™çº§æ¶ˆæ¯åªèƒ½å•èŠ‚ç‚¹å¤„ç†
  - âš ï¸ æ— è‡ªåŠ¨æ¢å¤æœºåˆ¶
- [ ] é…ç½®é™çº§å‘Šè­¦è§„åˆ™ï¼ˆä¼˜å…ˆçº§ï¼šCriticalï¼‰
- [ ] å‡†å¤‡æ‰‹åŠ¨ä»‹å…¥æµç¨‹æ–‡æ¡£
- [ ] **TODO: å®ç°æŒä¹…åŒ–å¤±è´¥é˜Ÿåˆ—ï¼ˆPhase 2ï¼‰**

### åˆ†å¸ƒå¼é”å‡†å¤‡
- [ ] **å¼€å‘ç¯å¢ƒ**ï¼šå•ç”Ÿäº§è€…å®ä¾‹å¯è·³è¿‡
- [ ] **ç”Ÿäº§ç¯å¢ƒï¼ˆå¤šç”Ÿäº§è€…ï¼‰**ï¼š
  - [ ] Rediså®ä¾‹å°±ç»ªï¼ˆå¯å¤ç”¨ç°æœ‰ï¼‰
  - [ ] é€‰æ‹©åˆ†å¸ƒå¼é”å®¢æˆ·ç«¯ï¼š
    - Go: [go-redsync](https://github.com/go-redsync/redsync)
    - Java: [Redisson](https://github.com/redisson/redisson)
  - [ ] é…ç½®é”è¶…æ—¶æ—¶é—´ï¼ˆå»ºè®®30ç§’ï¼‰
  - [ ] å®ç°é”è·å–å¤±è´¥çš„å¤„ç†é€»è¾‘

### æµ‹è¯•ä¸æ¼”ç»ƒ
- [ ] æ•…éšœæ¼”ç»ƒå®Œæˆ
  - Kafka å®•æœºåœºæ™¯
  - ç½‘ç»œåˆ†åŒºåœºæ™¯
  - é™çº§æµç¨‹æµ‹è¯•
- [ ] **æ¶æ„ç”¨æˆ·é˜²æŠ¤æµ‹è¯•**
  - [ ] é™æµæµ‹è¯•ï¼šå‘é€100æ¡/åˆ†é’Ÿï¼ŒéªŒè¯æ‹’ç»
  - [ ] ç†”æ–­æµ‹è¯•ï¼šæ¨¡æ‹Ÿé˜Ÿåˆ—æ»¡ï¼ŒéªŒè¯30ç§’ç†”æ–­
  - [ ] æ¢å¤æµ‹è¯•ï¼šéªŒè¯Half-OpençŠ¶æ€è‡ªåŠ¨æ¢å¤
  - [ ] éš”ç¦»æµ‹è¯•ï¼šéªŒè¯æ¶æ„ç”¨æˆ·ä¸å½±å“å…¶ä»–ç”¨æˆ·
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡
  - æ­£å¸¸è´Ÿè½½ï¼š1000ç”¨æˆ· Ã— 2æ¡/åˆ†é’Ÿ
  - å³°å€¼è´Ÿè½½ï¼š100ç”¨æˆ· Ã— 60æ¡/åˆ†é’Ÿ
  - æ”»å‡»è´Ÿè½½ï¼š10ç”¨æˆ· Ã— 1000æ¡/åˆ†é’Ÿ
- [ ] æ¶ˆæ¯é¡ºåºæ€§éªŒè¯
- [ ] **ç›‘æ§å‘Šè­¦éªŒè¯**
  - [ ] ç†”æ–­å™¨å‘Šè­¦è§¦å‘
  - [ ] é™æµå‘Šè­¦è§¦å‘
  - [ ] å‘Šè­¦é€šçŸ¥é“¾è·¯

## 9. å…³é”®è®¾è®¡å†³ç­–

### 9.1 Offset æäº¤å®‰å…¨æ€§

**æ ¸å¿ƒåŸåˆ™ï¼šåªæœ‰æ¶ˆæ¯å¤„ç†æˆåŠŸåæ‰æäº¤ Offset**

**é”™è¯¯æ¨¡å¼ï¼ˆä¼šå¯¼è‡´æ¶ˆæ¯ä¸¢å¤±ï¼‰ï¼š**
```go
// âŒ å±é™©ï¼šè¿‡æ—©æäº¤
case processor.messageChan <- msg:
    commitOffset(msg)  // æ¶ˆæ¯è¿˜åœ¨å†…å­˜ï¼Œæœªå®é™…å¤„ç†
    // å¦‚æœæ­¤æ—¶å´©æºƒï¼Œæ¶ˆæ¯æ°¸ä¹…ä¸¢å¤±
```

**æ­£ç¡®æ¨¡å¼ï¼š**
```go
// âœ… å®‰å…¨ï¼šå¤„ç†åæäº¤
func processUserMessages(processor *UserProcessor) {
    for wrapper := range processor.messageChan {
        err := handleMessage(wrapper.ChatMessage)
        if err == nil {
            // åªæœ‰æˆåŠŸå¤„ç†æ‰æäº¤
            consumer.CommitMessage(wrapper.KafkaMessage)
        }
        // å¤±è´¥ä¸æäº¤ï¼Œé‡å¯åé‡è¯•
    }
}
```

**è®¾è®¡è¦ç‚¹ï¼š**
1. å°† Kafka å…ƒä¿¡æ¯ï¼ˆoffsetã€partitionï¼‰ä¼ é€’åˆ°å¤„ç†åç¨‹
2. å¤„ç†åç¨‹è´Ÿè´£æäº¤ï¼ˆè°å¤„ç†ï¼Œè°æäº¤ï¼‰
3. å¤±è´¥ä¸æäº¤ï¼Œä¾èµ– at-least-once + å¹‚ç­‰æ€§

### 9.2 é™çº§ç­–ç•¥çš„æƒè¡¡

**å½“å‰é™çº§æ–¹æ¡ˆçš„å±€é™æ€§**ï¼š

```go
// âš ï¸ å½“å‰çš„ä¸´æ—¶é™çº§æ–¹æ¡ˆ
Kafkaå¤±è´¥ â†’ æœ¬åœ°channel â†’ å•èŠ‚ç‚¹å¤„ç†

é—®é¢˜ï¼š
1. æ¶ˆæ¯ä¹±åºï¼šUser123çš„M1èµ°æœ¬åœ°ï¼ŒM2èµ°Kafkaï¼Œå¯èƒ½M2å…ˆå¤„ç†
2. åˆ†å¸ƒå¼å¤±æ•ˆï¼šé™çº§æ¶ˆæ¯åªèƒ½åœ¨æ•…éšœèŠ‚ç‚¹å¤„ç†
3. æ— æ³•æ¢å¤ï¼šä¸€æ—¦é™çº§ï¼Œå³ä½¿Kafkaæ¢å¤ä¹Ÿæ— æ³•è¿ç§»å›å»
```

**ç†æƒ³çš„é™çº§æ–¹æ¡ˆï¼ˆå¾…å®ç°ï¼‰**ï¼š

```go
// âœ… ç†æƒ³çš„æŒä¹…åŒ–é™çº§æ–¹æ¡ˆ
Kafkaå¤±è´¥ â†’ æŒä¹…åŒ–å­˜å‚¨ â†’ è¡¥å¿ä»»åŠ¡ â†’ é‡è¯•Kafka

ä¼˜åŠ¿ï¼š
1. ä¿è¯é¡ºåºï¼šåŒä¸€ç”¨æˆ·çš„æ¶ˆæ¯æŒ‰åºé‡è¯•
2. æœ€ç»ˆä¸€è‡´ï¼šæ‰€æœ‰æ¶ˆæ¯æœ€ç»ˆéƒ½ä¼šè¿›å…¥Kafka
3. å¯è§‚æµ‹æ€§ï¼šå¤±è´¥æ¶ˆæ¯å¯æŸ¥è¯¢ã€å¯ç›‘æ§ã€å¯æ‰‹åŠ¨å¹²é¢„
```

**ä¸ºä»€ä¹ˆæš‚æ—¶æ¥å—ä¸å®Œç¾**ï¼š
1. **è¿­ä»£å¼€å‘**ï¼šå…ˆä¿è¯åŸºç¡€åŠŸèƒ½å¯ç”¨ï¼Œå†ä¼˜åŒ–
2. **ä¾èµ–æœªå°±ç»ª**ï¼šæŒä¹…å­˜å‚¨å±‚å°šæœªå®ç°
3. **é£é™©å¯æ§**ï¼šé€šè¿‡ç›‘æ§å’Œå‘Šè­¦åŠæ—¶å‘ç°é—®é¢˜
4. **æ˜ç¡®TODO**ï¼šæ¸…æ™°çš„æ”¹è¿›è·¯çº¿å›¾

### 9.3 å¼‚æ­¥å‘é€æ˜¯æ ¸å¿ƒ
**è®¾è®¡åŸåˆ™**ï¼šç”Ÿäº§è€…å¿…é¡»æ˜¯çœŸæ­£çš„å¼‚æ­¥ï¼Œå¦åˆ™æ•´ä¸ªç³»ç»Ÿçš„ååé‡ä¼šè¢«ç”Ÿäº§è€…æ‹–ç´¯ã€‚

**é”™è¯¯ç¤ºä¾‹åˆ†æ**ï¼š
```go
// âŒ è¡¨é¢å¼‚æ­¥ï¼Œå®é™…åŒæ­¥çš„é™·é˜±
err = producer.Produce(msg, deliveryChan)
e := <-deliveryChan  // é˜»å¡ç­‰å¾…ï¼

// é—®é¢˜ï¼š
// 1. å®Œå…¨å¦å®šäº† Kafka æ‰¹é‡å‘é€çš„ä¼˜åŠ¿
// 2. linger.ms=10 å˜å¾—æ¯«æ— æ„ä¹‰
// 3. æ¯æ¡æ¶ˆæ¯çš„å»¶è¿Ÿç´¯åŠ ï¼Œå½¢æˆç“¶é¢ˆ
```

**æ­£ç¡®å®è·µ**ï¼š
- ç”Ÿäº§è€…ï¼šFire-and-forget + åå°äº‹ä»¶å¤„ç†
- æ‰¹é‡ç¡®è®¤ï¼šé€šè¿‡ Events() channel æ‰¹é‡å¤„ç†ç»“æœ
- èƒŒå‹å¤„ç†ï¼šé˜Ÿåˆ—æ»¡æ—¶çš„ä¼˜é›…é™çº§ç­–ç•¥

### 9.3 ç”¨æˆ·çº§å¹¶è¡Œçš„å¿…è¦æ€§
- åŒä¸€ç”¨æˆ·ä¸²è¡Œï¼šä¿è¯æ¶ˆæ¯é¡ºåº
- ä¸åŒç”¨æˆ·å¹¶è¡Œï¼šæœ€å¤§åŒ–ååé‡
- åŠ¨æ€ç®¡ç†ï¼š3å°æ—¶ç”Ÿå‘½å‘¨æœŸï¼Œè‡ªåŠ¨æ¸…ç†

### 9.4 æ€§èƒ½åŸºå‡†
åŸºäºæ­£ç¡®çš„å¼‚æ­¥è®¾è®¡ï¼š
- å•èŠ‚ç‚¹ç”Ÿäº§ï¼š10,000+ msg/s
- å•èŠ‚ç‚¹æ¶ˆè´¹ï¼š5,000+ msg/sï¼ˆå«ä¸šåŠ¡å¤„ç†ï¼‰
- ç«¯åˆ°ç«¯å»¶è¿Ÿï¼šP99 < 100ms

## 10. å¹¶å‘å®‰å…¨æ€§ä¿è¯

### 10.1 Processor ç”Ÿå‘½å‘¨æœŸçš„å¹¶å‘å®‰å…¨

**æ ¸å¿ƒæŒ‘æˆ˜**ï¼šåœ¨é«˜å¹¶å‘ç¯å¢ƒä¸‹ï¼ŒåŒä¸€ç”¨æˆ·çš„ processor å¯èƒ½åŒæ—¶é¢ä¸´åˆ›å»ºã€æ¶ˆæ¯è·¯ç”±ã€è¶…æ—¶æ¸…ç†ç­‰å¹¶å‘æ“ä½œã€‚

**è§£å†³æ–¹æ¡ˆï¼šè‡ªç®¡ç†ç”Ÿå‘½å‘¨æœŸ + åŸå­çŠ¶æ€æœº**

#### æ¶æ„èŒƒå¼è½¬å˜ï¼šä»"è¢«ç®¡ç†"åˆ°"è‡ªæ²»"

è¿™ä¸ªè®¾è®¡å®ç°äº†ä¸€ä¸ªå…³é”®çš„æ¶æ„èŒƒå¼è½¬å˜ï¼Œé€šè¿‡è®© UserProcessor å®Œå…¨è‡ªæ²»ï¼Œ**ä»ç†è®ºä¸Šæ¶ˆé™¤äº†æ‰€æœ‰ç«æ€æ¡ä»¶**ï¼š

```go
// âŒ ä¼ ç»Ÿæ¨¡å¼ï¼šå¤–éƒ¨ç®¡ç†ï¼ˆå¤æ‚ã€æ˜“é”™ã€æœ‰ç«æ€ï¼‰
type TraditionalScheduler struct {
    processors  map[string]*Processor
    cleanupChan chan string
    cleanupWG   sync.WaitGroup
}

func (s *Scheduler) cleanupLoop() {
    for {
        s.processorsMu.Lock()
        for userID, processor := range s.processors {
            if isExpired(processor) {
                delete(s.processors, userID)  // å±é™©ï¼šç«æ€çª—å£
            }
        }
        s.processorsMu.Unlock()
        time.Sleep(cleanupInterval)
    }
}

// âœ… è‡ªæ²»æ¨¡å¼ï¼šå†…éƒ¨ç®¡ç†ï¼ˆç®€å•ã€å®‰å…¨ã€æ— ç«æ€ï¼‰
type SelfManagedProcessor struct {
    state       atomic.Int32    // åŸå­çŠ¶æ€
    registry    *sync.Map       // ä»…ç”¨äºè‡ªæˆ‘æ³¨é”€
    lifetime    time.Duration   // è‡ªå·±çš„ç”Ÿå‘½å‘¨æœŸ
    // æ²¡æœ‰å¤–éƒ¨æ¸…ç†å™¨ï¼
}

func (p *UserProcessor) run() {
    idleTimer := time.NewTimer(p.lifetime)
    defer p.cleanup()  // è‡ªæˆ‘æ¸…ç†
    
    for {
        select {
        case <-idleTimer.C:
            // ç”Ÿå‘½å‘¨æœŸç»“æŸï¼Œè‡ªä¸»é€€å‡º
            return
        case msg := <-p.messageChan:
            // å¤„ç†æ¶ˆæ¯ï¼Œé‡ç½®ç”Ÿå‘½
            idleTimer.Reset(p.lifetime)
        }
    }
}
```

#### ä¸ºä»€ä¹ˆ"æ²¡æœ‰ä»»ä½•ç«æ€æ¡ä»¶"çš„è®ºæ–­æˆç«‹

**ä¼ ç»Ÿæ–¹æ¡ˆçš„ç«æ€çª—å£**ï¼š
```
T1: cleanupLoop æ£€æŸ¥ processor P1 è¶…æ—¶
T2: æ–°æ¶ˆæ¯ M1 åˆ°è¾¾ï¼Œè·¯ç”±åˆ° P1
T3: cleanupLoop åˆ é™¤ P1
T4: M1 è¿›å…¥å·²åˆ é™¤çš„ processorï¼ˆæ•°æ®ä¸¢å¤±ï¼ï¼‰
```

**è‡ªæ²»æ–¹æ¡ˆçš„å®‰å…¨ä¿è¯**ï¼š
```
T1: P1 çš„ idleTimer è‡ªå·±è§¦å‘
T2: P1 è‡ªå·±å†³å®š state = Dying
T3: æ–°æ¶ˆæ¯ M1 è¢« TrySend æ‹’ç»ï¼ˆçŠ¶æ€æ£€æŸ¥ï¼‰
T4: P1 å®Œæˆå‰©ä½™å·¥ä½œ
T5: P1 è‡ªå·±ä» registry æ³¨é”€
T6: P1 ä¼˜é›…é€€å‡º

ç»“æœï¼šæ²¡æœ‰å¤–éƒ¨å¹²é¢„ï¼Œæ²¡æœ‰ç«æ€çª—å£ï¼Œæ²¡æœ‰æ•°æ®ä¸¢å¤±
```

#### Actor Model çš„å®Œç¾ä½“ç°

è¿™ä¸ªè‡ªæ²»è®¾è®¡æ— æ„ä¸­å®ç°äº† **Actor Model** çš„æ ¸å¿ƒåŸåˆ™ï¼š

```go
// æ¯ä¸ª UserProcessor å°±æ˜¯ä¸€ä¸ª Actor
type UserProcessor struct {
    // Actor çš„ç§æœ‰çŠ¶æ€ï¼ˆå°è£…æ€§ï¼‰
    state        atomic.Int32
    messageChan  chan *MessageWrapper  // Actor çš„é‚®ç®±
    
    // ç”Ÿå‘½å‘¨æœŸè‡ªç®¡ç†ï¼ˆè‡ªæ²»æ€§ï¼‰
    lifetime     time.Duration
    idleTimer    *time.Timer
    
    // å”¯ä¸€çš„å¤–éƒ¨å¼•ç”¨ï¼ˆæœ€å°ä¾èµ–ï¼‰
    registry     *sync.Map  // ä»…ç”¨äºè‡ªæˆ‘æ³¨é”€
}

// Actor çš„ä¸‰å¤§ç‰¹æ€§ï¼š
// 1. å°è£…ï¼šå†…éƒ¨çŠ¶æ€å®Œå…¨ç§æœ‰
// 2. å¼‚æ­¥ï¼šé€šè¿‡ channel æ¥æ”¶æ¶ˆæ¯
// 3. è‡ªæ²»ï¼šç”Ÿå‘½å‘¨æœŸå®Œå…¨è‡ªä¸»
```

#### è®¾è®¡ä¼˜é›…æ€§åˆ†æ

**ç³»ç»Ÿå¤æ‚åº¦çš„ç®€åŒ–**ï¼š
```
ä¼ ç»Ÿè®¾è®¡çš„ä¾èµ–å…³ç³»ï¼ˆå¤æ‚ï¼‰ï¼š
Scheduler â†â†’ CleanupLoop â†â†’ Processor â†â†’ Registry
    â†‘            â†“              â†‘          â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         éœ€è¦å¤æ‚çš„åŒæ­¥å’Œåè°ƒ

è‡ªæ²»è®¾è®¡çš„ä¾èµ–å…³ç³»ï¼ˆç®€å•ï¼‰ï¼š
Scheduler â†’ Registry â† Processor
             â†‘            â†“
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        Processor è‡ªå·±ç®¡ç†è‡ªå·±
```

**å†…èšæ€§çš„æè‡´è¿½æ±‚**ï¼š
```go
// é«˜å†…èšï¼šæ‰€æœ‰ç”Ÿå‘½å‘¨æœŸç›¸å…³é€»è¾‘éƒ½åœ¨ Processor å†…éƒ¨
func (p *UserProcessor) run() {
    // åˆå§‹åŒ–
    idleTimer := time.NewTimer(p.lifetime)
    defer p.cleanup()  // æ¸…ç†ä¹Ÿåœ¨å†…éƒ¨
    
    // è¿è¡Œ
    for {
        select {
        case <-idleTimer.C:
            return  // è‡ªä¸»å†³å®šé€€å‡º
        case msg := <-p.messageChan:
            p.processMessage(msg)
            idleTimer.Reset(p.lifetime)  // è‡ªä¸»ç»­å‘½
        }
    }
}

// å¯¹æ¯”ä¼ ç»Ÿè®¾è®¡ï¼šç”Ÿå‘½å‘¨æœŸé€»è¾‘åˆ†æ•£åœ¨å¤šå¤„
// - Scheduler åˆ›å»º
// - CleanupLoop æ£€æŸ¥
// - Manager åˆ é™¤
// - Processor è¢«åŠ¨æ¥å—

### 10.2 æ¶ˆæ¯é¡ºåºæ€§ä¿è¯

**åœºæ™¯åˆ†æï¼šProcessor è¶…æ—¶åˆ‡æ¢æ—¶çš„æ¶ˆæ¯é¡ºåº**

```
æ—¶é—´çº¿ï¼š
T1: User123 çš„ processor P1 ç”Ÿå‘½å‘¨æœŸåˆ°æœŸ
T2: P1 æ ‡è®°ä¸º StateDyingï¼Œå¼€å§‹ drainMessages
T3: User123 å‘é€æ–°æ¶ˆæ¯ M1
T4: consumeLoop å°è¯•å‘ P1 å‘é€ M1ï¼Œè¢«æ‹’ç»ï¼ˆStateDyingï¼‰
T5: consumeLoop åˆ›å»ºæ–° processor P2
T6: M1 è¢«å‘é€åˆ° P2
T7: P1 å®Œæˆå‰©ä½™æ¶ˆæ¯å¤„ç†ï¼Œè‡ªè¡Œæ¸…ç†
T8: P2 å¼€å§‹å¤„ç† M1
```

**é¡ºåºæ€§ä¿è¯æœºåˆ¶**ï¼š

1. **çŠ¶æ€æ£€æŸ¥**ï¼šTrySend ç¡®ä¿åªæœ‰ Active çŠ¶æ€æ‰æ¥æ”¶æ¶ˆæ¯
2. **ä¼˜é›…é€€å‡º**ï¼šdrainMessages å¤„ç†å®Œé˜Ÿåˆ—ä¸­çš„æ¶ˆæ¯
3. **åŸå­åˆ‡æ¢**ï¼šLoadOrStore ç¡®ä¿æ–°æ—§ processor ä¸ä¼šå¹¶å­˜
4. **é‡è¯•æœºåˆ¶**ï¼šæ¶ˆæ¯è¢«æ‹’ç»åç«‹å³å°è¯•åˆ›å»ºæ–° processor

### 10.3 å¹¶å‘åœºæ™¯è¯¦è§£

#### åœºæ™¯1ï¼šå¹¶å‘åˆ›å»ºåŒä¸€ç”¨æˆ·çš„ Processor

```go
// çº¿ç¨‹Aå’ŒBåŒæ—¶ä¸ºUser123åˆ›å»ºprocessor
Thread A: getOrCreateProcessor("User123")
Thread B: getOrCreateProcessor("User123")

// LoadOrStoreä¿è¯åªæœ‰ä¸€ä¸ªæˆåŠŸåˆ›å»º
actual, loaded := userProcessors.LoadOrStore("User123", processorA)
// loaded=false è¡¨ç¤ºAèµ¢å¾—ç«äº‰
// loaded=true è¡¨ç¤ºBå‘ç°Aå·²åˆ›å»ºï¼Œä½¿ç”¨Açš„processor
```

#### åœºæ™¯2ï¼šæ¶ˆæ¯åˆ°è¾¾æ—¶ Processor æ­£åœ¨æ¸…ç†

```go
// Processorç”Ÿå‘½å‘¨æœŸç®¡ç†
func (p *UserProcessor) cleanup() {
    // 1. å…ˆæ”¹çŠ¶æ€ï¼Œé˜»æ­¢æ–°æ¶ˆæ¯
    p.state.Store(StateDead)
    
    // 2. ä»æ³¨å†Œè¡¨åˆ é™¤ï¼ˆå…³é”®ï¼šé˜²æ­¢æ–°æ¶ˆæ¯è·¯ç”±è¿‡æ¥ï¼‰
    p.registry.Delete(p.userID)
    
    // 3. å®‰å…¨å…³é—­channel
    close(p.messageChan)
}

// æ¶ˆè´¹è€…è·¯ç”±æ¶ˆæ¯æ—¶
if processor.TrySend(msg) != nil {
    // å‘é€å¤±è´¥ï¼Œprocessorå¯èƒ½åœ¨dying
    // ç¨ç­‰åé‡æ–°è·å–æˆ–åˆ›å»º
    time.Sleep(10 * time.Millisecond)
    newProcessor := getOrCreateProcessor(userID)
    newProcessor.TrySend(msg)
}
```

#### åœºæ™¯3ï¼šRebalance æœŸé—´çš„å¹¶å‘æ§åˆ¶

```go
// Kafka rebalanceæ—¶å¯èƒ½å¯¼è‡´ï¼š
// 1. åŒä¸€partitionè¢«é‡æ–°åˆ†é…
// 2. æ¶ˆæ¯å¯èƒ½è¢«é‡å¤æ¶ˆè´¹

// è§£å†³æ–¹æ¡ˆï¼š
// - å¹‚ç­‰æ€§æ£€æŸ¥ï¼ˆMessageDeduplicatorï¼‰
// - Cooperative-stickyåˆ†é…ç­–ç•¥å‡å°‘rebalanceå½±å“
// - æ¶ˆè´¹è€…ä¼˜é›…åœæ­¢ï¼Œå¤„ç†å®Œå½“å‰æ¶ˆæ¯
```

### 10.4 æ­»é”é¢„é˜²ä¸è‡ªæ„ˆèƒ½åŠ›

**è‡ªæ²»è®¾è®¡å¤©ç„¶é¿å…æ­»é”**ï¼š

1. **æ— å¤–éƒ¨é”ä¾èµ–**ï¼šProcessor ä¸éœ€è¦å¤–éƒ¨é”æ¥ç®¡ç†ç”Ÿå‘½å‘¨æœŸ
2. **å•å‘é€šä¿¡**ï¼šåªé€šè¿‡ channel å’ŒåŸå­æ“ä½œé€šä¿¡
3. **è¶…æ—¶ä¿æŠ¤**ï¼šæ‰€æœ‰é˜»å¡æ“ä½œéƒ½æœ‰è¶…æ—¶æœºåˆ¶

**è‡ªæ„ˆèƒ½åŠ›**ï¼š
```go
// å¦‚æœ Processor å´©æºƒ
func (p *UserProcessor) run() {
    defer p.cleanup()  // ä¿è¯æ¸…ç†
    // å³ä½¿ panicï¼Œä¹Ÿä¼šï¼š
    // 1. è‡ªåŠ¨ä» registry ç§»é™¤
    // 2. å…³é—­ channel
    // 3. é‡Šæ”¾æ‰€æœ‰èµ„æº
}

// ç³»ç»Ÿè‡ªåŠ¨æ¢å¤
// - æ–°æ¶ˆæ¯åˆ°è¾¾æ—¶ä¼šåˆ›å»ºæ–°çš„ Processor
// - æ²¡æœ‰åƒµå°¸è¿›ç¨‹
// - æ²¡æœ‰èµ„æºæ³„æ¼
```

```go
// âŒ é”™è¯¯ï¼šå¯èƒ½æ­»é”
func bad() {
    processors.Range(func(k, v interface{}) bool {
        processor := v.(*UserProcessor)
        processors.Delete(k)  // åœ¨Rangeä¸­åˆ é™¤ï¼Œå¯èƒ½æ­»é”
        return true
    })
}

// âœ… æ­£ç¡®ï¼šå…ˆæ”¶é›†ååˆ é™¤
func good() {
    var toDelete []string
    processors.Range(func(k, v interface{}) bool {
        toDelete = append(toDelete, k.(string))
        return true
    })
    for _, key := range toDelete {
        processors.Delete(key)
    }
}
```

### 10.5 åå‹æœºåˆ¶ä¸æ¶æ„ç”¨æˆ·é˜²æŠ¤

#### é—®é¢˜ï¼šé˜Ÿå¤´é˜»å¡ï¼ˆHead-of-Line Blockingï¼‰

**åŸè®¾è®¡çš„ç¼ºé™·**ï¼š
```go
// âŒ æœ‰é—®é¢˜çš„åå‹æœºåˆ¶
if processor.QueueFull() {
    kc.pausePartition(partition)  // æš‚åœæ•´ä¸ªåˆ†åŒºï¼
}

// åæœï¼š
// Partition P1: [User123(æ¶æ„), User456(æ­£å¸¸), User789(æ­£å¸¸)]
// User123 å‘é€1000æ¡/åˆ†é’Ÿ â†’ é˜Ÿåˆ—æ»¡ â†’ P1æš‚åœ â†’ User456å’Œ789è¢«è¿ç´¯
```

**ä¼˜åŒ–åçš„è®¾è®¡**ï¼š
```go
// âœ… ç”¨æˆ·çº§éš”ç¦»
if processor.QueueFull() {
    // åªå½±å“å½“å‰ç”¨æˆ·ï¼Œä¸å½±å“åˆ†åŒºå†…å…¶ä»–ç”¨æˆ·
    processor.circuitBreaker.OnFailure()
    commitOffset(msg)  // ç»§ç»­å¤„ç†å…¶ä»–ç”¨æˆ·
}
```

#### ä¸‰å±‚é˜²æŠ¤ä½“ç³»

**ç¬¬ä¸€å±‚ï¼šç²¾ç¡®é™æµï¼ˆSliding Window Rate Limitingï¼‰**
```go
// æ»‘åŠ¨çª—å£ç®—æ³•ï¼Œç²¾ç¡®æ§åˆ¶æ¶ˆæ¯é¢‘ç‡
é…ç½®å‚æ•°ï¼š
- Nç§’æ—¶é—´çª—å£ï¼ˆå¦‚ï¼š10ç§’ï¼‰
- Xæ¡æ¶ˆæ¯ä¸Šé™ï¼ˆå¦‚ï¼š5æ¡ï¼‰
- Yæ¬¡è¿è§„å°ç¦ï¼ˆå¦‚ï¼š3æ¬¡ï¼‰
- Zå°æ—¶æ‹’ç»æœåŠ¡ï¼ˆå¦‚ï¼š1å°æ—¶ï¼‰

å·¥ä½œåŸç†ï¼š
1. ç»´æŠ¤Nç§’å†…çš„æ¶ˆæ¯æ—¶é—´æˆ³é˜Ÿåˆ—
2. æ–°æ¶ˆæ¯åˆ°è¾¾æ—¶æ¸…ç†è¿‡æœŸæ—¶é—´æˆ³
3. è¶…è¿‡Xæ¡ç«‹å³é™æµï¼Œè¿è§„è®¡æ•°+1
4. è¿ç»­Yæ¬¡è¿è§„ï¼Œå°ç¦Zå°æ—¶
5. å°ç¦æœŸé—´æ‰€æœ‰è¯·æ±‚ç›´æ¥æ‹’ç»
```

**ç¬¬äºŒå±‚ï¼šé˜Ÿåˆ—ç¼“å†²ï¼ˆQueue Bufferï¼‰**
```go
// å¸æ”¶æ­£å¸¸çš„æµé‡æ³¢åŠ¨
é˜Ÿåˆ—å¤§å°ï¼š1000æ¡
ä½œç”¨ï¼šåº”å¯¹çŸ­æ—¶é—´çš„æ¶ˆæ¯çªå‘
ç‰¹ç‚¹ï¼šper-useréš”ç¦»ï¼Œäº’ä¸å½±å“
```

**ç¬¬ä¸‰å±‚ï¼šç†”æ–­å™¨ï¼ˆCircuit Breakerï¼‰**
```go
// æœ€åçš„é˜²çº¿
è§¦å‘æ¡ä»¶ï¼šè¿ç»­100æ¬¡é˜Ÿåˆ—æ»¡
ç†”æ–­æ—¶é—´ï¼š30ç§’
æ¢å¤æœºåˆ¶ï¼šHalf-OpençŠ¶æ€æµ‹è¯•
ä½œç”¨ï¼šéš”ç¦»æ¶æ„/æ•…éšœç”¨æˆ·
```

#### é™æµé…ç½®ç­–ç•¥

```yaml
# é™æµå™¨é…ç½®ï¼ˆconfig.yamlï¼‰
rate_limiter:
  # é»˜è®¤é…ç½®
  default:
    window_seconds: 10      # æ¯10ç§’
    max_messages: 5         # æœ€å¤š5æ¡æ¶ˆæ¯
    penalty_threshold: 3    # è¿ç»­è¿è§„3æ¬¡
    block_hours: 1.0        # å°ç¦1å°æ—¶
  
  # ç”¨æˆ·ç»„é…ç½®ï¼ˆå¯æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´ï¼‰
  groups:
    normal:                 # æ™®é€šç”¨æˆ·
      window_seconds: 10
      max_messages: 5
      penalty_threshold: 3
      block_hours: 1.0
    
    vip:                    # VIPç”¨æˆ·ï¼ˆæ”¾å®½é™åˆ¶ï¼‰
      window_seconds: 10
      max_messages: 20
      penalty_threshold: 10
      block_hours: 0.5
    
    suspicious:             # å¯ç–‘ç”¨æˆ·ï¼ˆä¸¥æ ¼é™åˆ¶ï¼‰
      window_seconds: 30
      max_messages: 3
      penalty_threshold: 1
      block_hours: 24.0
    
    whitelist:              # ç™½åå•ï¼ˆä¸é™åˆ¶ï¼‰
      window_seconds: 1
      max_messages: 9999
      penalty_threshold: 9999
      block_hours: 0.0
```

#### å…¸å‹æ”»å‡»åœºæ™¯ä¸é˜²æŠ¤

**åœºæ™¯1ï¼šæ­£å¸¸ç”¨æˆ·**
```
é…ç½®ï¼š10ç§’5æ¡ï¼Œè¿è§„3æ¬¡å°ç¦1å°æ—¶
è¡Œä¸ºï¼šæ­£å¸¸å¯¹è¯ï¼Œ1-2æ¡/åˆ†é’Ÿ
æ—¶é—´çº¿ï¼š
T0-T10: å‘é€2æ¡ â†’ é€šè¿‡ âœ“
T10-T20: å‘é€1æ¡ â†’ é€šè¿‡ âœ“
ç»“æœï¼šæ°¸è¿œä¸ä¼šè§¦å‘é™æµ
```

**åœºæ™¯2ï¼šæ¿€åŠ¨ç”¨æˆ·**
```
é…ç½®ï¼š10ç§’5æ¡ï¼Œè¿è§„3æ¬¡å°ç¦1å°æ—¶
è¡Œä¸ºï¼šæƒ…ç»ªæ¿€åŠ¨ï¼Œè¿ç»­å‘é€
æ—¶é—´çº¿ï¼š
T0: å‘é€ç¬¬1-5æ¡ â†’ é€šè¿‡ âœ“
T1: å‘é€ç¬¬6æ¡ â†’ é™æµï¼Œè¿è§„1/3 âš ï¸
T2: å‘é€ç¬¬7æ¡ â†’ é™æµï¼Œè¿è§„2/3 âš ï¸
T11: çª—å£æ»‘åŠ¨ï¼Œå‘é€ç¬¬8æ¡ â†’ é€šè¿‡ âœ“ï¼ˆè¿è§„é‡ç½®ï¼‰
ç»“æœï¼šçŸ­æš‚é™æµåæ¢å¤æ­£å¸¸
```

**åœºæ™¯3ï¼šæœºå™¨äººæ”»å‡»**
```
é…ç½®ï¼š10ç§’5æ¡ï¼Œè¿è§„3æ¬¡å°ç¦1å°æ—¶
æ”»å‡»ï¼šBotå‘é€100æ¡/ç§’
æ—¶é—´çº¿ï¼š
T0.00: å‘é€ç¬¬1-5æ¡ â†’ é€šè¿‡ âœ“
T0.01: å‘é€ç¬¬6æ¡ â†’ é™æµï¼Œè¿è§„1/3 âš ï¸
T0.02: å‘é€ç¬¬7æ¡ â†’ é™æµï¼Œè¿è§„2/3 âš ï¸
T0.03: å‘é€ç¬¬8æ¡ â†’ é™æµï¼Œè¿è§„3/3 âš ï¸
T0.04: è§¦å‘å°ç¦ â†’ å°ç¦1å°æ—¶ âŒ
T0.05-T3600: æ‰€æœ‰è¯·æ±‚ç›´æ¥æ‹’ç»
ç»“æœï¼š4æ¯«ç§’å†…è¯†åˆ«å¹¶éš”ç¦»æ”»å‡»è€…
```

**åœºæ™¯4ï¼šæ…¢é€Ÿæ”»å‡»ï¼ˆSlowlorisç±»ä¼¼ï¼‰**
```
é…ç½®ï¼š10ç§’5æ¡ï¼Œè¿è§„3æ¬¡å°ç¦1å°æ—¶
æ”»å‡»ï¼šæ¯2ç§’1æ¡ï¼Œä½†æ¯æ¡å¤„ç†éœ€10ç§’
æ—¶é—´çº¿ï¼š
T0-T10: å‘é€5æ¡ â†’ é€šè¿‡ï¼ˆä½†å¤„ç†ç¼“æ…¢ï¼‰
T10-T20: å‘é€5æ¡ â†’ ç¬¬6æ¡è§¦å‘é™æµï¼Œè¿è§„1/3
T20-T30: æŒç»­å‘é€ â†’ è¿è§„2/3, 3/3
T30: è§¦å‘å°ç¦
ç»“æœï¼šè™½ç„¶å‘é€é¢‘ç‡ä¸é«˜ï¼Œä½†ä»ä¼šè¢«å°ç¦
```

#### é€’å¢æƒ©ç½šæœºåˆ¶ï¼ˆå¯é€‰ï¼‰

```go
// ProgressivePenalty é€’å¢æƒ©ç½šç­–ç•¥
type ProgressivePenalty struct {
    baseBlockHours   float64      // åŸºç¡€å°ç¦æ—¶é•¿ï¼ˆå¦‚1å°æ—¶ï¼‰
    multiplier       float64      // å€æ•°é€’å¢ï¼ˆå¦‚2å€ï¼‰
    maxBlockHours    float64      // æœ€å¤§å°ç¦æ—¶é•¿ï¼ˆå¦‚24å°æ—¶ï¼‰
    penaltyHistory   []time.Time  // å†å²å°ç¦è®°å½•
}

// è®¡ç®—å°ç¦æ—¶é•¿
func (pp *ProgressivePenalty) calculateBlockDuration() time.Duration {
    // ç»Ÿè®¡24å°æ—¶å†…çš„è¿è§„æ¬¡æ•°
    recentViolations := pp.countRecentViolations(24 * time.Hour)
    
    // æŒ‡æ•°é€’å¢ï¼š1h â†’ 2h â†’ 4h â†’ 8h â†’ 16h â†’ 24hï¼ˆä¸Šé™ï¼‰
    hours := pp.baseBlockHours * math.Pow(pp.multiplier, float64(recentViolations))
    if hours > pp.maxBlockHours {
        hours = pp.maxBlockHours
    }
    
    return time.Duration(hours * float64(time.Hour))
}

// ä½¿ç”¨ç¤ºä¾‹
æƒ©ç½šå‡çº§è¡¨ï¼š
ç¬¬1æ¬¡è¿è§„ï¼šå°ç¦1å°æ—¶
ç¬¬2æ¬¡è¿è§„ï¼šå°ç¦2å°æ—¶
ç¬¬3æ¬¡è¿è§„ï¼šå°ç¦4å°æ—¶
ç¬¬4æ¬¡è¿è§„ï¼šå°ç¦8å°æ—¶
ç¬¬5æ¬¡è¿è§„ï¼šå°ç¦16å°æ—¶
ç¬¬6æ¬¡åŠä»¥ä¸Šï¼šå°ç¦24å°æ—¶ï¼ˆä¸Šé™ï¼‰
```

#### è¿ç»´ç®¡ç†æ¥å£

```go
// RateLimiterAdmin è¿ç»´ç®¡ç†æ¥å£
type RateLimiterAdmin interface {
    // æŸ¥è¯¢ç”¨æˆ·çŠ¶æ€
    GetUserStatus(userID string) (*UserRateLimitStatus, error)
    
    // æ‰‹åŠ¨è§£å°ï¼ˆç´§æ€¥æƒ…å†µï¼‰
    Unblock(userID string, reason string) error
    
    // ä¸´æ—¶è°ƒæ•´é™åˆ¶ï¼ˆç‰¹æ®Šæ´»åŠ¨ï¼‰
    AdjustLimit(userID string, tempConfig *RateLimiterConfig) error
    
    // ç™½åå•ç®¡ç†
    AddToWhitelist(userID string) error
    RemoveFromWhitelist(userID string) error
    
    // é»‘åå•ç®¡ç†
    GetBlockedUsers() ([]BlockedUser, error)
    PermanentlyBlock(userID string, reason string) error
}

// ç®¡ç†APIç¤ºä¾‹
GET /admin/ratelimit/status/{userID}
POST /admin/ratelimit/unblock/{userID}
POST /admin/ratelimit/whitelist/{userID}
GET /admin/ratelimit/blocked
```

#### ç›‘æ§ä¸å‘Šè­¦

```yaml
# é™æµç›¸å…³æŒ‡æ ‡
rate_limit_metrics:
  # å®æ—¶æŒ‡æ ‡
  - name: rate_limit_violations_total
    description: é™æµè¿è§„æ¬¡æ•°
    alert: > 100/hour
    
  - name: users_blocked_total
    description: ç”¨æˆ·å°ç¦æ¬¡æ•°
    alert: > 10/hour
    
  - name: blocked_users_current
    description: å½“å‰è¢«å°ç¦ç”¨æˆ·æ•°
    alert: > 50
    
  - name: messages_denied_total
    description: æ‹’ç»æ¶ˆæ¯æ€»æ•°
    alert: > 1000/hour

# å‘Šè­¦åˆ†çº§ï¼ˆå¢å¼ºç‰ˆï¼‰
alerts:
  - level: INFO
    condition: å•æ¬¡é™æµè§¦å‘
    action: è®°å½•æ—¥å¿—
    
  - level: WARNING  
    condition: ç”¨æˆ·è¿ç»­è¿è§„2æ¬¡
    action: è®°å½•è¯¦ç»†æ—¥å¿— + ç›‘æ§é¢æ¿å‘Šè­¦
    
  - level: CRITICAL
    condition: ç”¨æˆ·è¢«å°ç¦
    action: é€šçŸ¥è¿ç»´ + è‡ªåŠ¨è¯„ä¼°æ˜¯å¦æ°¸ä¹…å°ç¦
    details: |
      - ç”¨æˆ·ID
      - è¿è§„æ¬¡æ•°
      - å°ç¦æ—¶é•¿
      - æœ€è¿‘100æ¡æ¶ˆæ¯æ ·æœ¬
    
  - level: EMERGENCY
    condition: 10ä¸ªä»¥ä¸Šç”¨æˆ·åŒæ—¶è¢«å°ç¦
    action: ç«‹å³äººå·¥ä»‹å…¥ + å¯èƒ½é­å—DDoSæ”»å‡»
```

#### é•¿æœŸä¸»ä¹‰çš„è®¾è®¡ç†å¿µ

**ä¸ºä»€ä¹ˆè¦ç²¾ç¡®çš„é™æµæ§åˆ¶ï¼Ÿ**

1. **å¢¨è²å®šå¾‹**ï¼šå¯èƒ½å‡ºé”™çš„äº‹æƒ…ç»ˆå°†å‡ºé”™
2. **ç”Ÿäº§æ— å°äº‹**ï¼šä¸€ä¸ªæ¶æ„ç”¨æˆ·å¯èƒ½å½±å“æ•´ä¸ªç³»ç»Ÿ
3. **æˆæœ¬è€ƒè™‘**ï¼šé˜²æŠ¤çš„æˆæœ¬ << æ•…éšœçš„æˆæœ¬
4. **ç”¨æˆ·ä½“éªŒ**ï¼š99.9%çš„æ­£å¸¸ç”¨æˆ·ä¸åº”è¢«0.1%çš„å¼‚å¸¸ç”¨æˆ·å½±å“

**æ»‘åŠ¨çª—å£ç®—æ³•çš„ä¼˜åŠ¿**ï¼š
```
å¯¹æ¯”å…¶ä»–é™æµç®—æ³•ï¼š

1. å›ºå®šçª—å£ç®—æ³•ï¼š
   é—®é¢˜ï¼šçª—å£è¾¹ç•Œçªå‘æµé‡
   ç¤ºä¾‹ï¼š59ç§’å‘10æ¡ï¼Œ61ç§’å‘10æ¡ï¼Œ2ç§’å†…20æ¡
   
2. ä»¤ç‰Œæ¡¶ç®—æ³•ï¼š
   é—®é¢˜ï¼šå®ç°å¤æ‚ï¼Œå‚æ•°éš¾è°ƒ
   
3. æ»‘åŠ¨çª—å£ç®—æ³•ï¼ˆæˆ‘ä»¬çš„é€‰æ‹©ï¼‰ï¼š
   ä¼˜åŠ¿ï¼šç²¾ç¡®æ§åˆ¶ï¼Œæ— è¾¹ç•Œé—®é¢˜
   å®ç°ï¼šç®€å•æ¸…æ™°ï¼Œæ˜“äºç†è§£
   æ€§èƒ½ï¼šO(N)ç©ºé—´ï¼ŒO(N)æ—¶é—´ï¼ŒNå¾ˆå°ï¼ˆå¦‚5ï¼‰
```

**è®¾è®¡æƒè¡¡**ï¼š
```
å®ç°æˆæœ¬ï¼š
- æ»‘åŠ¨çª—å£å®ç°ï¼š~100è¡Œä»£ç 
- æƒ©ç½šæœºåˆ¶ï¼š~50è¡Œä»£ç 
- ç®¡ç†æ¥å£ï¼š~100è¡Œä»£ç 

è¿ç»´æ”¶ç›Šï¼š
- ç²¾ç¡®æ§åˆ¶æ¯ä¸ªç”¨æˆ·çš„æ¶ˆæ¯é¢‘ç‡
- è‡ªåŠ¨è¯†åˆ«å¹¶éš”ç¦»æ¶æ„ç”¨æˆ·
- çµæ´»çš„é…ç½®å’Œç®¡ç†èƒ½åŠ›
- å®Œå–„çš„ç›‘æ§å’Œå‘Šè­¦ä½“ç³»

ç»“è®ºï¼šç²¾ç¡®é™æµæ˜¯ç”Ÿäº§ç³»ç»Ÿçš„å¿…éœ€å“
```

### 10.6 è®¾è®¡å“²å­¦ï¼šçœŸæ­£çš„è‡ªæ²»

è¿™ä¸ªè®¾è®¡è¾¾åˆ°äº†åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡çš„ç†æƒ³çŠ¶æ€ï¼š

**Linus Torvalds çš„ "Good Taste" ä½“ç°**ï¼š
> "Bad programmers worry about the code. Good programmers worry about data structures and their relationships."

è‡ªæ²»è®¾è®¡å®Œç¾è¯ é‡Šäº†è¿™ä¸€ç‚¹ï¼šé€šè¿‡æ”¹å˜æ•°æ®ç»“æ„çš„å…³ç³»ï¼ˆä»å¤–éƒ¨ç®¡ç†åˆ°è‡ªæˆ‘ç®¡ç†ï¼‰ï¼Œå½»åº•ç®€åŒ–äº†ç³»ç»Ÿå¤æ‚åº¦ã€‚

**è®¾è®¡åŸåˆ™å¯¹æ¯”**ï¼š
| æ–¹é¢ | ä¼ ç»Ÿè®¾è®¡ | è‡ªæ²»è®¾è®¡ |
|------|---------|---------|
| ç”Ÿå‘½å‘¨æœŸç®¡ç† | å¤–éƒ¨ CleanupLoop | å†…éƒ¨ idleTimer |
| å¹¶å‘æ§åˆ¶ | å¤æ‚çš„é”æœºåˆ¶ | åŸå­çŠ¶æ€ + Channel |
| é”™è¯¯æ¢å¤ | éœ€è¦å¤–éƒ¨å¹²é¢„ | è‡ªåŠ¨è‡ªæ„ˆ |
| ä»£ç å¤æ‚åº¦ | é«˜ï¼ˆå¤šå¤„åè°ƒï¼‰ | ä½ï¼ˆé«˜å†…èšï¼‰ |
| ç«æ€æ¡ä»¶ | å­˜åœ¨ç†è®ºé£é™© | **ä»æ ¹æœ¬ä¸Šä¸å¯èƒ½** |
| ç»´æŠ¤æˆæœ¬ | é«˜ | ä½ |

**æ¶æ„å¯ç¤º**ï¼š
1. **ä¸æ˜¯é€šè¿‡æ·»åŠ æœºåˆ¶è§£å†³é—®é¢˜ï¼Œè€Œæ˜¯é€šè¿‡æ”¹å˜è§†è§’æ¶ˆé™¤é—®é¢˜**
2. **æœ€å¥½çš„å¹¶å‘æ§åˆ¶æ˜¯ä¸éœ€è¦å¹¶å‘æ§åˆ¶**
3. **è‡ªæ²»æ¯”ç®¡ç†æ›´å¯é **
4. **ç®€å•æ˜¯ç»ˆæçš„å¤æ‚**

## 11. å•ç‚¹åˆ°é›†ç¾¤çš„å¹³æ»‘è¿‡æ¸¡è®¾è®¡

### 11.1 ç¯å¢ƒå·®å¼‚å¯¹æ¯”

| ç‰¹æ€§ | å•ç‚¹Kafka | Kafkaé›†ç¾¤ | å½±å“ |
|------|-----------|-----------|------|
| **é«˜å¯ç”¨æ€§** | âŒ å•ç‚¹æ•…éšœ | âœ… è‡ªåŠ¨æ•…éšœè½¬ç§» | é›†ç¾¤æ‰èƒ½çœŸæ­£é«˜å¯ç”¨ |
| **æ•°æ®æŒä¹…æ€§** | âš ï¸ å•å‰¯æœ¬ | âœ… å¤šå‰¯æœ¬ä¿éšœ | é›†ç¾¤æ•°æ®æ›´å®‰å…¨ |
| **è´Ÿè½½å‡è¡¡** | âŒ å•èŠ‚ç‚¹æ‰¿è½½ | âœ… è‡ªåŠ¨Rebalance | é›†ç¾¤æ”¯æŒæ°´å¹³æ‰©å±• |
| **DLQå¯é æ€§** | âš ï¸ é™çº§åˆ°æ–‡ä»¶ | âœ… å¤šå‰¯æœ¬DLQ | é›†ç¾¤DLQæ›´å¯é  |
| **æ€§èƒ½ä¸Šé™** | å—é™äºå•æœº | çº¿æ€§æ‰©å±• | é›†ç¾¤æ€§èƒ½æ— ä¸Šé™ |

### 11.2 é…ç½®ç®¡ç†ç­–ç•¥

#### ç¯å¢ƒæ„ŸçŸ¥é…ç½®ï¼ˆæ¨èï¼‰
```yaml
# config/development.yaml
kafka:
  environment: development
  brokers: "localhost:9092"
  topics:
    main:
      partitions: 4
      replication_factor: 1      # âš ï¸ å…³é”®è°ƒæ•´
      min_insync_replicas: 1     # âš ï¸ å…³é”®è°ƒæ•´
    dlq:
      partitions: 2
      replication_factor: 1      # âš ï¸ å…³é”®è°ƒæ•´
      min_insync_replicas: 1     # âš ï¸ å…³é”®è°ƒæ•´

# config/production.yaml  
kafka:
  environment: production
  brokers: "kafka1:9092,kafka2:9092,kafka3:9092"
  topics:
    main:
      partitions: 12
      replication_factor: 3
      min_insync_replicas: 2
    dlq:
      partitions: 6
      replication_factor: 3
      min_insync_replicas: 2
```

#### ä»£ç é€‚é…ç¤ºä¾‹
```go
// ç¯å¢ƒæ„ŸçŸ¥çš„Topicåˆ›å»º
func CreateTopics(adminClient *kafka.AdminClient, env string) error {
    cfg := config.GetKafkaConfig(env)
    
    topicSpecs := []kafka.TopicSpecification{
        {
            Topic:             cfg.Topics.Main.Name,
            NumPartitions:     cfg.Topics.Main.Partitions,
            ReplicationFactor: cfg.Topics.Main.ReplicationFactor,
            Config: map[string]string{
                "min.insync.replicas": strconv.Itoa(cfg.Topics.Main.MinInsyncReplicas),
            },
        },
    }
    
    // åˆ›å»ºTopicæ—¶è‡ªåŠ¨é€‚é…ç¯å¢ƒ
    _, err := adminClient.CreateTopics(context.Background(), topicSpecs)
    return err
}
```

### 11.3 è¿‡æ¸¡æœŸé—´çš„æ³¨æ„äº‹é¡¹

#### å•ç‚¹ç¯å¢ƒé™åˆ¶
1. **æ— çœŸæ­£é«˜å¯ç”¨**ï¼šéœ€è¦å‡†å¤‡é™çº§æ–¹æ¡ˆ
2. **æ€§èƒ½ç“¶é¢ˆæ˜æ˜¾**ï¼šæ§åˆ¶æ¶ˆæ¯æµé‡
3. **æ— æ•…éšœè‡ªæ„ˆ**ï¼šéœ€è¦äººå·¥å¹²é¢„æµç¨‹

#### å‡çº§åˆ°é›†ç¾¤çš„æ­¥éª¤
```bash
# Phase 1: å•ç‚¹è¿è¡Œï¼ˆå¼€å‘/æµ‹è¯•ï¼‰
1. éƒ¨ç½²å•ç‚¹Kafka
2. åˆ›å»ºTopicï¼ˆreplication-factor=1ï¼‰
3. å¯åŠ¨åº”ç”¨ï¼ˆé…ç½®æŒ‡å‘å•ç‚¹ï¼‰
4. åŠŸèƒ½éªŒè¯

# Phase 2: é›†ç¾¤å‡†å¤‡
1. éƒ¨ç½²3èŠ‚ç‚¹Kafkaé›†ç¾¤
2. æ•°æ®è¿ç§»ï¼ˆå¦‚éœ€è¦ï¼‰
3. é‡å»ºTopicï¼ˆreplication-factor=3ï¼‰

# Phase 3: å¹³æ»‘åˆ‡æ¢
1. åœæ­¢åº”ç”¨
2. æ›´æ–°é…ç½®æ–‡ä»¶ï¼ˆbrokersåˆ—è¡¨ï¼‰
3. é‡å¯åº”ç”¨
4. éªŒè¯åŠŸèƒ½
```

### 11.4 ä»£ç è®¾è®¡çš„è¿‡æ¸¡å‹å¥½æ€§

æœ¬è®¾è®¡**å¤©ç„¶æ”¯æŒå¹³æ»‘è¿‡æ¸¡**ï¼š

âœ… **é…ç½®å¤–éƒ¨åŒ–**ï¼šæ‰€æœ‰ç¯å¢ƒç›¸å…³å‚æ•°å¯é…ç½®
âœ… **æ— ç¡¬ç¼–ç **ï¼šbrokeråœ°å€ã€topicé…ç½®å‡å¯è°ƒæ•´
âœ… **é™çº§æœºåˆ¶**ï¼šå•ç‚¹æ•…éšœæœ‰åº”æ€¥æ–¹æ¡ˆ
âœ… **ç›‘æ§å®Œå¤‡**ï¼šå•ç‚¹å’Œé›†ç¾¤ä½¿ç”¨ç›¸åŒç›‘æ§ä½“ç³»

**æ— éœ€ä»£ç ä¿®æ”¹**çš„éƒ¨åˆ†ï¼š
- ç”Ÿäº§è€…/æ¶ˆè´¹è€…é€»è¾‘
- æ¶ˆæ¯å¤„ç†æµç¨‹
- DLQæœºåˆ¶
- ç›‘æ§æŒ‡æ ‡æ”¶é›†

**ä»…éœ€é…ç½®è°ƒæ•´**çš„éƒ¨åˆ†ï¼š
- bootstrap.servers
- replication-factor
- min.insync.replicas

## 12. æ€»ç»“

æœ¬æ–¹æ¡ˆé€šè¿‡ Kafka çš„åˆ†åŒºæœºåˆ¶å’Œ**è‡ªæ²»çš„ç”¨æˆ·çº§å¹¶è¡Œå¤„ç†**ï¼Œå®ç°äº†ï¼š
- âœ… **æ¶ˆæ¯é¡ºåºæ€§**ï¼šåŒä¸€ç”¨æˆ·çš„æ¶ˆæ¯ä¸¥æ ¼æœ‰åº
- âœ… **æ¶ˆæ¯å¯é æ€§**ï¼šAt-least-once + å¹‚ç­‰æ€§ä¿è¯ä¸ä¸¢æ¶ˆæ¯
- âœ… **é«˜å¹¶å‘**ï¼šä¸åŒç”¨æˆ·å®Œå…¨å¹¶è¡Œï¼Œå……åˆ†åˆ©ç”¨èµ„æº
- âœ… **é«˜å¯ç”¨**ï¼š3å‰¯æœ¬ + è‡ªåŠ¨æ•…éšœè½¬ç§»ï¼ˆé›†ç¾¤æ¨¡å¼ï¼‰
- âœ… **å¯æ‰©å±•**ï¼šæ”¯æŒæ°´å¹³æ‰©å±•åˆ°æ›´å¤šèŠ‚ç‚¹
- âœ… **æ— ç«æ€**ï¼šè‡ªæ²»è®¾è®¡ä»æ ¹æœ¬ä¸Šæ¶ˆé™¤ç«æ€æ¡ä»¶
- âœ… **è‡ªæ„ˆæ€§**ï¼šç³»ç»Ÿå…·å¤‡è‡ªåŠ¨æ¢å¤èƒ½åŠ›
- âœ… **æŠ—æ”»å‡»**ï¼šä¸‰å±‚é˜²æŠ¤ä½“ç³»ï¼Œæ¶æ„ç”¨æˆ·æ— æ³•å½±å“ç³»ç»Ÿ
- âœ… **ç”Ÿäº§çº§**ï¼šé•¿æœŸä¸»ä¹‰è®¾è®¡ï¼Œå……åˆ†è€ƒè™‘è¾¹ç¼˜åœºæ™¯
- âœ… **å¹³æ»‘è¿‡æ¸¡**ï¼šä»å•ç‚¹åˆ°é›†ç¾¤æ— éœ€æ”¹ä»£ç ï¼Œä»…éœ€è°ƒé…ç½®

### æ ¸å¿ƒè®¾è®¡è¦ç‚¹å›é¡¾

1. **ç”Ÿäº§è€…**ï¼šçœŸæ­£çš„å¼‚æ­¥å‘é€ï¼Œé¿å…åŒæ­¥é˜»å¡é™·é˜±
2. **æ¶ˆè´¹è€…**ï¼šå¤„ç†æˆåŠŸåæ‰æäº¤ Offsetï¼Œé˜²æ­¢æ¶ˆæ¯ä¸¢å¤±
3. **å¹¶è¡Œæ¨¡å‹**ï¼šåŒä¸€ç”¨æˆ·ä¸²è¡Œï¼Œä¸åŒç”¨æˆ·å¹¶è¡Œ
4. **ç”Ÿå‘½å‘¨æœŸ**ï¼š**Processor å®Œå…¨è‡ªæ²»ï¼Œæ— éœ€å¤–éƒ¨ç®¡ç†**
5. **å®¹é”™æœºåˆ¶**ï¼šå¹‚ç­‰æ€§è®¾è®¡ï¼Œæ”¯æŒæ¶ˆæ¯é‡å¤å¤„ç†
6. **é˜²æŠ¤ä½“ç³»**ï¼š**é™æµ â†’ ç¼“å†² â†’ ç†”æ–­ï¼Œä¸‰å±‚é€’è¿›é˜²æŠ¤**
7. **éš”ç¦»åŸåˆ™**ï¼šæ¶æ„ç”¨æˆ·å½±å“èŒƒå›´é™åˆ¶åœ¨è‡ªèº«ï¼Œä¸å½±å“ä»–äºº

### å®æ–½å»ºè®®

- **ç¬¬ä¸€é˜¶æ®µ**ï¼šå®ç°åŸºç¡€åŠŸèƒ½ï¼Œç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±
- **ç¬¬äºŒé˜¶æ®µ**ï¼šä¼˜åŒ–æ€§èƒ½ï¼Œæ‰¹é‡æäº¤ Offset
- **ç¬¬ä¸‰é˜¶æ®µ**ï¼šå®Œå–„ç›‘æ§ï¼Œå»ºç«‹å‘Šè­¦æœºåˆ¶

### è®¾è®¡ç†å¿µæ€»ç»“

**æ ¸å¿ƒç†å¿µ**ï¼š
- **æ­£ç¡®æ€§ä¼˜äºæ€§èƒ½**ï¼šå®å¯æ…¢ä¹Ÿè¦å¯¹
- **ç®€å•å¯é ä¼˜äºå¤æ‚å®Œç¾**ï¼šå¤æ‚åº¦æ˜¯bugçš„æ¸©åºŠ
- **è‡ªæ²»ä¼˜äºç®¡ç†**ï¼šè®©ç»„ä»¶è‡ªå·±ç®¡ç†è‡ªå·±
- **æ•°æ®ç»“æ„ä¼˜äºç®—æ³•**ï¼šå¥½çš„æ•°æ®ç»“æ„è®©ç®—æ³•å˜ç®€å•

**è¿™ä¸ªè®¾è®¡çš„ç²¾é«“**ï¼š
> é€šè¿‡è®© UserProcessor å®Œå…¨è‡ªæ²»ï¼Œæˆ‘ä»¬ä¸æ˜¯è§£å†³äº†ç«æ€æ¡ä»¶é—®é¢˜ï¼Œè€Œæ˜¯è®©ç«æ€æ¡ä»¶ä»æ ¹æœ¬ä¸Šä¸å¯èƒ½å‘ç”Ÿã€‚è¿™æ˜¯ä¸€ä¸ªæ•™ç§‘ä¹¦çº§åˆ«çš„è®¾è®¡èŒƒä¾‹ï¼Œå±•ç¤ºäº†å¦‚ä½•é€šè¿‡æ”¹å˜é—®é¢˜çš„è§†è§’æ¥å½»åº•ç®€åŒ–ç³»ç»Ÿå¤æ‚åº¦ã€‚

æ­£å¦‚è®¡ç®—æœºç§‘å­¦å®¶ Tony Hoare æ‰€è¯´ï¼š
> "There are two ways of constructing a software design: One way is to make it so simple that there are obviously no deficiencies, and the other way is to make it so complicated that there are no obvious deficiencies. The first method is far more difficult."

è¿™ä¸ªè‡ªæ²»è®¾è®¡é€‰æ‹©äº†ç¬¬ä¸€æ¡è·¯ï¼š**è®©ç³»ç»Ÿç®€å•åˆ°æ˜æ˜¾æ²¡æœ‰ç¼ºé™·**ã€‚
