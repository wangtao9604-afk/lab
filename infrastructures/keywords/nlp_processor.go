package keywords

import (
	"fmt"
	"regexp"
	"sort"
	"strconv"
	"strings"
	"unicode"

	"qywx/infrastructures/common"

	"github.com/mozillazg/go-pinyin"
)

// AdvancedNLPProcessor é«˜çº§NLPå¤„ç†å™¨
type AdvancedNLPProcessor struct {
	segmenter   *AdvancedSegmenter
	dictionary  *KeywordDictionary
	entityRules map[string]*regexp.Regexp
	numberRegex *regexp.Regexp
	initialized bool
}

// NewAdvancedNLPProcessor åˆ›å»ºé«˜çº§NLPå¤„ç†å™¨
func NewAdvancedNLPProcessor(segmenter *AdvancedSegmenter, dict *KeywordDictionary) *AdvancedNLPProcessor {
	processor := &AdvancedNLPProcessor{
		segmenter:   segmenter,
		dictionary:  dict,
		entityRules: make(map[string]*regexp.Regexp),
	}

	processor.initializeEntityRules()
	processor.initialized = true

	return processor
}

// initializeEntityRules åˆå§‹åŒ–å®ä½“è¯†åˆ«è§„åˆ™
func (nlp *AdvancedNLPProcessor) initializeEntityRules() {
	nlp.entityRules = map[string]*regexp.Regexp{
		// ä»·æ ¼ç›¸å…³æ­£åˆ™
		"PRICE_RANGE":  regexp.MustCompile(`(\d+)\s*[-åˆ°è‡³~]\s*(\d+)(ä¸‡|äº¿)|(\d+)(ä¸‡|äº¿)\s*[-åˆ°è‡³~]\s*(\d+)(ä¸‡|äº¿)|(\d+)(ä¸‡|äº¿)ä»¥ä¸‹|(\d+)(ä¸‡|äº¿)ä»¥ä¸Š|é¢„ç®—(\d+)\s*[-åˆ°è‡³~]\s*(\d+)(ä¸‡|äº¿)|é¢„ç®—(\d+)(ä¸‡|äº¿)\s*[-åˆ°è‡³~]\s*(\d+)(ä¸‡|äº¿)|é¢„ç®—(\d+)(ä¸‡|äº¿)ä»¥ä¸Š|é¢„ç®—(\d+)(ä¸‡|äº¿)ä»¥ä¸‹|é¢„ç®—(\d+)(ä¸‡|äº¿)?|(\d+)\s*[-åˆ°è‡³~]\s*(\d+)å…ƒ|(\d+)å…ƒä»¥ä¸‹|(\d+)å…ƒä»¥ä¸Š`),
		"SINGLE_PRICE": regexp.MustCompile(`(\d+(?:\.\d+)?)ä¸‡å·¦å³|(\d+(?:\.\d+)?)ä¸‡å…ƒ$|(\d+(?:\.\d+)?)å…ƒ$`),

		// ç§Ÿé‡‘ç›¸å…³æ­£åˆ™ - æ”¹è¿›ç‰ˆæœ¬
		"RENT_PRICE": regexp.MustCompile(`æœˆç§Ÿé‡‘(\d+)[-åˆ°è‡³~](\d+)|æœˆç§Ÿ(\d+)[-åˆ°è‡³~](\d+)|ç§Ÿé‡‘(\d+)[-åˆ°è‡³~](\d+)å…ƒ?/?æœˆ?|æœˆç§Ÿé‡‘(\d+)åˆ°(\d+)å…ƒ?|ç§Ÿé‡‘(\d+)åˆ°(\d+)å…ƒ?|æœˆç§Ÿé‡‘(\d+)å·¦å³|æœˆç§Ÿ(\d+)å·¦å³|ç§Ÿé‡‘(\d+)å·¦å³|æœˆç§Ÿé‡‘(\d+)|æœˆç§Ÿ(\d+)|ç§Ÿé‡‘(\d+)å…ƒ?/?æœˆ?`),

		// é¢ç§¯ç›¸å…³æ­£åˆ™ - ä¿®å¤åŸºç¡€åŒ¹é…
		"AREA_RANGE": regexp.MustCompile(`(\d+)[-åˆ°è‡³~](\d+)å¹³ç±³?|(\d+)å¹³ç±³?ä»¥ä¸Š|(\d+)å¹³ç±³?ä»¥ä¸‹|é¢ç§¯(\d+)|(\d+)å¹³ç±³?å·¦å³|(\d+)å¹³æ–¹ç±³?|(\d+)å¹³ç±³?`),

		// æˆ¿å‹ç›¸å…³æ­£åˆ™ - æ”¯æŒä¸­æ–‡æ•°å­—ï¼ˆå¥—æˆ¿é€šè¿‡é¢„å¤„ç†è½¬æ¢ï¼‰
		// æ–°å¢ï¼šå•ç‹¬çš„"Nå®¤"æ¨¡å¼ï¼Œæ”¯æŒé˜¿æ‹‰ä¼¯æ•°å­—ã€ä¸­æ–‡æ•°å­—ã€ä¸­æ–‡å¤§å†™æ•°å­—
		"ROOM_LAYOUT": regexp.MustCompile(`(\d+)å®¤(\d+)å…(\d+)å«|(\d+)æˆ¿(\d+)å…|([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾1-9])å±…å®¤|([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾1-9])å®¤([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾1-9])å…|(\d+)æˆ¿(\d+)å…(\d+)å«|(\d+)å®¤(\d+)å…|(\d+)å±…å®¤|(\d+)æˆ¿é—´|(\d+)å®¤|([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾])å®¤`),

		// æ¥¼å±‚ç›¸å…³æ­£åˆ™
		"FLOOR_INFO": regexp.MustCompile(`(\d+)æ¥¼|(\d+)å±‚|(\d+)/(\d+)å±‚|ç¬¬(\d+)å±‚|(\d+)-(\d+)å±‚|é«˜å±‚|ä¸­å±‚|ä½å±‚`),

		// å¹´ä»½ç›¸å…³æ­£åˆ™
		"YEAR_INFO": regexp.MustCompile(`(\d{4})å¹´|å»ºæˆ(\d+)å¹´|æˆ¿é¾„(\d+)å¹´|(\d+)å¹´ä»£|(\d{2})å¹´å»º`),

		// åœ°é“äº¤é€šæ­£åˆ™
		"SUBWAY_INFO": regexp.MustCompile(`(\w+)å·çº¿|åœ°é“(\w+)ç«™|(\w+)åœ°é“ç«™|è·ç¦»(\w+)ç«™|è½¨äº¤(\d+)å·çº¿|(\d+)å·çº¿(\w+)ç«™`),

		// è·ç¦»ç›¸å…³æ­£åˆ™
		"DISTANCE_INFO": regexp.MustCompile(`(\d+)ç±³|(\d+)å…¬é‡Œ|(\d+)km|æ­¥è¡Œ(\d+)åˆ†é’Ÿ|è½¦ç¨‹(\d+)åˆ†é’Ÿ|è·ç¦»(\d+)`),

		// æœå‘ç›¸å…³æ­£åˆ™ - é•¿è¯ä¼˜å…ˆåŒ¹é…
		"ORIENTATION": regexp.MustCompile(`å—åŒ—é€šé€|ä¸œè¥¿æœå‘|æ­£å—å‘|æ­£åŒ—å‘|æ­£ä¸œå‘|æ­£è¥¿å‘|æœå—|æœåŒ—|æœä¸œ|æœè¥¿|ä¸œå—|ä¸œåŒ—|è¥¿å—|è¥¿åŒ—`),

		// è£…ä¿®ç›¸å…³æ­£åˆ™
		"DECORATION": regexp.MustCompile(`æ¯›å¯|ç²¾è£…|ç®€è£…|è±ªè£…|æ‹åŒ…å…¥ä½|å…¨è£…ä¿®|æœªè£…ä¿®|å·²è£…ä¿®|è£…ä¿®å¥½|æ–°è£…ä¿®`),

		// æˆ¿å±‹ç±»å‹æ­£åˆ™
		"PROPERTY_TYPE": regexp.MustCompile(`æ–°æˆ¿|äºŒæ‰‹æˆ¿|æœŸæˆ¿|ç°æˆ¿|åˆ«å¢…|å…¬å¯“|å•†é“º|å†™å­—æ¥¼|æ´‹æˆ¿|è”æ’|ç‹¬æ ‹|å æ‹¼|å•†ä¸šç»¼åˆä½“|ç»¼åˆä½“`),
	}

	// é€šç”¨æ•°å­—æ­£åˆ™
	nlp.numberRegex = regexp.MustCompile(`\d+(?:\.\d+)?`)
}

// convertRoomLayout å±…å®¤è½¬æ¢é¢„å¤„ç† - è§£å†³56%æˆåŠŸç‡é—®é¢˜
func (nlp *AdvancedNLPProcessor) convertRoomLayout(text string) string {
	// ğŸ  å±…å®¤åˆ°å®¤å…çš„è½¬æ¢è§„åˆ™ - ç»Ÿä¸€æ ¼å¼ï¼Œé¿å…åˆ†è¯å†²çª
	conversions := [][]string{
		// ğŸ†• å¥—æˆ¿æ˜ å°„ï¼šå¥—[2/äºŒ,3/ä¸‰,4/å››]åˆ°[2/äºŒ,3/ä¸‰,4/å››]å±…å®¤
		{"å¥—äºŒ", "äºŒå±…å®¤"},
		{"å¥—ä¸‰", "ä¸‰å±…å®¤"},
		{"å¥—å››", "å››å±…å®¤"},
		{"å¥—äº”", "äº”å±…å®¤"},
		{"å¥—2", "2å±…å®¤"},
		{"å¥—3", "3å±…å®¤"},
		{"å¥—4", "4å±…å®¤"},
		{"å¥—5", "5å±…å®¤"},

		// ä¸­æ–‡æ•°å­—å±…å®¤è½¬æ¢
		{"ä¸€å±…å®¤", "ä¸€å®¤ä¸€å…"},
		{"äºŒå±…å®¤", "äºŒå®¤ä¸€å…"},
		{"ä¸‰å±…å®¤", "ä¸‰å®¤ä¸€å…"},
		{"å››å±…å®¤", "å››å®¤ä¸€å…"},
		{"äº”å±…å®¤", "äº”å®¤ä¸€å…"},
		{"å…­å±…å®¤", "å…­å®¤ä¸€å…"},

		// ç‰¹æ®Šå¤„ç†"ä¸¤"
		{"ä¸¤å±…å®¤", "äºŒå®¤ä¸€å…"},

		// é˜¿æ‹‰ä¼¯æ•°å­—å±…å®¤è½¬æ¢
		{"1å±…å®¤", "ä¸€å®¤ä¸€å…"},
		{"2å±…å®¤", "äºŒå®¤ä¸€å…"},
		{"3å±…å®¤", "ä¸‰å®¤ä¸€å…"},
		{"4å±…å®¤", "å››å®¤ä¸€å…"},
		{"5å±…å®¤", "äº”å®¤ä¸€å…"},
		{"6å±…å®¤", "å…­å®¤ä¸€å…"},
	}

	result := text
	for _, conv := range conversions {
		result = strings.ReplaceAll(result, conv[0], conv[1])
	}

	return result
}

// normalizeTextForStability æ ‡å‡†åŒ–æ–‡æœ¬ä»¥æé«˜å®ä½“æå–ç¨³å®šæ€§
func (nlp *AdvancedNLPProcessor) normalizeTextForStability(text string) string {
	// ğŸ  å±…å®¤è½¬æ¢é¢„å¤„ç† - ä¼˜å…ˆå¤„ç†ï¼Œé¿å…åˆ†è¯å†²çª
	normalizedText := nlp.convertRoomLayout(text)

	// ğŸ”§ é€šç”¨ä¸­æ–‡æ•°å­—æ ‡å‡†åŒ–ç³»ç»Ÿ

	// 1. å¤åˆä¸­æ–‡æ•°å­—çš„ç‰¹æ®Šå¤„ç†ï¼ˆé¿å…é”™è¯¯æ‹†åˆ†ï¼‰
	complexNumbers := map[string]string{
		"ä¸€å": "10", "äºŒå": "20", "ä¸‰å": "30", "å››å": "40", "äº”å": "50",
		"å…­å": "60", "ä¸ƒå": "70", "å…«å": "80", "ä¹å": "90", "ä¸€ç™¾": "100",
		"äºŒç™¾": "200", "ä¸‰ç™¾": "300", "å››ç™¾": "400", "äº”ç™¾": "500",
	}

	// ğŸ”§ ä¿®å¤ç¨³å®šæ€§é—®é¢˜ï¼šç¡®ä¿ç¡®å®šæ€§çš„éå†é¡ºåº
	// å…ˆå¤„ç†å¤åˆæ•°å­—
	var complexKeys []string
	for complex := range complexNumbers {
		complexKeys = append(complexKeys, complex)
	}
	sort.Strings(complexKeys)

	for _, complex := range complexKeys {
		number := complexNumbers[complex]
		normalizedText = strings.ReplaceAll(normalizedText, complex, number)
	}

	// 2. åŸºç¡€ä¸­æ–‡æ•°å­—åˆ°é˜¿æ‹‰ä¼¯æ•°å­—çš„æ˜ å°„
	chineseNumbers := map[string]string{
		"é›¶": "0", "ä¸€": "1", "äºŒ": "2", "ä¸¤": "2", "ä¸‰": "3", "å››": "4", "äº”": "5",
		"å…­": "6", "ä¸ƒ": "7", "å…«": "8", "ä¹": "9", "å": "10",
	}

	// 3. æˆ¿åœ°äº§é¢†åŸŸç‰¹å®šçš„æ ‡å‡†åŒ–è§„åˆ™
	housingPatterns := []string{
		"å±…å®¤", "å®¤", "å…", "å«", "æˆ¿", "å±‚", "æ¥¼", "æ ‹", "å•å…ƒ", "å·",
		"å¹³", "å¹³ç±³", "å¹³æ–¹", "å¹³æ–¹ç±³", "ä¸‡", "å…ƒ", "å—", "åƒ", "ç™¾",
	}

	// 4. é€šç”¨æ•°å­—ç›¸å…³åç¼€
	numberSuffixes := []string{
		"ä¸ª", "ä½", "å¥—", "é—´", "å¤„", "å®¶", "åº§", "å°", "éƒ¨", "è¾†",
		"å¹´", "æœˆ", "æ—¥", "å¤©", "å°æ—¶", "åˆ†é’Ÿ", "ç±³", "åƒç±³", "å…¬é‡Œ",
		"å…¬æ–¤", "æ–¤", "å¨", "å‡", "æ¯«å‡", "åº¦", "æ‘„æ°åº¦", "åæ°åº¦",
	}

	// 5. ç»„åˆæ‰€æœ‰éœ€è¦æ ‡å‡†åŒ–çš„åç¼€
	allSuffixes := append(housingPatterns, numberSuffixes...)

	// 6. å¯¹å‰©ä½™çš„å•ä¸ªä¸­æ–‡æ•°å­—è¿›è¡Œæ ‡å‡†åŒ–
	for chinese, arabic := range chineseNumbers {
		// å¤„ç†å¸¦åç¼€çš„ä¸­æ–‡æ•°å­—
		for _, suffix := range allSuffixes {
			normalizedText = strings.ReplaceAll(normalizedText, chinese+suffix, arabic+suffix)
		}
	}

	// 7. æ ‡å‡†åŒ–å¸¸è§çš„å˜ä½“è¡¨è¾¾ - å¤„ç†é¡ºåºå¾ˆé‡è¦
	// å…ˆå¤„ç†å¸¦"å¹³"çš„å¤åˆå•ä½ï¼Œé¿å…é‡å¤
	normalizedText = strings.ReplaceAll(normalizedText, "å¹³ã¡", "å¹³ç±³")
	normalizedText = strings.ReplaceAll(normalizedText, "å¹³æ–¹ç±³", "å¹³ç±³")
	normalizedText = strings.ReplaceAll(normalizedText, "å¹³æ–¹", "å¹³ç±³")
	normalizedText = strings.ReplaceAll(normalizedText, "mÂ²", "å¹³ç±³")

	variants := map[string]string{
		// é¢ç§¯ç›¸å…³ - åªå¤„ç†å•ç‹¬çš„å•ä½ç¬¦å·
		"ã¡": "å¹³ç±³",

		// ä»·æ ¼ç›¸å…³
		"å—é’±": "å…ƒ", "åœ†": "å…ƒ", "å—": "å…ƒ", "åˆ€": "å…ƒ",
		"ä¸‡å—": "ä¸‡å…ƒ", "ä¸‡åœ†": "ä¸‡å…ƒ", "ä¸‡åˆ€": "ä¸‡å…ƒ",

		// æ•°é‡ç›¸å…³
		"å‡ ä¸ª": "å¤šä¸ª", "è‹¥å¹²": "å¤šä¸ª", "ä¸€äº›": "å¤šä¸ª",

		// æœå‘ç›¸å…³ - æ›´å…¨é¢çš„æ˜ å°„
		"å—é¢": "æœå—", "åŒ—é¢": "æœåŒ—", "ä¸œé¢": "æœä¸œ", "è¥¿é¢": "æœè¥¿",
		"å‘å—": "æœå—", "å‘åŒ—": "æœåŒ—", "å‘ä¸œ": "æœä¸œ", "å‘è¥¿": "æœè¥¿",
		"å‘å—é¢": "æœå—", "å‘åŒ—é¢": "æœåŒ—", "å‘ä¸œé¢": "æœä¸œ", "å‘è¥¿é¢": "æœè¥¿",
		"å—å‘": "æœå—", "åŒ—å‘": "æœåŒ—", "ä¸œå‘": "æœä¸œ", "è¥¿å‘": "æœè¥¿",

		// è£…ä¿®ç›¸å…³
		"è£…ä¿®è¿‡": "å·²è£…ä¿®", "è£…å¥½": "å·²è£…ä¿®", "è£…ä¿®å®Œ": "å·²è£…ä¿®",
		"æ²¡è£…ä¿®": "æœªè£…ä¿®", "æœªè£…": "æœªè£…ä¿®", "æ¯›èƒš": "æ¯›å¯",

		// ä½ç½®ç›¸å…³
		"é™„è¿‘": "å‘¨è¾¹", "æ—è¾¹": "å‘¨è¾¹", "è¾¹ä¸Š": "å‘¨è¾¹", "ä¸´è¿‘": "å‘¨è¾¹",

		// æ—¶é—´ç›¸å…³
		"å‰å¹´": "2å¹´å‰", "å»å¹´": "1å¹´å‰", "ä»Šå¹´": "0å¹´", "æ˜å¹´": "1å¹´å",
	}

	for variant, standard := range variants {
		normalizedText = strings.ReplaceAll(normalizedText, variant, standard)
	}

	// 8. æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹ç¬¦å·
	normalizedText = regexp.MustCompile(`\s+`).ReplaceAllString(normalizedText, " ")
	normalizedText = strings.TrimSpace(normalizedText)

	// 9. ç»Ÿä¸€å…¨è§’åŠè§’å­—ç¬¦
	fullToHalfMap := map[string]string{
		"ï¼": "0", "ï¼‘": "1", "ï¼’": "2", "ï¼“": "3", "ï¼”": "4",
		"ï¼•": "5", "ï¼–": "6", "ï¼—": "7", "ï¼˜": "8", "ï¼™": "9",
		"ï¼ˆ": "(", "ï¼‰": ")", "ã€": "[", "ã€‘": "]", "ã€Š": "<", "ã€‹": ">",
		"ï¼Œ": ",", "ã€‚": ".", "ï¼›": ";", "ï¼š": ":", "ï¼": "!", "ï¼Ÿ": "?",
	}

	// ğŸ”§ ä¿®å¤ç¨³å®šæ€§é—®é¢˜ï¼šç¡®ä¿ç¡®å®šæ€§çš„éå†é¡ºåº
	var fullKeys []string
	for full := range fullToHalfMap {
		fullKeys = append(fullKeys, full)
	}
	sort.Strings(fullKeys)

	for _, full := range fullKeys {
		half := fullToHalfMap[full]
		normalizedText = strings.ReplaceAll(normalizedText, full, half)
	}

	return normalizedText
}

func (nlp *AdvancedNLPProcessor) ExtractEntities(text string) ([]Entity, error) {
	if !nlp.initialized {
		return nil, ErrNotInitialized
	}

	if text == "" {
		return nil, ErrEmptyInput
	}

	// æ ‡å‡†åŒ–æ–‡æœ¬ä»¥æé«˜ç¨³å®šæ€§
	normalizedText := nlp.normalizeTextForStability(text)

	var entities []Entity

	// 1. åŸºäºæ­£åˆ™è¡¨è¾¾å¼çš„å®ä½“æå–ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ–‡æœ¬ï¼‰
	regexEntities := nlp.extractRegexEntities(normalizedText)
	entities = append(entities, regexEntities...)

	// 2. åŸºäºè¯å…¸çš„å®ä½“æå–ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ–‡æœ¬ï¼‰
	dictEntities := nlp.extractDictionaryEntities(normalizedText)
	entities = append(entities, dictEntities...)

	// 3. åŸºäºåˆ†è¯çš„å®ä½“æå–ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ–‡æœ¬ï¼‰
	segmentEntities := nlp.extractSegmentEntities(normalizedText)
	entities = append(entities, segmentEntities...)

	// 4. åœ°ç†ä½ç½®å®ä½“æå–ï¼ˆä½¿ç”¨æ ‡å‡†åŒ–æ–‡æœ¬ï¼‰
	locationEntities := nlp.extractLocationEntities(normalizedText)
	entities = append(entities, locationEntities...)

	return nlp.deduplicateEntities(entities), nil
}

// extractRegexEntities åŸºäºæ­£åˆ™è¡¨è¾¾å¼æå–å®ä½“
func (nlp *AdvancedNLPProcessor) extractRegexEntities(text string) []Entity {
	var entities []Entity

	for entityType, regex := range nlp.entityRules {
		matches := regex.FindAllStringSubmatch(text, -1)
		for _, match := range matches {
			if len(match) > 0 {
				entity := Entity{
					Text:       match[0],
					Type:       entityType,
					Start:      strings.Index(text, match[0]),
					End:        strings.Index(text, match[0]) + len(match[0]),
					Confidence: 0.8,
				}

				// è§£æå®ä½“å€¼
				entity.Value = nlp.parseEntityValue(entityType, match)
				entities = append(entities, entity)
			}
		}
	}

	return entities
}

// extractDictionaryEntities åŸºäºè¯å…¸æå–å®ä½“
func (nlp *AdvancedNLPProcessor) extractDictionaryEntities(text string) []Entity {
	var entities []Entity

	// æå–å„ç±»è¯å…¸åŒ¹é… - commercialä¼˜å…ˆï¼Œé¿å…è¢«locationè¯¯åŒ¹é…
	categories := []string{"commercial", "property_type", "decoration", "orientation", "facility", "interest_point", "location"}

	for _, category := range categories {
		matches := nlp.dictionary.MatchKeywords(text, category)
		for _, match := range matches {
			// æŸ¥æ‰¾åŒ¹é…ä½ç½®
			keywords := nlp.dictionary.GetCategoryKeywords(category)

			// ğŸ”§ ä¿®å¤ç¨³å®šæ€§é—®é¢˜ï¼šç¡®ä¿ç¡®å®šæ€§çš„éå†é¡ºåº
			var standardKeys []string
			for standardKey := range keywords {
				standardKeys = append(standardKeys, standardKey)
			}
			sort.Strings(standardKeys)

			for _, standardKey := range standardKeys {
				keywordList := keywords[standardKey]
				if standardKey == match {
					for _, keyword := range keywordList {
						if strings.Contains(strings.ToLower(text), strings.ToLower(keyword)) {
							start := strings.Index(strings.ToLower(text), strings.ToLower(keyword))
							entity := Entity{
								Text:       keyword,
								Type:       strings.ToUpper(category),
								Start:      start,
								End:        start + len(keyword),
								Confidence: 0.7,
								Value:      standardKey,
							}
							entities = append(entities, entity)
							break
						}
					}
					break
				}
			}
		}
	}

	return entities
}

// extractSegmentEntities åŸºäºåˆ†è¯æå–å®ä½“
func (nlp *AdvancedNLPProcessor) extractSegmentEntities(text string) []Entity {
	var entities []Entity

	if nlp.segmenter == nil {
		return entities
	}

	// å¸¦è¯æ€§æ ‡æ³¨çš„åˆ†è¯
	segments := nlp.segmenter.CutWithTag(text)

	// åŸºäºè¯æ€§æå–å®ä½“
	for _, segment := range segments {
		entityType := nlp.mapPosToEntityType(segment.Tag)
		if entityType != "" {
			start := strings.Index(text, segment.Word)
			entity := Entity{
				Text:       segment.Word,
				Type:       entityType,
				Start:      start,
				End:        start + len(segment.Word),
				Confidence: 0.6,
				Value:      segment.Word,
			}
			entities = append(entities, entity)
		}
	}

	return entities
}

// extractLocationEntities æå–åœ°ç†ä½ç½®å®ä½“
func (nlp *AdvancedNLPProcessor) extractLocationEntities(text string) []Entity {
	var entities []Entity

	// ä¸­å›½çœå¸‚åŒºåŒ¹é…
	locationDict := nlp.dictionary.GetCategoryKeywords("location")

	// å…ˆæ”¶é›†æ‰€æœ‰åŒ¹é…çš„çœä»½å’ŒåŒºå¿
	var foundProvinces []string
	var foundDistricts []Entity

	for province, districts := range locationDict {
		// åŒ¹é…çœä»½
		if strings.Contains(text, province) {
			start := strings.Index(text, province)
			entities = append(entities, Entity{
				Text:       province,
				Type:       "PROVINCE",
				Start:      start,
				End:        start + len(province),
				Confidence: 0.9,
				Value:      province,
			})
			foundProvinces = append(foundProvinces, province)
		}

		// åŒ¹é…åŒºå¿ - ç‹¬ç«‹äºçœä»½åŒ¹é…
		for _, district := range districts {
			if strings.Contains(text, district) {
				start := strings.Index(text, district)

				// ğŸ†• ç‰¹æ®Šå¤„ç†ä¸Šæµ·æ¿å—
				if province == "ä¸Šæµ·" {
					// å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºæ¿å—ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
					if isPlate, districtName := IsShanghaiPlate(district); isPlate {
						// è¿™æ˜¯ä¸€ä¸ªæ¿å—ï¼Œåˆ›å»ºPLATEå®ä½“
						districtEntity := Entity{
							Text:       district,
							Type:       "PLATE",
							Start:      start,
							End:        start + len(district),
							Confidence: 0.95,
							Value:      map[string]string{"province": province, "district": districtName, "plate": district},
						}
						foundDistricts = append(foundDistricts, districtEntity)
					} else if strings.Contains(district, "åŒº") {
						// è¿™æ˜¯ä¸€ä¸ªåŒºåŸŸï¼ˆå¸¦"åŒº"å­—çš„ï¼‰
						districtEntity := Entity{
							Text:       district,
							Type:       "DISTRICT",
							Start:      start,
							End:        start + len(district),
							Confidence: 0.9,
							Value:      map[string]string{"province": province, "district": district},
						}
						foundDistricts = append(foundDistricts, districtEntity)
					} else {
						// å…¶ä»–ä¸Šæµ·åœ°åï¼Œå¯èƒ½æ˜¯ç®€å†™çš„åŒºåæˆ–å…¶ä»–åœ°ç‚¹
						// æ£€æŸ¥æ˜¯å¦æ˜¯ä¸Šæµ·çš„åŒºï¼ˆæµ¦ä¸œã€é—µè¡Œç­‰ä¸å¸¦"åŒº"å­—çš„ï¼‰
						isDistrictName := false
						shanghaiDistricts := []string{
							"æµ¦ä¸œ", "é»„æµ¦", "å¾æ±‡", "é•¿å®", "é™å®‰", "æ™®é™€", "è™¹å£", "æ¨æµ¦",
							"é—µè¡Œ", "å®å±±", "å˜‰å®š", "é‡‘å±±", "æ¾æ±Ÿ", "é’æµ¦", "å¥‰è´¤", "å´‡æ˜",
						}
						for _, d := range shanghaiDistricts {
							if district == d {
								isDistrictName = true
								break
							}
						}

						if isDistrictName {
							// æ˜¯åŒºå
							districtEntity := Entity{
								Text:       district,
								Type:       "DISTRICT",
								Start:      start,
								End:        start + len(district),
								Confidence: 0.9,
								Value:      map[string]string{"province": province, "district": district},
							}
							foundDistricts = append(foundDistricts, districtEntity)
						} else {
							// ä¸æ˜¯åŒºåä¹Ÿä¸æ˜¯æ¿å—ï¼Œä½œä¸ºæ™®é€šLOCATIONå¤„ç†
							districtEntity := Entity{
								Text:       district,
								Type:       "LOCATION",
								Start:      start,
								End:        start + len(district),
								Confidence: 0.8,
								Value:      province,
							}
							foundDistricts = append(foundDistricts, districtEntity)
						}
					}
				} else {
					// å…¶ä»–çœä»½æ­£å¸¸å¤„ç†
					districtEntity := Entity{
						Text:       district,
						Type:       "DISTRICT",
						Start:      start,
						End:        start + len(district),
						Confidence: 0.9,
						Value:      map[string]string{"province": province, "district": district},
					}
					foundDistricts = append(foundDistricts, districtEntity)
				}
			}
		}
	}

	// æ·»åŠ æ‰€æœ‰æ‰¾åˆ°çš„åŒºå¿å®ä½“
	entities = append(entities, foundDistricts...)

	return entities
}

// mapPosToEntityType å°†è¯æ€§æ˜ å°„åˆ°å®ä½“ç±»å‹
func (nlp *AdvancedNLPProcessor) mapPosToEntityType(pos string) string {
	posMapping := map[string]string{
		"ns": "LOCATION",     // åœ°å
		"nt": "ORGANIZATION", // æœºæ„å
		"nz": "OTHER_PROPER", // å…¶ä»–ä¸“å
		"m":  "NUMBER",       // æ•°è¯
		"mq": "QUANTITY",     // æ•°é‡è¯
		"tg": "TIME",         // æ—¶é—´è¯
	}

	return posMapping[pos]
}

// parseEntityValue è§£æå®ä½“å€¼
func (nlp *AdvancedNLPProcessor) parseEntityValue(entityType string, match []string) interface{} {
	switch entityType {
	case "PRICE_RANGE":
		return nlp.parsePriceRange(match)
	case "SINGLE_PRICE":
		return nlp.parseSinglePrice(match)
	case "RENT_PRICE":
		return nlp.parseRentPrice(match)
	case "AREA_RANGE":
		return nlp.parseAreaRange(match)
	case "ROOM_LAYOUT":
		return nlp.parseRoomLayout(match)
	case "FLOOR_INFO":
		return nlp.parseFloorInfo(match)
	case "YEAR_INFO":
		return nlp.parseYearInfo(match)
	default:
		return match[0]
	}
}

// convertUnitToWan å°†ä»·æ ¼è½¬æ¢ä¸ºä¸‡å•ä½ï¼ˆå¥å£®ç‰ˆæœ¬ï¼‰
// ç›´æ¥æ ¹æ®å•ä½å­—ç¬¦ä¸²è½¬æ¢ï¼Œæ¶ˆé™¤å­—ç¬¦ä¸²æŸ¥æ‰¾çš„è„†å¼±æ€§
func (nlp *AdvancedNLPProcessor) convertUnitToWan(value float64, unit string) float64 {
	if unit == "äº¿" {
		return value * 10000 // 1äº¿ = 10000ä¸‡
	}
	return value // ä¸‡æˆ–ç©ºå­—ç¬¦ä¸²éƒ½æŒ‰ä¸‡å¤„ç†
}

// parsePriceRange è§£æä»·æ ¼åŒºé—´ï¼ˆå¥å£®ç‰ˆæœ¬ï¼Œæ¶ˆé™¤strings.Indexé£é™©ï¼‰
func (nlp *AdvancedNLPProcessor) parsePriceRange(match []string) interface{} {
	result := make(map[string]interface{})

	// éå†æ•è·ç»„ï¼Œå¯»æ‰¾æœ‰æ•ˆçš„ä»·æ ¼æ¨¡å¼
	for i := 1; i < len(match); i++ {
		if match[i] == "" {
			continue
		}

		// æ£€æŸ¥ä»·æ ¼åŒºé—´æ¨¡å¼: æ•°å­—-æ•°å­—+å•ä½
		if i+2 < len(match) && match[i+1] != "" && match[i+2] != "" {
			// æ¨¡å¼: "5000-1ä¸‡" æˆ– "1-2äº¿"
			if min, err := strconv.ParseFloat(match[i], 64); err == nil {
				if max, err := strconv.ParseFloat(match[i+1], 64); err == nil {
					unit := match[i+2]
					if unit == "ä¸‡" || unit == "äº¿" {
						result["min"] = nlp.convertUnitToWan(min, unit)
						result["max"] = nlp.convertUnitToWan(max, unit)
						result["unit"] = "ä¸‡"
						return result
					}
				}
			}
		}

		// æ£€æŸ¥æ··åˆå•ä½æ¨¡å¼: æ•°å­—+å•ä½-æ•°å­—+å•ä½
		if i+3 < len(match) && match[i+1] != "" && match[i+2] != "" && match[i+3] != "" {
			// æ¨¡å¼: "5000ä¸‡-1äº¿"
			if min, err := strconv.ParseFloat(match[i], 64); err == nil {
				if max, err := strconv.ParseFloat(match[i+2], 64); err == nil {
					minUnit := match[i+1]
					maxUnit := match[i+3]
					if (minUnit == "ä¸‡" || minUnit == "äº¿") && (maxUnit == "ä¸‡" || maxUnit == "äº¿") {
						result["min"] = nlp.convertUnitToWan(min, minUnit)
						result["max"] = nlp.convertUnitToWan(max, maxUnit)
						result["unit"] = "ä¸‡"
						return result
					}
				}
			}
		}

		// æ£€æŸ¥å•å€¼+æ“ä½œç¬¦æ¨¡å¼: æ•°å­—+å•ä½+æ“ä½œç¬¦
		if i+1 < len(match) && match[i+1] != "" {
			unit := match[i+1]
			if unit == "ä¸‡" || unit == "äº¿" {
				if value, err := strconv.ParseFloat(match[i], 64); err == nil {
					// æŸ¥çœ‹å®Œæ•´åŒ¹é…æ–‡æœ¬åˆ¤æ–­æ“ä½œç¬¦
					fullText := match[0]
					if strings.Contains(fullText, "ä»¥ä¸‹") || strings.Contains(fullText, "ä»¥å†…") {
						result["max"] = nlp.convertUnitToWan(value, unit)
						result["operator"] = "<="
					} else if strings.Contains(fullText, "ä»¥ä¸Š") {
						result["min"] = nlp.convertUnitToWan(value, unit)
						result["operator"] = ">="
					} else {
						result["value"] = nlp.convertUnitToWan(value, unit)
					}
					result["unit"] = "ä¸‡"
					return result
				}
			}
		}
	}

	// å¤„ç†å…ƒå•ä½ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
	for i := 1; i < len(match); i++ {
		if match[i] == "" {
			continue
		}
		if value, err := strconv.ParseFloat(match[i], 64); err == nil {
			fullText := match[0]
			if strings.Contains(fullText, "å…ƒ") {
				if strings.Contains(fullText, "ä»¥ä¸‹") {
					result["max"] = value
					result["operator"] = "<="
				} else if strings.Contains(fullText, "ä»¥ä¸Š") {
					result["min"] = value
					result["operator"] = ">="
				} else {
					result["value"] = value
				}
				result["unit"] = "å…ƒ"
				return result
			}
		}
	}

	return nil
}

// parseSinglePrice è§£æå•ä¸ªä»·æ ¼
func (nlp *AdvancedNLPProcessor) parseSinglePrice(match []string) interface{} {
	result := make(map[string]interface{})

	// æŸ¥æ‰¾æ•°å­—åŒ¹é…ç»„
	for i := 1; i < len(match); i++ {
		if match[i] != "" {
			if price, err := strconv.ParseFloat(match[i], 64); err == nil {
				result["value"] = price
				result["unit"] = "ä¸‡"
				break
			}
		}
	}

	if len(result) == 0 {
		return nil
	}
	return result
}

// parseRentPrice è§£æç§Ÿé‡‘ä»·æ ¼
func (nlp *AdvancedNLPProcessor) parseRentPrice(match []string) interface{} {
	result := make(map[string]interface{})

	// æ–°çš„æ­£åˆ™åˆ†ç»„ï¼š
	// 1,2: æœˆç§Ÿé‡‘(\d+)[-åˆ°è‡³~](\d+)
	// 3,4: æœˆç§Ÿ(\d+)[-åˆ°è‡³~](\d+)
	// 5,6: ç§Ÿé‡‘(\d+)[-åˆ°è‡³~](\d+)å…ƒ?/?æœˆ?
	// 7,8: æœˆç§Ÿé‡‘(\d+)åˆ°(\d+)å…ƒ?
	// 9,10: ç§Ÿé‡‘(\d+)åˆ°(\d+)å…ƒ?
	// 11: æœˆç§Ÿé‡‘(\d+)å·¦å³
	// 12: æœˆç§Ÿ(\d+)å·¦å³
	// 13: ç§Ÿé‡‘(\d+)å·¦å³
	// 14: æœˆç§Ÿé‡‘(\d+)
	// 15: æœˆç§Ÿ(\d+)
	// 16: ç§Ÿé‡‘(\d+)å…ƒ?/?æœˆ?

	if len(match) > 2 && match[1] != "" && match[2] != "" {
		// åŒºé—´æ ¼å¼: "æœˆç§Ÿé‡‘3000-5000"
		if min, err := strconv.ParseFloat(match[1], 64); err == nil {
			result["min"] = min
		}
		if max, err := strconv.ParseFloat(match[2], 64); err == nil {
			result["max"] = max
		}
		result["unit"] = "å…ƒ"
	} else if len(match) > 4 && match[3] != "" && match[4] != "" {
		// åŒºé—´æ ¼å¼: "æœˆç§Ÿ3000-5000"
		if min, err := strconv.ParseFloat(match[3], 64); err == nil {
			result["min"] = min
		}
		if max, err := strconv.ParseFloat(match[4], 64); err == nil {
			result["max"] = max
		}
		result["unit"] = "å…ƒ"
	} else if len(match) > 6 && match[5] != "" && match[6] != "" {
		// åŒºé—´æ ¼å¼: "ç§Ÿé‡‘3000-5000å…ƒ/æœˆ"
		if min, err := strconv.ParseFloat(match[5], 64); err == nil {
			result["min"] = min
		}
		if max, err := strconv.ParseFloat(match[6], 64); err == nil {
			result["max"] = max
		}
		result["unit"] = "å…ƒ"
	} else if len(match) > 8 && match[7] != "" && match[8] != "" {
		// åŒºé—´æ ¼å¼: "æœˆç§Ÿé‡‘3000åˆ°5000å…ƒ"
		if min, err := strconv.ParseFloat(match[7], 64); err == nil {
			result["min"] = min
		}
		if max, err := strconv.ParseFloat(match[8], 64); err == nil {
			result["max"] = max
		}
		result["unit"] = "å…ƒ"
	} else if len(match) > 10 && match[9] != "" && match[10] != "" {
		// åŒºé—´æ ¼å¼: "ç§Ÿé‡‘3000åˆ°5000å…ƒ"
		if min, err := strconv.ParseFloat(match[9], 64); err == nil {
			result["min"] = min
		}
		if max, err := strconv.ParseFloat(match[10], 64); err == nil {
			result["max"] = max
		}
		result["unit"] = "å…ƒ"
	} else {
		// å•ä¸ªä»·æ ¼å€¼
		for i := 11; i < len(match); i++ {
			if match[i] != "" {
				if price, err := strconv.ParseFloat(match[i], 64); err == nil {
					result["value"] = price
					result["unit"] = "å…ƒ"
					break
				}
			}
		}
	}

	if len(result) == 0 {
		return nil
	}
	return result
}

// parseAreaRange è§£æé¢ç§¯åŒºé—´
func (nlp *AdvancedNLPProcessor) parseAreaRange(match []string) interface{} {
	result := make(map[string]interface{})

	if len(match) >= 3 && match[1] != "" && match[2] != "" {
		// ç»„1,2: é¢ç§¯åŒºé—´ "80-120å¹³ç±³"
		if min, err := strconv.ParseFloat(match[1], 64); err == nil {
			result["min"] = min
		}
		if max, err := strconv.ParseFloat(match[2], 64); err == nil {
			result["max"] = max
		}
	} else if len(match) >= 4 && match[3] != "" {
		// ç»„3: "120å¹³ç±³ä»¥ä¸Š"
		if area, err := strconv.ParseFloat(match[3], 64); err == nil {
			result["min"] = area
			result["operator"] = ">="
		}
	} else if len(match) >= 5 && match[4] != "" {
		// ç»„4: "120å¹³ç±³ä»¥ä¸‹"
		if area, err := strconv.ParseFloat(match[4], 64); err == nil {
			result["max"] = area
			result["operator"] = "<="
		}
	} else if len(match) >= 6 && match[5] != "" {
		// ç»„5: "é¢ç§¯120"
		if area, err := strconv.ParseFloat(match[5], 64); err == nil {
			result["value"] = area
		}
	} else if len(match) >= 7 && match[6] != "" {
		// ç»„6: "120å¹³ç±³å·¦å³"
		if area, err := strconv.ParseFloat(match[6], 64); err == nil {
			result["value"] = area
			result["operator"] = "about"
		}
	} else {
		// ç»„7,8: å…¶ä»–å•ä¸ªé¢ç§¯å€¼
		for i := 7; i < len(match); i++ {
			if match[i] != "" {
				if area, err := strconv.ParseFloat(match[i], 64); err == nil {
					result["value"] = area
					break
				}
			}
		}
	}

	result["unit"] = "å¹³ç±³"
	return result
}

// parseRoomLayout è§£ææˆ¿å‹å¸ƒå±€
func (nlp *AdvancedNLPProcessor) parseRoomLayout(match []string) interface{} {
	result := make(map[string]interface{})

	// æ­£åˆ™è¡¨è¾¾å¼åˆ†ç»„ï¼ˆå¥—æˆ¿é€šè¿‡é¢„å¤„ç†convertRoomLayoutè½¬æ¢ï¼‰ï¼š
	// 1,2,3: (\d+)å®¤(\d+)å…(\d+)å«
	// 4,5: (\d+)æˆ¿(\d+)å…
	// 6: ([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾1-9])å±…å®¤
	// 7,8: ([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾1-9])å®¤([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾1-9])å…
	// 9,10,11: (\d+)æˆ¿(\d+)å…(\d+)å«
	// 12,13: (\d+)å®¤(\d+)å…
	// 14: (\d+)å±…å®¤
	// 15: (\d+)æˆ¿é—´
	// 16: (\d+)å®¤ - å•ç‹¬çš„é˜¿æ‹‰ä¼¯æ•°å­—å®¤
	// 17: ([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åä¸¤å£¹è´°åè‚†ä¼é™†æŸ’æŒç–æ‹¾])å®¤ - å•ç‹¬çš„ä¸­æ–‡æ•°å­—å®¤


	// æŸ¥æ‰¾éç©ºçš„åŒ¹é…ç»„
	for i := 1; i < len(match); i++ {
		if match[i] != "" {
			switch i {
			case 1, 4, 9, 12: // å®¤/æˆ¿çš„æ•°é‡
				if num, err := strconv.Atoi(match[i]); err == nil {
					result["bedrooms"] = num
				}
			case 2, 5, 10, 13: // å…çš„æ•°é‡
				if num, err := strconv.Atoi(match[i]); err == nil {
					result["living_rooms"] = num
				}
			case 3, 11: // å«çš„æ•°é‡
				if num, err := strconv.Atoi(match[i]); err == nil {
					result["bathrooms"] = num
				}
			case 6: // å±…å®¤çš„æƒ…å†µ - ç‰¹æ®Šå¤„ç†"ä¸€å±…å®¤"ç­‰
				// "ä¸€å±…å®¤"é€šå¸¸æ„æ€æ˜¯1å®¤1å…
				bedrooms := common.ConvertChineseToInt(match[i])
				if bedrooms > 0 {
					result["bedrooms"] = bedrooms
					result["living_rooms"] = 1
					result["description"] = fmt.Sprintf("%då®¤1å…", bedrooms)
				}
			case 7: // ä¸­æ–‡æ•°å­—å®¤
				bedrooms := common.ConvertChineseToInt(match[i])
				if bedrooms > 0 {
					result["bedrooms"] = bedrooms
				}
			case 8: // ä¸­æ–‡æ•°å­—å…
				livingRooms := common.ConvertChineseToInt(match[i])
				if livingRooms > 0 {
					result["living_rooms"] = livingRooms
				}
			case 14: // é˜¿æ‹‰ä¼¯æ•°å­—å±…å®¤ - å¤„ç†"1å±…å®¤"ç­‰
				if num, err := strconv.Atoi(match[i]); err == nil {
					result["bedrooms"] = num
					result["living_rooms"] = 1
					result["description"] = fmt.Sprintf("%då®¤1å…", num)
				}
			case 15: // æˆ¿é—´æ•°é‡
				if num, err := strconv.Atoi(match[i]); err == nil {
					result["bedrooms"] = num
					result["living_rooms"] = 1
					result["description"] = fmt.Sprintf("%då®¤1å…", num)
				}
			case 16: // å•ç‹¬çš„é˜¿æ‹‰ä¼¯æ•°å­—"Nå®¤"
				if num, err := strconv.Atoi(match[i]); err == nil {
					result["bedrooms"] = num
					result["living_rooms"] = 1
					result["description"] = fmt.Sprintf("%då®¤1å…", num)
				}
			case 17: // å•ç‹¬çš„ä¸­æ–‡æ•°å­—"Nå®¤"ï¼ˆåŒ…æ‹¬å¤§å†™ï¼‰
				bedrooms := common.ConvertChineseToInt(match[i])
				if bedrooms > 0 {
					result["bedrooms"] = bedrooms
					result["living_rooms"] = 1
					result["description"] = fmt.Sprintf("%då®¤1å…", bedrooms)
				}
			}
		}
	}

	// å¦‚æœæ²¡æœ‰è§£æå‡ºä»»ä½•å†…å®¹ï¼Œè¿”å›nil
	if len(result) == 0 {
		return nil
	}

	// ç”Ÿæˆæè¿°å­—ç¬¦ä¸²
	if desc, exists := result["description"]; !exists || desc == "" {
		var descParts []string
		if bedrooms, ok := result["bedrooms"].(int); ok {
			descParts = append(descParts, fmt.Sprintf("%då®¤", bedrooms))
		}
		if livingRooms, ok := result["living_rooms"].(int); ok {
			descParts = append(descParts, fmt.Sprintf("%då…", livingRooms))
		}
		if bathrooms, ok := result["bathrooms"].(int); ok {
			descParts = append(descParts, fmt.Sprintf("%då«", bathrooms))
		}
		if len(descParts) > 0 {
			result["description"] = strings.Join(descParts, "")
		}
	}

	return result
}

// parseFloorInfo è§£ææ¥¼å±‚ä¿¡æ¯
func (nlp *AdvancedNLPProcessor) parseFloorInfo(match []string) interface{} {
	result := make(map[string]interface{})

	if len(match) >= 2 && match[1] != "" {
		if floor, err := strconv.Atoi(match[1]); err == nil {
			result["floor"] = floor
		}
	}

	if len(match) >= 4 && match[3] != "" && match[4] != "" {
		// "8/30å±‚" æ ¼å¼
		if current, err := strconv.Atoi(match[3]); err == nil {
			result["current"] = current
		}
		if total, err := strconv.Atoi(match[4]); err == nil {
			result["total"] = total
		}
	}

	return result
}

// parseYearInfo è§£æå¹´ä»½ä¿¡æ¯
func (nlp *AdvancedNLPProcessor) parseYearInfo(match []string) interface{} {
	for i := 1; i < len(match); i++ {
		if match[i] != "" {
			if year, err := strconv.Atoi(match[i]); err == nil {
				return year
			}
		}
	}
	return nil
}

// ExtractNumbers æå–æ•°å­—å®ä½“
func (nlp *AdvancedNLPProcessor) ExtractNumbers(text string) ([]NumberEntity, error) {
	if text == "" {
		return nil, ErrEmptyInput
	}

	var numbers []NumberEntity

	// æå–é˜¿æ‹‰ä¼¯æ•°å­—
	arabicMatches := nlp.numberRegex.FindAllString(text, -1)
	for _, match := range arabicMatches {
		if value, err := strconv.ParseFloat(match, 64); err == nil {
			start := strings.Index(text, match)
			number := NumberEntity{
				Text:       match,
				Value:      value,
				Start:      start,
				End:        start + len(match),
				Confidence: 0.9,
			}

			// åˆ†ææ•°å­—ç±»å‹å’Œå•ä½
			number.Type, number.Unit = nlp.analyzeNumberContext(text, start)
			numbers = append(numbers, number)
		}
	}

	// ğŸ”§ ä¿®å¤ï¼šä¸å†åœ¨æ ‡å‡†åŒ–åçš„æ–‡æœ¬ä¸­æå–ä¸­æ–‡æ•°å­—
	// å› ä¸ºæ ‡å‡†åŒ–è¿‡ç¨‹å·²ç»å°†ä¸­æ–‡æ•°å­—è½¬æ¢ä¸ºé˜¿æ‹‰ä¼¯æ•°å­—
	// å¦‚æœå†æå–ä¸­æ–‡æ•°å­—ï¼Œä¼šå¯¼è‡´"å…«å"è¢«è¯†åˆ«ä¸º"8"+"10"=810çš„é”™è¯¯
	//
	// ğŸ’¡ è®¾è®¡å†³ç­–ï¼šextractChineseNumbers å‡½æ•°å·²é‡æ„å¹¶ä¿ç•™ï¼Œä½†å½“å‰æœªå¯ç”¨
	// è‹¥æœªæ¥éœ€è¦åœ¨æ ‡å‡†åŒ–å‰è¿›è¡Œä¸­æ–‡æ•°å­—ç›´æ¥æå–ï¼Œå¯åŸºäº common åŒ…æ¢å¤ä½¿ç”¨
	//
	// åŸä»£ç ï¼š
	// chineseNumbers := nlp.extractChineseNumbers(text)
	// numbers = append(numbers, chineseNumbers...)

	return numbers, nil
}

// extractChineseNumbers æå–ä¸­æ–‡æ•°å­—
// ğŸ”„ å½“å‰çŠ¶æ€ï¼šä¿ç•™ä½†æœªå¯ç”¨ (ä¸Šå±‚æœªåˆå¹¶)
// ğŸ“ é‡æ„è¯´æ˜ï¼šå·²åŸºäº common.ChineseToArabicExtended é‡æ„ï¼Œæ¶ˆé™¤é‡å¤æ˜ å°„
// ğŸš€ æœªæ¥è§„åˆ’ï¼šè‹¥éœ€è¦ä¸­æ–‡æ•°å­—ç›´æ¥æŠ½å–ï¼Œå¯åŸºäº common åŒ…åšæ›´ç²¾ç»†çš„ä¸Šä¸‹æ–‡è§„åˆ™åæ¢å¤
func (nlp *AdvancedNLPProcessor) extractChineseNumbers(text string) []NumberEntity {
	var numbers []NumberEntity

	// ä½¿ç”¨ç»Ÿä¸€çš„ä¸­æ–‡æ•°å­—æ˜ å°„è¡¨
	for chinese, value := range common.ChineseToArabicExtended {
		if strings.Contains(text, chinese) {
			start := strings.Index(text, chinese)
			number := NumberEntity{
				Text:       chinese,
				Value:      value,
				Start:      start,
				End:        start + len(chinese),
				Confidence: 0.8,
			}

			// åˆ†ææ•°å­—ç±»å‹å’Œå•ä½
			number.Type, number.Unit = nlp.analyzeNumberContext(text, start)
			numbers = append(numbers, number)
		}
	}

	return numbers
}

// analyzeNumberContext åˆ†ææ•°å­—ä¸Šä¸‹æ–‡
func (nlp *AdvancedNLPProcessor) analyzeNumberContext(text string, position int) (string, string) {
	// åˆ†ææ•°å­—å‰åçš„ä¸Šä¸‹æ–‡æ¥åˆ¤æ–­ç±»å‹
	contextBefore := ""
	contextAfter := ""

	if position > 10 {
		contextBefore = text[position-10 : position]
	} else {
		contextBefore = text[:position]
	}

	if position+20 < len(text) {
		contextAfter = text[position : position+20]
	} else {
		contextAfter = text[position:]
	}

	context := contextBefore + contextAfter

	// ä¼˜å…ˆæ£€æŸ¥å…·ä½“çš„å•ä½è¯ï¼Œé¿å…è¢«æ¨¡ç³Šè¯å¹²æ‰°

	// é¢ç§¯ç›¸å…³ - ä¼˜å…ˆæ£€æŸ¥ï¼Œå› ä¸º"å¹³"æ˜¯ä¸“ç”¨å•ä½
	if strings.Contains(context, "å¹³") || strings.Contains(context, "ã¡") ||
		strings.Contains(context, "é¢ç§¯") {
		return "area", "å¹³ç±³"
	}

	// æ¥¼å±‚ç›¸å…³ - ä¼˜å…ˆæ£€æŸ¥ï¼Œå› ä¸º"å±‚"/"æ¥¼"æ˜¯ä¸“ç”¨å•ä½
	if strings.Contains(context, "å±‚") || strings.Contains(context, "æ¥¼") {
		return "floor", "å±‚"
	}

	// æˆ¿å‹ç›¸å…³ - ä¼˜å…ˆæ£€æŸ¥ï¼Œå› ä¸º"å®¤"/"å…"/"å«"æ˜¯ä¸“ç”¨å•ä½
	if strings.Contains(context, "å®¤") || strings.Contains(context, "å…") ||
		strings.Contains(context, "å«") {
		return "room", "é—´"
	}

	// å¹´ä»½ç›¸å…³ - ä¼˜å…ˆæ£€æŸ¥
	if strings.Contains(context, "å¹´") {
		return "year", "å¹´"
	}

	// ä»·æ ¼ç›¸å…³ - æœ€åæ£€æŸ¥ï¼Œé¿å…"ä»·"å­—çš„æ¨¡ç³ŠåŒ¹é…å¹²æ‰°å…¶ä»–ä¸“ç”¨å•ä½
	if strings.Contains(context, "ä¸‡") || strings.Contains(context, "å…ƒ") ||
		strings.Contains(context, "ä»·") || strings.Contains(context, "é’±") {
		if strings.Contains(context, "ä¸‡") {
			return "price", "ä¸‡"
		}
		return "price", "å…ƒ"
	}

	return "unknown", ""
}

// AnalyzeSentiment åˆ†ææƒ…æ„Ÿ
func (nlp *AdvancedNLPProcessor) AnalyzeSentiment(text string) (*SentimentResult, error) {
	// ç®€å•çš„åŸºäºè¯å…¸çš„æƒ…æ„Ÿåˆ†æ
	positiveWords := []string{
		"å¥½", "æ£’", "ä¼˜ç§€", "æ»¡æ„", "å–œæ¬¢", "èµ", "ä¸é”™", "å®Œç¾", "ç†æƒ³",
		"æ–¹ä¾¿", "ä¾¿åˆ©", "èˆ’é€‚", "è±ªå", "ç²¾ç¾", "æ¼‚äº®", "å¹²å‡€", "æ–°",
		"å®½æ•", "æ˜äº®", "å®‰é™", "äº¤é€šä¾¿åˆ©", "é…å¥—é½å…¨", "æ€§ä»·æ¯”é«˜",
	}

	negativeWords := []string{
		"å·®", "å", "ç³Ÿç³•", "ä¸æ»¡æ„", "è®¨åŒ", "åƒåœ¾", "ç ´", "æ—§", "è„",
		"åµ", "æš—", "å°", "è´µ", "è¿œ", "ä¸æ–¹ä¾¿", "éº»çƒ¦", "é—®é¢˜", "ç¼ºç‚¹",
		"å™ªéŸ³", "æ±¡æŸ“", "è€æ—§", "ç‹­çª„", "æ˜æš—", "ååƒ»",
	}

	positiveCount := 0
	negativeCount := 0

	text = strings.ToLower(text)

	for _, word := range positiveWords {
		if strings.Contains(text, word) {
			positiveCount++
		}
	}

	for _, word := range negativeWords {
		if strings.Contains(text, word) {
			negativeCount++
		}
	}

	// è®¡ç®—æƒ…æ„Ÿå¾—åˆ†
	totalWords := positiveCount + negativeCount
	var score float64
	var label string
	var confidence float64

	if totalWords == 0 {
		score = 0
		label = "neutral"
		confidence = 0.5
	} else {
		score = float64(positiveCount-negativeCount) / float64(totalWords)
		confidence = float64(totalWords) / 10.0 // ç®€å•çš„ç½®ä¿¡åº¦è®¡ç®—
		if confidence > 1.0 {
			confidence = 1.0
		}

		if score > 0.1 {
			label = "positive"
		} else if score < -0.1 {
			label = "negative"
		} else {
			label = "neutral"
		}
	}

	return &SentimentResult{
		Score:      score,
		Label:      label,
		Confidence: confidence,
	}, nil
}

// ExtractPredicate æå–å¥å­çš„è°“è¯­åŠ¨è¯
// Jennyå¼è®¾è®¡ï¼šä¸“æ³¨äºè¯†åˆ«ä¸»è¦åŠ¨ä½œåŠ¨è¯ï¼Œç”¨äºè¿‡æ»¤ç§Ÿå”®åœºæ™¯
func (nlp *AdvancedNLPProcessor) ExtractPredicate(text string) string {
	// åˆ†è¯
	segments := nlp.segmenter.Cut(text)
	if len(segments) == 0 {
		return ""
	}

	// ç§Ÿå”®ç›¸å…³çš„åŠ¨è¯ï¼ˆåªåŒ…å«æ ¸å¿ƒåŠ¨è¯ï¼Œä¸åŒ…å«æ¨¡æ€åŠ¨è¯ç»„åˆï¼‰
	rentalSellingVerbs := map[string]bool{
		"å‡ºç§Ÿ": true, "ç§Ÿ": true, "æ±‚ç§Ÿ": true,
		"å‡ºå”®": true, "å”®": true, "å–": true, "å”®å–": true,
		"è½¬è®©": true, "å‡ºè®©": true, "è½¬æ‰‹": true,
	}

	// æ¨¡æ€åŠ¨è¯å’ŒåŠ©åŠ¨è¯ï¼ˆé€šå¸¸åœ¨ä¸»è¦åŠ¨è¯å‰ï¼‰
	modalVerbs := map[string]bool{
		"æƒ³": true, "è¦": true, "æƒ³è¦": true, "æ‰“ç®—": true, "å‡†å¤‡": true,
		"éœ€è¦": true, "å¸Œæœ›": true, "è®¡åˆ’": true, "è€ƒè™‘": true,
		"èƒ½": true, "å¯ä»¥": true, "åº”è¯¥": true, "å¿…é¡»": true,
		"æˆ‘è¦": true, "æˆ‘æƒ³": true, "æˆ‘å‡†å¤‡": true, "æˆ‘æ‰“ç®—": true,
	}

	// ğŸ†• ä»å¥æ ‡è®°è¯ - è¿™äº›è¯åé¢çš„åŠ¨è¯é€šå¸¸æ˜¯ä»å¥åŠ¨è¯ï¼Œä¸æ˜¯ä¸»å¥è°“è¯­
	subordinateMarkers := map[string]bool{
		"å": true, "ä»¥å": true, "ä¹‹å": true, // æ—¶é—´ä»å¥
		"ç„¶å": true, "æ¥ç€": true, "å†": true, // é¡ºåºä»å¥
		"ç”¨æ¥": true, "ç”¨äº": true, "ç”¨ä½œ": true, // ç›®çš„ä»å¥
		"ä¸ºäº†": true, "ä»¥ä¾¿": true, "ä»¥å…": true, // ç›®çš„ä»å¥
		"å¦‚æœ": true, "è¦æ˜¯": true, "å‡å¦‚": true, // æ¡ä»¶ä»å¥
		"è™½ç„¶": true, "å°½ç®¡": true, "å³ä½¿": true, // è®©æ­¥ä»å¥
		"å› ä¸º": true, "ç”±äº": true, "æ—¢ç„¶": true, // åŸå› ä»å¥
	}

	// ğŸ†• æ£€æŸ¥æ˜¯å¦è¿›å…¥ä»å¥åŒºåŸŸ
	inSubordinate := false

	// ğŸ”§ Jennyå¼æ”¹è¿›ï¼šå¤„ç†å¤åˆåˆ†è¯æ®µ
	// æ£€æŸ¥æ¯ä¸ªåˆ†è¯æ®µå†…éƒ¨æ˜¯å¦åŒ…å«ç§Ÿå”®åŠ¨è¯
	for i, segment := range segments {
		// æ£€æŸ¥æ˜¯å¦é‡åˆ°ä»å¥æ ‡è®°
		if subordinateMarkers[segment] {
			inSubordinate = true
			continue
		}

		// é‡åˆ°é€—å·ï¼Œå¥å·ç­‰æ ‡ç‚¹å¯èƒ½ç»“æŸä»å¥
		if segment == "ï¼Œ" || segment == "ã€‚" || segment == "ï¼›" {
			// ä»å¥å¯èƒ½ç»§ç»­ï¼Œä¸è¦ç«‹å³é‡ç½®
			continue
		}

		// ğŸ†• å¦‚æœåœ¨ä»å¥ä¸­ï¼Œè·³è¿‡ç§Ÿå”®åŠ¨è¯æ£€æŸ¥
		// é™¤éè¿™æ˜¯å¥å­çš„ç¬¬ä¸€ä¸ªåŠ¨è¯ï¼ˆè¯´æ˜æ²¡æœ‰ä¸»å¥åŠ¨è¯ï¼‰
		if inSubordinate && i > 2 {
			// ä»å¥ä¸­çš„åŠ¨è¯ä¸ä½œä¸ºä¸»è¦è°“è¯­
			continue
		}
		// è·³è¿‡æ˜æ˜¾çš„åè¯æ€§å¤åˆè¯
		if strings.Contains(segment, "ç§Ÿé‡‘") || strings.Contains(segment, "å”®ä»·") ||
			strings.Contains(segment, "ç§Ÿèµè´¹") || strings.Contains(segment, "å‡ºå”®ä»·") {
			continue
		}

		// 1ï¸âƒ£ é¦–å…ˆæ£€æŸ¥å®Œæ•´åŒ¹é…ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
		if rentalSellingVerbs[segment] {
			// é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿ä¸æ˜¯å¤åˆè¯çš„ä¸€éƒ¨åˆ†
			if i > 0 {
				prevWord := segments[i-1]
				if prevWord+segment == "ç§Ÿé‡‘" || prevWord+segment == "å”®ä»·" {
					continue
				}
			}
			return segment
		}

		// 2ï¸âƒ£ ç„¶åæ£€æŸ¥åˆ†è¯æ®µå†…éƒ¨æ˜¯å¦åŒ…å«ç§Ÿå”®åŠ¨è¯
		// å¤„ç†"æƒ³ç§Ÿä¸ª"ã€"å–æˆ¿å­"è¿™ç§å¤åˆåˆ†è¯
		// æŒ‰é•¿åº¦é™åºæ£€æŸ¥ï¼Œä¼˜å…ˆåŒ¹é…é•¿çš„åŠ¨è¯
		var foundVerb string
		maxLen := 0
		for verb := range rentalSellingVerbs {
			if strings.Contains(segment, verb) && len(verb) > maxLen {
				// ç¡®ä¿ä¸æ˜¯"ç§Ÿé‡‘"è¿™æ ·çš„åè¯
				if verb == "ç§Ÿ" && strings.Contains(segment, "ç§Ÿé‡‘") {
					continue
				}
				if verb == "å”®" && (strings.Contains(segment, "å”®ä»·") || strings.Contains(segment, "å”®æ¥¼")) {
					continue
				}
				if verb == "å–" && strings.Contains(segment, "ä¹°å–") {
					continue // "ä¹°å–"æ˜¯åè¯ï¼Œä¸æ˜¯åŠ¨è¯
				}
				// æ‰¾åˆ°äº†æ›´é•¿çš„ç§Ÿå”®åŠ¨è¯
				foundVerb = verb
				maxLen = len(verb)
			}
		}
		if foundVerb != "" {
			return foundVerb
		}

		// æ£€æŸ¥æ¨¡æ€åŠ¨è¯+åŠ¨è¯ç»„åˆ
		if modalVerbs[segment] && i+1 < len(segments) {
			nextWord := segments[i+1]

			// ğŸ†• æ£€æŸ¥ä¸‹ä¸€ä¸ªè¯å†…éƒ¨æ˜¯å¦åŒ…å«ç§Ÿå”®åŠ¨è¯
			// å¤„ç†"å–æˆ¿å­"è¿™ç§æƒ…å†µ
			for verb := range rentalSellingVerbs {
				if strings.Contains(nextWord, verb) {
					// æ’é™¤åè¯æƒ…å†µ
					if verb == "ç§Ÿ" && strings.Contains(nextWord, "ç§Ÿé‡‘") {
						continue
					}
					if verb == "å”®" && (strings.Contains(nextWord, "å”®ä»·") || strings.Contains(nextWord, "å”®æ¥¼")) {
						continue
					}
					if verb == "å–" && strings.Contains(nextWord, "ä¹°å–") {
						continue
					}
					return verb
				}
			}

			// å¦‚æœä¸‹ä¸€ä¸ªè¯æ˜¯å®Œæ•´çš„ç§Ÿå”®åŠ¨è¯
			if rentalSellingVerbs[nextWord] {
				return nextWord
			}
		}
	}

	// æ²¡æœ‰æ‰¾åˆ°ç§Ÿå”®ç›¸å…³åŠ¨è¯ï¼Œå°è¯•è¯†åˆ«å…¶ä»–ä¸»è¦åŠ¨è¯
	// å¸¸è§çš„è´­æˆ¿ç›¸å…³éç§Ÿå”®åŠ¨è¯ï¼ˆè¿™äº›ä¸åº”è¯¥è¢«è¿‡æ»¤ï¼‰
	commonVerbs := []string{
		"æ‰¾", "ä¹°", "è´­ä¹°", "è´­ç½®", "é€‰", "é€‰æ‹©", "çœ‹", "æŸ¥çœ‹",
		"äº†è§£", "å’¨è¯¢", "æŠ•èµ„", "ç½®ä¸š", "å…¥æ‰‹", "è€ƒè™‘",
	}

	for i, word := range segments {
		// è·³è¿‡æ¨¡æ€åŠ¨è¯
		if modalVerbs[word] {
			continue
		}

		// æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§åŠ¨è¯
		for _, verb := range commonVerbs {
			if word == verb || strings.HasPrefix(word, verb) {
				// ç‰¹æ®Šå¤„ç†"ä¹°"ï¼Œéœ€è¦ç¡®è®¤ä¸æ˜¯"ä¹°æˆ¿"çš„ä¹°
				if verb == "ä¹°" {
					// æ£€æŸ¥æ˜¯å¦è·Ÿç€ç§Ÿå”®ç›¸å…³çš„è¯
					if i+1 < len(segments) {
						nextWord := segments[i+1]
						if strings.Contains(nextWord, "æˆ¿") || strings.Contains(nextWord, "å¥—") {
							// è¿™æ˜¯è´­æˆ¿ï¼Œä¸æ˜¯å‡ºå”®ï¼Œè¿”å›"ä¹°"
							return "ä¹°"
						}
					}
				}
				return word
			}
		}
	}

	// å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œè¿”å›ç¬¬ä¸€ä¸ªå¯èƒ½çš„åŠ¨è¯ï¼ˆåŸºäºè¯æ€§åˆ¤æ–­ï¼‰
	// ç®€å•å¯å‘å¼ï¼š2-3ä¸ªå­—çš„è¯ï¼Œä¸æ˜¯åè¯æ€§è¯ç¼€çš„ï¼Œå¯èƒ½æ˜¯åŠ¨è¯
	nounSuffixes := []string{"æˆ¿", "å®¤", "å…", "åŒº", "å¸‚", "è·¯", "è¡—", "å›­", "æ¥¼", "ä¸‡", "å¹³", "ç±³"}
	for _, word := range segments {
		if len(word) >= 2 && len(word) <= 6 {
			isNoun := false
			for _, suffix := range nounSuffixes {
				if strings.HasSuffix(word, suffix) {
					isNoun = true
					break
				}
			}
			if !isNoun && !modalVerbs[word] {
				// å¯èƒ½æ˜¯åŠ¨è¯
				return word
			}
		}
	}

	return ""
}

// NormalizeText æ–‡æœ¬æ ‡å‡†åŒ–
func (nlp *AdvancedNLPProcessor) NormalizeText(text string) string {
	// 1. ç§»é™¤å¤šä½™ç©ºç™½
	text = regexp.MustCompile(`\s+`).ReplaceAllString(text, " ")
	text = strings.TrimSpace(text)

	// 2. æ ‡å‡†åŒ–æ ‡ç‚¹ç¬¦å·
	punctuationMap := map[string]string{
		"ï¼Œ": ",",
		"ã€‚": ".",
		"ï¼": "!",
		"ï¼Ÿ": "?",
		"ï¼›": ";",
		"ï¼š": ":",
		"ï¼ˆ": "(",
		"ï¼‰": ")",
		"ã€": "[",
		"ã€‘": "]",
		"ã€Š": "<",
		"ã€‹": ">",
	}

	// ğŸ”§ ä¿®å¤ç¨³å®šæ€§é—®é¢˜ï¼šç¡®ä¿ç¡®å®šæ€§çš„éå†é¡ºåº
	var punctKeys []string
	for chinese := range punctuationMap {
		punctKeys = append(punctKeys, chinese)
	}
	sort.Strings(punctKeys)

	for _, chinese := range punctKeys {
		english := punctuationMap[chinese]
		text = strings.ReplaceAll(text, chinese, english)
	}

	// 3. ğŸ  å±…å®¤è½¬æ¢é¢„å¤„ç† - ä¼˜å…ˆå¤„ç†ï¼Œé¿å…åˆ†è¯å†²çª
	text = nlp.convertRoomLayout(text)

	// 4. ç¹ç®€è½¬æ¢ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
	text = nlp.simplifyTraditionalChinese(text)

	// 5. æ•°å­—æ ‡å‡†åŒ–
	text = nlp.normalizeNumbers(text)

	// 6. å¤§å°å†™æ ‡å‡†åŒ–
	text = nlp.normalizeCases(text)

	return text
}

// simplifyTraditionalChinese ç¹ä½“è½¬ç®€ä½“ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
func (nlp *AdvancedNLPProcessor) simplifyTraditionalChinese(text string) string {
	traditionalToSimplified := map[string]string{
		"æ¨“": "æ¥¼", "å±¤": "å±‚", "é–“": "é—´", "å»³": "å…", "è¡›": "å«",
		"å€": "åŒº", "é–€": "é—¨", "è»Š": "è½¦", "é›»": "ç”µ", "ç·š": "çº¿",
		"é–‹": "å¼€", "é—œ": "å…³", "è²·": "ä¹°", "è³£": "å–", "åƒ¹": "ä»·",
		"éŒ¢": "é’±", "å“¡": "å‘˜", "åœ‹": "å›½", "å­¸": "å­¦", "æœƒ": "ä¼š",
	}

	// ğŸ”§ ä¿®å¤ç¨³å®šæ€§é—®é¢˜ï¼šç¡®ä¿ç¡®å®šæ€§çš„éå†é¡ºåº
	var traditionalKeys []string
	for traditional := range traditionalToSimplified {
		traditionalKeys = append(traditionalKeys, traditional)
	}
	sort.Strings(traditionalKeys)

	for _, traditional := range traditionalKeys {
		simplified := traditionalToSimplified[traditional]
		text = strings.ReplaceAll(text, traditional, simplified)
	}

	return text
}

// normalizeNumbers æ•°å­—æ ‡å‡†åŒ–
func (nlp *AdvancedNLPProcessor) normalizeNumbers(text string) string {
	// ä½¿ç”¨å­—å…¸ä¸­çš„åŒä¹‰è¯æ˜ å°„
	if nlp.dictionary != nil {
		text = nlp.dictionary.NormalizeText(text)
	}

	return text
}

// normalizeCases å¤§å°å†™æ ‡å‡†åŒ–
func (nlp *AdvancedNLPProcessor) normalizeCases(text string) string {
	// è‹±æ–‡å•è¯é¦–å­—æ¯å¤§å†™
	words := strings.Fields(text)
	for i, word := range words {
		if nlp.isEnglishWord(word) {
			words[i] = strings.Title(strings.ToLower(word))
		}
	}

	return strings.Join(words, " ")
}

// isEnglishWord åˆ¤æ–­æ˜¯å¦ä¸ºè‹±æ–‡å•è¯
func (nlp *AdvancedNLPProcessor) isEnglishWord(word string) bool {
	for _, r := range word {
		if !unicode.IsLetter(r) || r > unicode.MaxASCII {
			return false
		}
	}
	return len(word) > 0
}

// deduplicateEntities å»é‡å®ä½“
func (nlp *AdvancedNLPProcessor) deduplicateEntities(entities []Entity) []Entity {
	if len(entities) <= 1 {
		return entities
	}

	// ğŸ”§ ä¿®å¤ç¨³å®šæ€§é—®é¢˜ï¼šç¡®ä¿ç¡®å®šæ€§çš„éå†é¡ºåº
	// 1. æŒ‰ä½ç½®åˆ†ç»„
	positionGroups := make(map[string][]Entity)

	for _, entity := range entities {
		key := fmt.Sprintf("%d_%d", entity.Start, entity.End)
		positionGroups[key] = append(positionGroups[key], entity)
	}

	// 2. ç¡®ä¿ç¡®å®šæ€§é¡ºåºï¼šæŒ‰ä½ç½®é”®æ’åº
	var keys []string
	for key := range positionGroups {
		keys = append(keys, key)
	}
	sort.Strings(keys) // æŒ‰å­—ç¬¦ä¸²æ’åºç¡®ä¿ç¨³å®šé¡ºåº

	var result []Entity

	// 3. å¤„ç†æ¯ä¸ªä½ç½®ç»„
	for _, key := range keys {
		group := positionGroups[key]
		if len(group) == 1 {
			result = append(result, group[0])
		} else {
			// 4. å¤šä¸ªå®ä½“åœ¨åŒä¸€ä½ç½®ï¼Œè¿›è¡Œæ™ºèƒ½å»é‡
			best := nlp.selectBestEntity(group)
			result = append(result, best)
		}
	}

	// 5. å¤„ç†é‡å å®ä½“ï¼ˆä¸åŒä½ç½®ä½†å†…å®¹é‡å ï¼‰
	result = nlp.resolveOverlappingEntities(result)

	// 6. æœ€ç»ˆæ’åºï¼šæŒ‰ä½ç½®æ’åºç¡®ä¿è¾“å‡ºç¨³å®šæ€§
	sort.Slice(result, func(i, j int) bool {
		if result[i].Start != result[j].Start {
			return result[i].Start < result[j].Start
		}
		return result[i].End < result[j].End
	})

	return result
}

// selectBestEntity ä»åŒä½ç½®çš„å¤šä¸ªå®ä½“ä¸­é€‰æ‹©æœ€ä½³å®ä½“
func (nlp *AdvancedNLPProcessor) selectBestEntity(entities []Entity) Entity {
	if len(entities) == 1 {
		return entities[0]
	}

	// 1. æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆç¡®ä¿ç¨³å®šæ€§ï¼‰
	sort.Slice(entities, func(i, j int) bool {
		priorityI := getEntityPriority(entities[i].Type)
		priorityJ := getEntityPriority(entities[j].Type)
		if priorityI != priorityJ {
			return priorityI > priorityJ // é«˜ä¼˜å…ˆçº§åœ¨å‰
		}
		// ä¼˜å…ˆçº§ç›¸åŒæ—¶ï¼ŒæŒ‰ç±»å‹åç§°æ’åºç¡®ä¿ç¨³å®šæ€§
		return entities[i].Type < entities[j].Type
	})

	// 2. é€‰æ‹©æœ€é«˜ä¼˜å…ˆçº§çš„å®ä½“
	best := entities[0]

	// 3. å¦‚æœå¤šä¸ªå®ä½“ä¼˜å…ˆçº§ç›¸åŒï¼Œé€‰æ‹©ç½®ä¿¡åº¦æœ€é«˜çš„
	for _, entity := range entities {
		if getEntityPriority(entity.Type) == getEntityPriority(best.Type) {
			if entity.Confidence > best.Confidence {
				best = entity
			} else if entity.Confidence == best.Confidence {
				// ç½®ä¿¡åº¦ä¹Ÿç›¸åŒï¼Œé€‰æ‹©æ–‡æœ¬æ›´å…·ä½“çš„ï¼ˆæ›´é•¿çš„ï¼‰
				if len(entity.Text) > len(best.Text) {
					best = entity
				} else if len(entity.Text) == len(best.Text) {
					// é•¿åº¦ä¹Ÿç›¸åŒï¼ŒæŒ‰å­—å…¸åºé€‰æ‹©ç¡®ä¿ç¨³å®šæ€§
					if entity.Text < best.Text {
						best = entity
					}
				}
			}
		}
	}

	return best
}

// resolveOverlappingEntities è§£å†³é‡å å®ä½“é—®é¢˜
func (nlp *AdvancedNLPProcessor) resolveOverlappingEntities(entities []Entity) []Entity {
	if len(entities) <= 1 {
		return entities
	}

	// æŒ‰å¼€å§‹ä½ç½®æ’åº
	sort.Slice(entities, func(i, j int) bool {
		return entities[i].Start < entities[j].Start
	})

	var result []Entity
	result = append(result, entities[0])

	for i := 1; i < len(entities); i++ {
		current := entities[i]
		last := &result[len(result)-1]

		// æ£€æŸ¥æ˜¯å¦é‡å 
		if current.Start < last.End {
			// é‡å æƒ…å†µï¼šé€‰æ‹©æ›´å¥½çš„å®ä½“
			if nlp.isEntityBetter(current, *last) {
				// ç”¨å½“å‰å®ä½“æ›¿æ¢æœ€åä¸€ä¸ª
				result[len(result)-1] = current
			}
			// å¦åˆ™å¿½ç•¥å½“å‰å®ä½“
		} else {
			// æ— é‡å ï¼Œç›´æ¥æ·»åŠ 
			result = append(result, current)
		}
	}

	return result
}

// isEntityBetter åˆ¤æ–­å®ä½“Aæ˜¯å¦æ¯”å®ä½“Bæ›´å¥½
func (nlp *AdvancedNLPProcessor) isEntityBetter(a, b Entity) bool {
	// 1. ä¼˜å…ˆçº§æ¯”è¾ƒ
	priorityA := getEntityPriority(a.Type)
	priorityB := getEntityPriority(b.Type)
	if priorityA != priorityB {
		return priorityA > priorityB
	}

	// 2. ç½®ä¿¡åº¦æ¯”è¾ƒ
	if a.Confidence != b.Confidence {
		return a.Confidence > b.Confidence
	}

	// 3. æ–‡æœ¬é•¿åº¦æ¯”è¾ƒï¼ˆæ›´å…·ä½“çš„æ›´å¥½ï¼‰
	if len(a.Text) != len(b.Text) {
		return len(a.Text) > len(b.Text)
	}

	// 4. ä½ç½®æ¯”è¾ƒï¼ˆèµ·å§‹ä½ç½®æ›´æ—©çš„æ›´å¥½ï¼‰
	if a.Start != b.Start {
		return a.Start < b.Start
	}

	// 5. å­—å…¸åºæ¯”è¾ƒï¼ˆç¡®ä¿ç¨³å®šæ€§ï¼‰
	return a.Text < b.Text
}

// getEntityPriority è·å–å®ä½“ç±»å‹çš„ä¼˜å…ˆçº§ï¼Œæ•°å­—è¶Šå¤§ä¼˜å…ˆçº§è¶Šé«˜
func getEntityPriority(entityType string) int {
	priorities := map[string]int{
		"PLATE":          11, // æ¿å—æœ€å…·ä½“ï¼ˆå¦‚é™†å®¶å˜´ã€å¼ æ±Ÿç­‰ï¼‰
		"DISTRICT":       10, // åŒºå¿æœ€å…·ä½“
		"PROVINCE":       8,  // çœä»½æ¬¡ä¹‹
		"COMMERCIAL":     7,  // å•†ä¸šåœ°äº§ä¿¡æ¯æ¯”é€šç”¨åœ°ç‚¹æ›´é‡è¦
		"INTEREST_POINT": 8,  // å…´è¶£ç‚¹æ¯”é€šç”¨åœ°ç‚¹æ›´é‡è¦
		"SINGLE_PRICE":   9,  // å•ä¸ªä»·æ ¼æ¯”ä»·æ ¼åŒºé—´æ›´å…·ä½“
		"PRICE_RANGE":    8,  // ä»·æ ¼åŒºé—´
		"ORIENTATION":    7,  // æœå‘
		"ROOM_LAYOUT":    9,  // æˆ¿å‹ - æé«˜ä¼˜å…ˆçº§ç¡®ä¿ç¨³å®šæ€§
		"DECORATION":     6,  // è£…ä¿®
		"PROPERTY_TYPE":  6,  // æˆ¿å±‹ç±»å‹
		"LOCATION":       5,  // é€šç”¨åœ°ç‚¹
		"RENT_PRICE":     9,  // ç§Ÿé‡‘ä»·æ ¼æ¯”ä»·æ ¼åŒºé—´æ›´å…·ä½“
	}

	if priority, exists := priorities[entityType]; exists {
		return priority
	}
	return 1 // é»˜è®¤ä¼˜å…ˆçº§
}

// GetProcessorStats è·å–å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯
func (nlp *AdvancedNLPProcessor) GetProcessorStats() map[string]interface{} {
	return map[string]interface{}{
		"initialized":          nlp.initialized,
		"entity_rules":         len(nlp.entityRules),
		"segmenter_available":  nlp.segmenter != nil,
		"dictionary_available": nlp.dictionary != nil,
	}
}

// ConvertToPinyin ä¸­æ–‡è½¬æ‹¼éŸ³
func (nlp *AdvancedNLPProcessor) ConvertToPinyin(text string) ([]string, error) {
	args := pinyin.NewArgs()
	args.Style = pinyin.NORMAL
	args.Heteronym = false
	args.Separator = " "
	args.Fallback = func(r rune, a pinyin.Args) []string {
		return []string{string(r)}
	}

	return pinyin.LazyConvert(text, &args), nil
}
